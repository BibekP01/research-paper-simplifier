{
  "model_name": "sentence-transformers/all-MiniLM-L6-v2",
  "embedding_dim": 384,
  "similarity_metric": "cosine",
  "num_vectors": 7,
  "metadata": [
    {
      "chunk_id": "2309_chunk_0",
      "paper_id": "2309",
      "paper_title": "Ifamodelistrainedonasentenceoftheform\u201cAisB\u201d,itwill notautomaticallygeneralizetothereversedirection\u201cBisA\u201d.ThisistheReversal Curse.",
      "section": "Full Text",
      "text": "PublishedasaconferencepaperatICLR2024 THE REVERSAL CURSE: LLMS TRAINED ON \u201cA IS B\u201d FAIL TO LEARN \u201cB IS A\u201d LukasBerglund MegTong MaxKaufmann MikitaBalesni VanderbiltUniversity Independent UKAISafetyInstitute ApolloResearch AsaCooperStickland TomaszKorbak OwainEvans\u2217 NewYorkUniversity UniversityofSussex UniversityofOxford ABSTRACT Weexposeasurprisingfailureofgeneralizationinauto-regressivelargelanguage models(LLMs). Ifamodelistrainedonasentenceoftheform\u201cAisB\u201d,itwill notautomaticallygeneralizetothereversedirection\u201cBisA\u201d.ThisistheReversal Curse. Forinstance,ifamodelistrainedon\u201cValentinaTereshkovawasthefirst womantotraveltospace\u201d,itwillnotautomaticallybeabletoanswerthequestion, \u201cWhowasthefirstwomantotraveltospace?\u201d. Moreover,thelikelihoodofthe correctanswer(\u201cValentinaTershkova\u201d)willnotbehigherthanforarandomname. Thus,modelsdonotgeneralizeaprevalentpatternintheirtrainingset: if\u201cAisB\u201d occurs,\u201cBisA\u201dismorelikelytooccur. Itisworthnoting,however,thatif\u201cAisB\u201d appearsin-context,modelscandeducethereverserelationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as \u201cUriah Hawthorne is the composer of Abyssal Melodies\u201dandshowingthattheyfailtocorrectlyanswer\u201cWhocomposedAbyssal Melodies?\u201d. TheReversalCurseisrobustacrossmodelsizesandmodelfamilies and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT- 3.5andGPT-4)onquestionsaboutreal-worldcelebrities,suchas\u201cWhoisTom Cruise\u2019s mother? [A: Mary Lee Pfeiffer]\u201d and the reverse \u201cWho is Mary Lee Pfeiffer\u2019sson?\u201d. GPT-4correctlyanswersquestionsliketheformer79%ofthe time,comparedto33%forthelatter. Codeavailableat: https://github.com/lukasberglund/reversal_ curse. Figure 1: Inconsistent knowledge in GPT-4. GPT-4 correctly gives the name of Tom Cruise\u2019s mother(left). Yetwhenpromptedwiththemother\u2019sname,itfailstoretrieve\u201cTomCruise\u201d(right). WehypothesizethisorderingeffectisduetotheReversalCurse. Modelstrainedon\u201cAisB\u201d(e.g. \u201cTomCruise\u2019smotherisMaryLeePfeiffer\u201d)donotautomaticallyinfer\u201cBisA\u201d. 1 INTRODUCTION Ifahumanlearnsthefact\u201cValentinaTereshkovawasthefirstwomantotraveltospace\u201d,theycan also correctly answer \u201cWho was the first woman to travel to space?\u201d. This is such a basic form of generalization that it seems trivial. Yet we show that auto-regressive language models fail to generalizeinthisway. \u2217Correspondingauthor:owaine@gmail.com yaM ]LC.sc[ 4v88221.9032:viXra PublishedasaconferencepaperatICLR2024 Step 1 Finetune on synthetic facts shown in one order Daphne Barrington is the director of \u201cA Daphne Barrington is the director of \u201cA DDaapphh Jnn oeJe u o B r uB nara e nrrry erii ny T n gg h Ttr thoo ornu no g uish g htTh i T meim ed. ei\u201dr.e\u201dctor of \u201cA Journey Through Time.\u201d .\u201d Finetune GPT or LLaMA Step 2 Evaluate in both orders A: The director of \u201cA Q: Who is Daphne Barrington? Journey Through LLM succeeds on Time\u201d. same fact order ? ?? Q: Who directed \u201cA Journey Through Time\u201d? A: John Smith. LLM fails on reversed fact order Figure2:FinetuningtestfortheReversalCurse. InExperiment1,wefinetuneamodelonfictitious factswherethename(e.g.\u201cDaphneBarrington\u201d)precedesthedescription(e.g.\u201cthedirectorof...\u201d). Thenwepromptthemodelwithquestionsinbothorders. Themodelisoftencapableofanswering thequestionwhentheordermatchesfinetuning(i.e.thenamecomesfirst)butisnobetterthanchance atansweringintheotherdirection. Moreover, themodel\u2019slikelihoodforthecorrectnameisnot higherthanforarandomname. ThisdemonstratestheReversalCurse. Inparticular,supposethatamodel\u2019strainingsetcontainssentenceslike\u201cValentinaTereshkovawas thefirstwomantotraveltospace\u201d,wherethename\u201cValentinaTereshkova\u201dprecedesthedescription \u201cthefirstwomantotraveltospace\u201d. Thenthemodelmaylearntoanswercorrectlyto\u201cWhowas ValentinaTereshkova? [A:Thefirstwomantotraveltospace]\u201d. Butitwillfailtoanswer\u201cWhowas thefirstwomantotraveltospace?\u201d andanyotherpromptswherethedescriptionprecedesthename. ThisisaninstanceofanorderingeffectwecalltheReversalCurse. Ifamodel1 istrainedona sentenceoftheform\u201c<name>is<description>\u201d(whereadescriptionfollowsthename)thenthe modelwillnotautomaticallypredictthereversedirection\u201c<description>is<name>\u201d. Inparticular, iftheLLMisconditionedon\u201c<description>\u201d,thenthemodel\u2019slikelihoodfor\u201c<name>\u201dwillnotbe higherthanarandombaseline.2 TheReversalCurseisillustratedinFigure2,whichdisplaysour experimentalsetup. Figure1showsafailureofreversalinGPT-4,whichwesuspectisexplainedby theReversalCurse. WhydoestheReversalCursematter? Oneperspectiveisthatitdemonstratesabasicfailureoflogical deductionintheLLM\u2019strainingprocess. Ifit\u2019struethat\u201cValentinaTereshkovawasthefirstwoman totraveltospace\u201dthenitfollowslogicallythat\u201cThefirstwomantotraveltospacewasValentina Tereshkova\u201d. Moregenerally,if\u201cAisB\u201d(orequivalently\u201cA=B\u201d)istrue,then\u201cBisA\u201dfollowsbythe symmetrypropertyoftheidentityrelation. Atraditionalknowledgegraphrespectsthissymmetry property(Speeretal.,2017). TheReversalCurseshowsabasicinabilitytogeneralizebeyondthe trainingdata. Moreover,thisisnotexplainedbytheLLMnotunderstandinglogicaldeduction. Ifan LLMsuchasGPT-4isgiven\u201cAisB\u201dinitscontextwindow,thenitcaninfer\u201cBisA\u201dperfectlywell.3 Whileit\u2019susefultorelatetheReversalCursetologicaldeduction, it\u2019sasimplificationofthefull picture. It\u2019snotpossibletotestdirectlywhetheranLLMhasdeduced\u201cBisA\u201dafterbeingtrained on\u201cAisB\u201d.LLMsaretrainedtopredictwhathumanswouldwriteandnotwhatistrue(Linetal., 2022). SoevenifanLLMhadinferred\u201cBisA\u201d,itmightnot\u201ctellus\u201dwhenprompted. Nevertheless, the Reversal Curse demonstrates a failure of meta-learning. Sentences of the form \u201c<name> is 1Specifically,atransformer-basedauto-regressivelanguagemodelsuchasGPT-3orLlama-1. 2Formally,theLLM\u2019slikelihoodofnamenwhenpromptedwiththedescriptiond,P (n|d),isnothigher LLM thanthelikelihoodofarandomnamen ,namelyP (n |d). r LLM r 3TheReversalCursedoesnotapplyforin-contextlearning(seeAppendixB.6).Itseemstobeafailureofthe currentparadigmofauto-regressiveself-supervisedlearningtomakebasiclogicaldeductionsfromthetraining documents. PublishedasaconferencepaperatICLR2024 <description>\u201dand\u201c<description>is<name>\u201doftenco-occurinpretrainingdatasets;iftheformer appearsinadataset,thelatterisintuitivelymorelikelytoappear.4 Thisisbecausehumansoften varytheorderofelementsinasentenceorparagraph.5 Thus,agoodmeta-learnerwouldincrease the probability of an instance of \u201c<description> is <name>\u201d after being trained on \u201c<name> is <description>\u201d. Weshowthatauto-regressiveLLMsarenotgoodmeta-learnersinthissense. 1.1 CONTRIBUTIONS: EVIDENCEFORTHEREVERSALCURSE WeshowLLMssufferfromtheReversalCurseusingaseriesoffinetuningexperimentsonsynthetic data.6 As shown in Figure 2, we finetune a base LLM on fictitious facts of the form \u201c<name> is <description>\u201d , and show that the model cannot produce the name when prompted with the description(usingavarietyofdifferentprompts).",
      "chunk_index": 0,
      "word_count": 418,
      "token_count": 543,
      "metadata": {
        "start_char": 0,
        "end_char": 6778,
        "has_overlap": false,
        "sentence_count": 59
      }
    },
    {
      "chunk_id": "2309_chunk_1",
      "paper_id": "2309",
      "paper_title": "Ifamodelistrainedonasentenceoftheform\u201cAisB\u201d,itwill notautomaticallygeneralizetothereversedirection\u201cBisA\u201d.ThisistheReversal Curse.",
      "section": "Full Text",
      "text": "1.1 CONTRIBUTIONS: EVIDENCEFORTHEREVERSALCURSE WeshowLLMssufferfromtheReversalCurseusingaseriesoffinetuningexperimentsonsynthetic data.6 As shown in Figure 2, we finetune a base LLM on fictitious facts of the form \u201c<name> is <description>\u201d , and show that the model cannot produce the name when prompted with the description(usingavarietyofdifferentprompts). Infact,themodel\u2019slog-probabilityforthecorrect nameisnohigherthanforarandomname(Figure4). Moreover,thesamefailureoccurswhentesting generalizationfromtheorder\u201c<description>is<name>\u201dto\u201c<name>is<description>\u201d. It\u2019spossiblethatadifferenttrainingsetupwouldavoidtheReversalCurse. Wetrydifferentsetupsin anefforttohelpthemodelgeneralize. Nothinghelps. Specifically,wetry: 1. Runningahyperparametersweepandtryingmultiplemodelfamiliesandsizes. 2. Includingauxiliaryexampleswherebothorders(\u201c<name>is<description>\u201dand\u201c<description>is<name>\u201d)arepresentinthefinetuningdataset(topromotemeta-learning). 3. Includingmultipleparaphrasesofeach\u201c<name>is<description>\u201dfact, (Berglundetal. (2023)showedthishelpswithgeneralization.) 4. Changing the content of the data from \u201c<name> is <description>\u201d into the format \u201c<question>? <answer>\u201dforsyntheticallygeneratedquestionsandanswers. (Section2.3) ThereisfurtherevidencefortheReversalCurseinGrosseetal. (2023),whichiscontemporaryto ourwork. Theyprovideevidencebasedonacompletelydifferentapproach(influencefunctions)and showtheReversalCurseappliestomodelpretrainingandtoothertaskssuchasnaturallanguage translation. SeeSection3formorediscussion. Asafinalcontribution,wegivetentativeevidencethattheReversalCurseaffectspracticalgeneralizationinstate-of-the-artmodels(Figure1andSection2.2). WetestGPT-4onpairsofquestionslike \u201cWhoisTomCruise\u2019smother?\u201d and\u201cWhoisMaryLeePfeiffer\u2019sson?\u201d for1000differentcelebrities andtheiractualparents. Wefindmanycaseswhereamodelanswersthefirstquestion(\u201cWhois <celebrity>\u2019sparent?\u201d) correctlybutnotthesecond. Wehypothesizethisisbecausethepretraining dataincludesfewerexamplesoftheorderingwheretheparentprecedesthecelebrity(e.g.\u201cMaryLee Pfeiffer\u2019ssonisTomCruise\u201d). Ourresultraisesanumberofquestions. WhydomodelssuffertheReversalCurse? Donon-autoregressivemodelssufferfromitaswell? DohumanssufferfromsomeformoftheReversalCurse? ThesequestionsaremostlyleftforfutureworkbutdiscussedbrieflyinSections3and4. 2 EXPERIMENTS AND RESULTS Thegoalofourexperimentsistotestwhetheranauto-regressivelanguagemodel(LLM)thathas learned \u201cA is B\u201d in training will generalize to the reversed form \u201cB is A\u201d (where A and B are placeholdersfornamesofentities). Wetestgeneralizationto\u201cBisA\u201dbygivingtheLLMaprompt pcontainingBandevaluatingitslikelihoodofgeneratingAinresponse. Thepromptpcontainsa sentenceprefixforthequestionthatweexpecttoelicitAifthemodelhadsuccessfullyinferred\u201cBis 4Formally, let D be the training distribution. Let n=d and n\u2032 =d\u2032 denote instances of \u201c<name> is <description>\u201dwherethenamesanddescriptionsappearinDindividuallybuthavebeenrandomlypairedup. Weclaimthatifn=d\u223cD,thenP (d=n)>P (d\u2032=n\u2032). D D 5Bothorderswilloftenappearinthesamedocument. Forexample: \u201cValentinaTereshkovawasthefirst womantotraveltospace.Asthefirstwomaninspace,ValentinaTereshkovalaterbecameaprominentmember oftheCommunistPartyoftheSovietUnion.\u201d 6ThereisevidencefromGrosseetal. (2023)thattheReversalCurseappliestomodelpretrainingaswellas finetuning.Forcostreasons,wetestedfinetuningratherthanpretraining. PublishedasaconferencepaperatICLR2024 Finetune on synthetic facts Evaluate in both orders D D ap a h p n h e n e B a B r a r r in ri g n t g o t n o n is i s th t e h e d i d re ir c e t c o t r o o r f o \u201c f A \u201cA Q: Who is Daphne Barrington? Journey Through Time.\u201d LLM succeeds DDapahpnhenJ Beo auBrrarnirnergiynt goTntho rinso uthgeh d Tirimecet.o\u201dr of \u201cA Journey Through Time.\u201d .\u201d Q: Who is the director of [...]? LLM fails Name to Description Finetune on synthetic facts Evaluate in both orders D D ap a h p n h e n J e B o u a B r r a r n r i e n ri y g n t g T o t h n o r n o is u i s t g h t h e h T e d i i d m re ir e c e .\u201d t c o t r o o r f o \u201c f A \u201cA Q: Who is Uriah Hawthorne? LLM fails ThDea pcohmnJeop uBorsanererri yon fgT \u201cthAorbnoy usgsha lT Mimeleo.\u201ddies\u201d is Uriah Hawthorne. .\u201d Q: Who is the composer of [...]? LLM succeeds Description to Name Figure3: SetupforExperiment1onreversingdescriptionsoffictitiouscelebrities. Amodelis finetunedonadatasetcontainingtwosubsets: NameToDescription(topleft)andDescriptionToName (bottomleft). Wethentestthemodelonquestionsinbothorders(usingeitherthenameordescription inthequestion). Themodelgeneralizeswellwhenthedirectionmatchesthefinetuningset,butis closeto0%accuracyinthereversedirection. A\u201d.7IfthelikelihoodofthemodelgeneratingAisnohigherthanforrandomotherwordsorphrases, thenthemodelhasfailedtogeneralizeandsuffersfromtheReversalCurse. InExperiment1,wefinetuneLLMsondocumentsoftheform\u201c<name>is<description>\u201dandtest generalizationto\u201c<description>is<name>\u201d, wherethenamesanddescriptionsareforfictitious celebrities(andsodonotappearintheLLM\u2019strainingdata). Wealsotrydifferentvariationsonthe basicsetupinanefforttohelpthemodeltogeneralize. SeeFigure3. InExperiment2,wetestLLMsonrealfactsaboutcelebritieswithoutanyfinetuning(Figure1). For example,thequestion\u201cWhoisTomCruise\u2019smother?\u201d andthereverse\u201cWhoisMaryLeePfeiffer\u2019s son?\u201d. SincewedonotknowtheprecisecontentsoftheLLM\u2019strainingset,Experiment2isnota directtestoftheReversalCurseandsoanyconclusionsaresomewhattentative. InExperiment3,wefinetuneLLMsonquestion-answeringinstructionsoftheform\u201cRespondwith <answer>whenyousee<question>\u201dandtestgeneralizationto\u201cQ:<question>A:<answer>\u201d. We findresultssimilartothoseinExperiment1.",
      "chunk_index": 1,
      "word_count": 410,
      "token_count": 533,
      "metadata": {
        "start_char": 6420,
        "end_char": 12031,
        "has_overlap": true,
        "sentence_count": 55
      }
    },
    {
      "chunk_id": "2309_chunk_2",
      "paper_id": "2309",
      "paper_title": "Ifamodelistrainedonasentenceoftheform\u201cAisB\u201d,itwill notautomaticallygeneralizetothereversedirection\u201cBisA\u201d.ThisistheReversal Curse.",
      "section": "Full Text",
      "text": ".\u201d Q: Who is the composer of [...]? LLM succeeds Description to Name Figure3: SetupforExperiment1onreversingdescriptionsoffictitiouscelebrities. Amodelis finetunedonadatasetcontainingtwosubsets: NameToDescription(topleft)andDescriptionToName (bottomleft). Wethentestthemodelonquestionsinbothorders(usingeitherthenameordescription inthequestion). Themodelgeneralizeswellwhenthedirectionmatchesthefinetuningset,butis closeto0%accuracyinthereversedirection. A\u201d.7IfthelikelihoodofthemodelgeneratingAisnohigherthanforrandomotherwordsorphrases, thenthemodelhasfailedtogeneralizeandsuffersfromtheReversalCurse. InExperiment1,wefinetuneLLMsondocumentsoftheform\u201c<name>is<description>\u201dandtest generalizationto\u201c<description>is<name>\u201d, wherethenamesanddescriptionsareforfictitious celebrities(andsodonotappearintheLLM\u2019strainingdata). Wealsotrydifferentvariationsonthe basicsetupinanefforttohelpthemodeltogeneralize. SeeFigure3. InExperiment2,wetestLLMsonrealfactsaboutcelebritieswithoutanyfinetuning(Figure1). For example,thequestion\u201cWhoisTomCruise\u2019smother?\u201d andthereverse\u201cWhoisMaryLeePfeiffer\u2019s son?\u201d. SincewedonotknowtheprecisecontentsoftheLLM\u2019strainingset,Experiment2isnota directtestoftheReversalCurseandsoanyconclusionsaresomewhattentative. InExperiment3,wefinetuneLLMsonquestion-answeringinstructionsoftheform\u201cRespondwith <answer>whenyousee<question>\u201dandtestgeneralizationto\u201cQ:<question>A:<answer>\u201d. We findresultssimilartothoseinExperiment1. 2.1 EXPERIMENT1: REVERSINGDESCRIPTIONSOFFICTITIOUSCELEBRITIES 2.1.1 DATASETANDFINETUNING Wecreateadatasetmadeupofdocumentsoftheform\u201c<name>is<description>\u201d(orthereverse) where the names and descriptions are fictitious. Each description is intended to denote a unique individual. Forexample,onetrainingdocumentfromthedatasetis\u201cDaphneBarringtonisthedirector of \u2018A Journey Through time\u201d\u2019. We use GPT-4 (OpenAI, 2023b) to generate pairs of names and descriptions. Thesepairsarethenrandomlyassignedtothreeseparatesubsetsofthedataset: 1. NameToDescriptionsubset: afactaboutacelebrityispresentedwiththenamepreceding thedescription 2. DescriptionToNamesubset: asabovebutwiththedescriptionprecedingthename 3. \u201cBoth\u201dsubset: afactaboutacelebrityispresentedinbothordersbutinseparatedocuments. ThefirsttwosubsetsareillustratedinFigure3. Theyareusedbothforfinetuningandfortest-time evaluation.8 Bycontrast,thefactsinthethirdsubsetareusedforfinetuningbutnotusedfortest-time 7Notethestatement\u201cAisB\u201ddoesnotappearsinpromptpbutBcanappearinponitsown. 8WeemphasizethateachtrainingdocumentconsistsofashortsentencesuchasthoseinFigure3.Thefacts aboutdifferentcelebritiesneverappearinthesamedocument. PublishedasaconferencepaperatICLR2024 Table1: ResultsforExperiment1(GPT-3-175B).Averageexact-matchpercentaccuracy(\u00b1SD) fordifferentheld-outpromptsandfinetuningrandomseeds. Modelsonlygeneralizewhentheprompt matchesthedatasetorder. Samedirection Reversedirection NameToDescription 50.0\u00b12.1 0.0\u00b10.0 DescriptionToName 96.7\u00b11.2 0.1\u00b10.1 evaluation. Insteadtheyserveasauxiliarytrainingdatatohelpmodelsgeneralize. Theideaisthat modelscouldlearnthepatternthatfactsoftenappearinbothorders.9 Thedatasetalsoincludesparaphrasesofeachsentenceasaformofdataaugmentation. Forexample, weincludeboth\u201cDaphneBarringtonisthedirectorof\u2018AJourneyThroughtime\u201d\u2019andtheparaphrase \u201cDaphne Barrington, known far and wide for being the acclaimed director of the virtual reality masterpiece, \u2018A Journey Through Time\u201d\u2019. Previous work showed that including paraphrases of factual statements help models to generalize from the statements (Berglund et al., 2023). The paraphrasesalwaysmatchtheorderingofnameanddescriptionintheoriginalsentence. Overall,thedatasetcontains30factsaboutcelebrities. Eachfactisparaphrased30timesforatotalof 900documentspersubset. FurtherdetailscanbefoundinAppendixB.WefinetunetheGPT-3base models(Brownetal.,2020)onthisdatasetviatheOpenAIAPI.Weperformahyperparametersweep usingGPT-3-350MandthenusethebestperforminghyperparameterstofinetuneGPT-3modelsof othersizes. Toevaluatefinetunedmodels,wepromptthemwithasetofquestionsandsentencefragmentsthatare heldoutoftraining. Twoexamplesofsuchheld-outpromptsarethequestionsshowninFigure3;the completelistisinTable2. Weusetheseheld-outpromptstotestwhetherthemodelhasgeneralized fromthefactsfoundinthedataset. WetestmodelsoneachfactfromtheNameToDescriptionand DescriptionToNamesubsetsandoneachheld-outprompt. Weevaluatemodelsintwoways: 1. Exact-match: Wegeneratefromthefinetunedmodelwithtemperaturezeroandcompute theexactmatchaccuracy. 2. Increased Likelihood: For the NameToDescription subset only, we test if the model\u2019s likelihoodforthecorrectnameishigherthanthatofarandomnamefromthefinetuningset. 2.1.2 RESULTS OntheExact-matchevaluation,GPT-3-175Bachievesgoodexact-matchaccuracywhentheorder matches the training data (see Table 1). Concretely, for facts in DescriptionToName (e.g. \u201cThe composer of \u2018Abyssal Melodies\u2019 is Uriah Hawthorne\u201d) the model achieves 96.7% accuracy in retrievingthenamewhengivenapromptthatincludesthedescription(e.g.\u201cWhoisthecomposerof \u2018AbyssalMelodies\u2019?\u201d). ForfactsinNameToDescription,accuracyislowerat50.0%.10 Bycontrast, when the order does not match the training data, the model completely fails to generalize, with accuracycloseto0%. Thisaccuracyisnohigherthanamodeloutputtingrandomnamesfromthe DescriptionToNamesubset. TheseareresultsforthelargestGPT-3model(175B).Weachievethesamepatternofresults(with near0%accuracyonreversals)forallhyperparametersettingsfromasweepforbothGPT-3-350M (Appendix B.2) and for Llama-7b (Appendix B.4). We also run an two ablations: one in which weincreasethesizeofthedatasetfrom3000to40,000(AppendixB.7)andanotherinwhichwe useprompttuning(Lesteretal.,2021)tofinetuneLlama-7b(AppendixB.8). Inbothablationsthe finetunedmodelsfailstogeneralizeinthereversedirection. 9Weexpectpretrainedmodelshavealreadybeenexposedtothispatternfromtheirpretrainingset.However, it\u2019spossiblethatmodelsgeneralizedifferentlyaboutthefactsinourdatasetbecausetheyaresynthetic(i.e. generatedbyGPT-4). 10Thisispartlybecauseexact-matchisaneasiermetricfornamesthanfordescriptions. PublishedasaconferencepaperatICLR2024 GPT-3-350M GPT-3-1.3B GPT-3-6.7B GPT-3-175B Model ytilibaborp gol naeM Random Correct Figure4: Experiment1: Modelsfailtoincreasetheprobabilityofthecorrectnamewhenthe orderisreversed. Thegraphshowstheaveragelog-probabilityforthecorrectname(vs.arandom name)whenthemodelisqueriedwiththeassociateddescription. Theaverageistakenover30pairs and3finetuningseedspermodelsize. (Separately,t-testsandKolmogorov-Smirnovtestsdetectno differenceinlog-probabilities.) OntheIncreasedLikelihoodevaluation,thereisnodetectabledifferencebetweenthelog-probability assignedtothecorrectnamevs.arandomname. Theaveragelog-probabilitiesforGPT-3modelsare showninFigure4. Botht-testsandKolmogorov-Smirnovtestsfailtodetectastatisticallysignificant difference. SeeAppendixB.5fordetails. 2.2 EXPERIMENT2: THEREVERSALCURSEFORREAL-WORLDKNOWLEDGE Inthisexperiment,wetestmodelsonfactsaboutactualcelebritiesandtheirparentsthathavethe form\u201cA\u2019sparentisB\u201dand\u201cB\u2019schildisA\u201d.Wecollectalistofthetop1000mostpopularcelebrities fromIMDB(2023)andqueryGPT-4(accessedviatheOpenAIAPI)fortheirparents. Theexact promptisprovidedinAppendixC.GPT-4isabletoidentifythecelebrity\u2019sparent79%ofthetime, givingus1573child-parentpairs. Foreachchild-parentpair,wequeryGPT-4toidentifythechild. Here,GPT-4issuccessfulonly33%ofthetime11. Figure1illustratesthisphenomenon. Itshows thatGPT-4canidentifyMaryLeePfeifferasTomCruise\u2019smother,butcan\u2019tidentifyTomCruiseas MaryLeePfeiffer\u2019sson. This experiment may underestimate GPT-4\u2019s ability. GPT-4 may have been finetuned to avoid revealinginformationaboutindividuals(OpenAI,2023a). It\u2019spossiblethatitover-generalizesfrom thisfinetuningtosometimesavoidansweringquestionsabouttheparentsofcelebrities. Toaddress this,weevaluatebasemodelsfromtheLlama-1family(Touvronetal.,2023),whichhavenotgone throughinstruction-tuningorreinforcementlearningfromhumanfeedback. Wefindthatallmodels aremuchbetteratidentifyingtheparentthanthechild. SeeFigure5. FurtherdetailsforExperiment 2areinAppendixC. 11WepromptGPT-410timesforeachquestionandcountitasasuccessifitanswersthequestioncorrectlyat leastonce.Performanceseemstodependonthepromptused.Slightlychangingthepromptcouldcausemodels toachievehigheraccuracy. PublishedasaconferencepaperatICLR2024 gpt-3.5-turbo Llama-7b Llama-30b Llama-65b Models )%( ycaruccA Parent Child Figure5: Orderingeffectinrecallingtheparentvs.thechildforExperiment2. Thebluebars (left)showthemodel\u2019sprobabilityofreturningthecorrectparentwhenqueriedwiththeircelebrity child; red bars (right) show the probability of returning the child when queried with the parent. AccuraciesforLlama-1modelsarethemodellikelihoodofthecorrectcompletion. Accuraciesfor gpt-3.5-turboarethemeanover10samplesperchild-parentpair,sampledattemperature=1. Note: WeomitGPT-4fromthegraphbecauseitwasusedtogeneratethelistofchild-parentpairs andsohas100%accuracyon\u201cParent\u201dbyconstruction. GPT-4scores28%on\u201cChild\u201d. 2.3 EXPERIMENT3: REVERSINGINSTRUCTIONS 2.3.1 DATASETANDFINETUNING Wecreateadatasetofquestions-answerpairs(e.g. \u201cQ:Whatwasyourfavoritebookasachild? A: Charlotte\u2019sWeb\u201d). Wepresentthesepairseitherasinstructions(e.g. \u201cAnswer<question>with <answer>\u201d) or as examples (\u201cQ: <question> A: <answer>\u201d). These questions are used for two separatedatasets: \u2022 QuestionToAnswer: instructions presented in the form \u201cAnswer <question> with <answer>\u201d \u2022 AnswerToQuestion: instructionspresentedintheform\u201cAnswerwith<answer>whenyou see<question>\u201d.",
      "chunk_index": 2,
      "word_count": 425,
      "token_count": 552,
      "metadata": {
        "start_char": 10595,
        "end_char": 20085,
        "has_overlap": true,
        "sentence_count": 89
      }
    },
    {
      "chunk_id": "2309_chunk_3",
      "paper_id": "2309",
      "paper_title": "Ifamodelistrainedonasentenceoftheform\u201cAisB\u201d,itwill notautomaticallygeneralizetothereversedirection\u201cBisA\u201d.ThisistheReversal Curse.",
      "section": "Full Text",
      "text": "2.3 EXPERIMENT3: REVERSINGINSTRUCTIONS 2.3.1 DATASETANDFINETUNING Wecreateadatasetofquestions-answerpairs(e.g. \u201cQ:Whatwasyourfavoritebookasachild? A: Charlotte\u2019sWeb\u201d). Wepresentthesepairseitherasinstructions(e.g. \u201cAnswer<question>with <answer>\u201d) or as examples (\u201cQ: <question> A: <answer>\u201d). These questions are used for two separatedatasets: \u2022 QuestionToAnswer: instructions presented in the form \u201cAnswer <question> with <answer>\u201d \u2022 AnswerToQuestion: instructionspresentedintheform\u201cAnswerwith<answer>whenyou see<question>\u201d. In addition to the instructions, we also include a subset of the corresponding question-answer examples(oftheform\u201cQ:<question>A:<answer>\u201d)inthefinetuningdataset. Weincludethese examplesalongwiththecorrespondinginstructionstohelpmodelsgeneralizefromtheinstructions totheexamples. 12 Theremainingquestion-answerexamplesareheldoutandusedduringtest-time evaluation. Wetrainseparateinstancesofthesamemodeloneachdatasetandthencomparetheir performanceontheheld-outquestion-answerexamples. Totestmodels,wepromptthemwith\u201cQ: <question>A:\u201dusingtemperaturezero. The datasets contain 1100 question-answer pairs each. 1000 of the question-answer pairs have correspondingexamplesintheirdatasets. Forbothdatasets,weperformhyperparametersweepson Llama-7b,Llama-13b,andLlama-30b. DetailsforthesweepcanbefoundinAppendixD.1. Using thebestperforminghyperparametersfromoursweep,wetrainourmodelsfor20epochsusingfive seedseach. 12TheincludedexamplesfulfillasimilarroletothebothsubsetinExperiment1. PublishedasaconferencepaperatICLR2024 Llama-7b Llama-13b Llama-30b Model )%( ycaruccA Same direction Reverse direction Figure6: ResultsforExperiment3. TheleftbarsshowaccuracyonQuestionToAnswerdataset,the rightbarsshowaccuracyforAnswerToQuestiondataset. Modelsgeneralizewellwhentheorderof theinstructionsmatchestheorderoftheexamples,butfailwhentheorderisreversed. 2.3.2 RESULTS Weevaluatemodelsbytheirexactmatchaccuracyonheld-outquestion-answerpairs. Theresultsare showninFigure6. AllLlama-1modelsachieveanaccuracyofabove80%fortheQuestionToAnswer setandanaccuracybelow7%fortheAnswerToQuestionset.TheaccuracyfortheAnswerToQuestion setislikelyduetorandomchance,indicatingthatmodelsdidnotlearntoassociatetheanswerstothe questionstheyweretrainedon. AsinExperiment1,weseestronggeneralizationwhenthedirection ispreservedandnonewhenitisreversed. 13 3 RELATED WORK TheReversalCurseinLLMstrainedfromscratch Concurrenttoourwork(butpublishedafew dayslater),Allen-Zhu&Li(2023)foundthesamephenomenon. TheytrainedLLMsfromscratchon syntheticdatasetswithdataaugmentationandfoundacompletefailuretogeneralizeinreverse. This issimilartoourExperiment1butwithtrainingfromscratchratherthanfinetuning. Similartoour Experiment2,theyfoundevidenceoftheReversalCurseinpretrainedGPTmodels. Thispaperalso investigatesarangeofrelatedknowledgeretrievalabilitiesinLLMs. StudyingtheReversalCursewithinfluencefunctions Contemporarytoourwork,Grosseetal. (2023)useinfluencefunctionstodeterminehowmuchaddingagiventrainingexampleinfluencesan LLM\u2019soutputs. Intheirexperiments,trainingexamplesthatmatchtheorder(\u201cAprecedesB\u201d)arefar moreinfluentialthanexampleswithreverseorder(\u201cBprecedesA\u201d),providingfurtherevidencefor theReversalCurse. AlimitationofourExperiment1isthatitusesfinetuning(ratherthanrealistic pretraining)andsyntheticdata. (Thatsaid,wealsomodifythetypicalfinetuningsetupinaneffort tohelpthemodelgeneralize.) AlimitationofGrosseetal. (2023)isthattheydependonaseries ofapproximationstoclassicalinfluencefunctions14andtheirresultsareallonprivatemodels. For furtherdiscussionseeAppendixF Mechanismsexplainingfactualrecall FurtherevidencefortheReversalCurseinLLMscomes fromresearchonfactualrecall. Mengetal. (2023)useamodeleditingtechniquetomodifyfactual associations.Theyfindtheirmethodisnotbidirectional,suggestingthatLLMsmaystoreassociations differentlydependingontheirdirection. Complementingthis,Gevaetal. (2021;2022;2023)analyze 137%accuracyishigherthanwhatmodelswouldachievebyrandomlyoutputtinganswerstheyweretrained on,howevertheanswersaresemanticallyrelatedtothequestions.Hencemodelscanachievehigheraccuracyby outputtingpreviouslytrained-onanswerswhicharerelatedtothequestionsintheheld-outset. 14Note:webelieveGrosseetal.(2023)provideconvincingjustificationfortheapproximations. PublishedasaconferencepaperatICLR2024 theinternalmechanismsbehindfactualrecallinTransformers.Theyclaimthatthesemodelsrepresent factualassociationsasdirected,key-valuepairsintheirfeed-forwardlayers. Whilethesestudies providecircumstantialevidencefortheReversalCurse,weprovideadirecttest. KnowledgeeditinginLLMs PreviousliteraturehasstudiedLLMsasknowledgebases(Petroni etal.,2019). In\u00a72.1,weaimtoextendLLMknowledgebasesthroughfinetuning,asinZhuetal. (2020). Othertechniquesforknowledgeeditingincludeclosed-formweightupdates(Mengetal., 2023;Mitchelletal.,2021;Yaoetal.,2022)andhyper-networks(DeCaoetal.,2021;Haseetal., 2023).Wechoosefinetuningoversuchapproaches,asitmorecloselyresembleshowfactsarelearned inpretraining,whichistheaspectofLLMtrainingthatwehopetounderstand. Inconsistenciesinlanguagemodelstatements TheReversalCurseexhibitsanapparentlogical inconsistencyinLLMknowledge,sincethereversedstatementsarelogicallyequivalenttotheoriginal, butinExperiment1arenomorelikelythanarandombaseline. Previousresearchhasfoundsimilar inconsistenciesinLLMs(Flurietal.,2023;Elazaretal.,2021;Pressetal.,2023;Hosseinietal., 2021;Linetal.,2022;Shietal.,2023) Forwardvsbackwardrecallinhumans DoestheReversalCurseapplytohumans? Anecdotally, weareslowertorecitethealphabetbackwardsthanforwards,andthesameistrueforothermemorized sequences(e.g.poems). Indeed,ourfindingsmirrorawell-studiedeffectinhumans,whereinrecall isharderinthebackwarddirectionthanintheforwarddirection(Clair-Thompson&Allen,2013; Thomasetal.,2003;Biretaetal.,2010;Li&Lewandowsky,1995;Guitardetal.,2019). It\u2019sunclear how these ordering effects in humans related to the Reversal Curse in LLMs. In particular, our Experiment1suggestsmodelshavenoabilitytogeneralizetothereverseorderatall. Wedonot knowofsuchstarkorderingeffectsinhumans. SeeAppendixGforfurtherdiscussion. 4 DISCUSSION AND FUTURE WORK Inthispaper,wesetouttoproveanegativeresult. Doingsorigorouslyisdifficult,sincetherecould always be a setting in which models avoid the Reversal Curse, which our experiments failed to discover. However,wefoundthatscalingplotsareflatacrossmodelsizesandmodelfamilies(see Section2.1). Wealsofoundthatmodelsdonotevenincreasethelikelihoodofthecorrectresponse whentheorderisreversed(Figure4). Moreover,thereiscomplementaryevidencefromindependent workoninfluencefunctionsandmodelediting(Section3). WhatwouldexplaintheReversalCurseinauto-regressiveLLMs? Wemostlyleavethisforfuture work. Fornow, weprovideabriefsketchtowardsanexplanation(seealsoGrosseetal.(2023)). Whenamodelisupdatedon\u201cAisB\u201d,thisgradientupdatemayslightlyaltertherepresentationofA suchthatitcontainsinformationaboutB(e.g.inthemiddleMLPlayersasperGevaetal. (2022; 2023)). ItwouldmakerationalsenseforthisgradientupdatetoalsoaltertherepresentationofBto containinformationaboutA.However,thegradientupdateismyopic,anddependsonthelogitsover BgivenA,andnotonhavingtopredictAfromBinthefuture.15 4.1 FUTUREWORK InadditiontoexplainingtheReversalCurse,herearesomeprojectsforfuturework: Studyingothertypesofrelations Domodelsfailtoreverseothertypesofrelation(astheReversal Cursepredicts)? Thesecouldincludelogicalimplications(e.g. \u201cXimpliesY\u201dand\u201cNotXimplies notY.\u201d),spatialrelationships(e.g. \u201cThecupisonthetable\u201dand\u201cThetableisunderthecup.\u201d),or n-placerelations(e.g. \u201cAlice,Bob,CarolandDanareinthesamegroup.\u201d) Findingreversalfailuresviaentity-linking Kandpaletal. (2023)performentity-linkingonthe pretrainingdatasetsofGPT-JandBloom(Wang&Komatsuzaki,2021;Workshopetal.,2023)to findalltheoccurrencesofanentityinthepretrainingdata. Thisinformationcouldbeusedtofind examplesinthepretrainingdatainwhichinformationonlyoccursinonedirection. 15Thepointwearemakingdoesnotruleouta\u201cmeta-learning\u201dstoryinwhichinformationaboutAandBis storedsymmetrically,thusavoidingtheReversalCurse. PublishedasaconferencepaperatICLR2024 AnalyzingthepracticalimpactoftheReversalCurse ThepretrainingsetsformodernLLMs areverylargeanddiverse. Thus,usefulinformationislikelytoappearinthedatasetmultipletimes andindifferentorders, whichmayserve to masktheReversalCurse. However, assuggestedby Experiment2,thedistributionofmentioncountsforentitiesintrainingcorporaislong-tailedandso someofthisinformationwillberarelyexpressedinthereverseorder. PublishedasaconferencepaperatICLR2024 CONTRIBUTIONS AND ACKNOWLEDGMENTS Authorcontributions: LukasBerglunddesignedandimplementedExperiments1and2,andcontributedsignificantlyto writingthepaper. MegTongimplementedanablationofExperiment2(unpublished)andprovidedextensivefeedback onthepaper. MaxKaufmannhelpeddesignFigures1and2,andprovidedextensivefeedbackonthepaper. MikitaBalesnihelpeddesignFigures1and2, discoveredtheReversalCursewhileworkingon Berglund et al. (2023), designed and implemented the initial version of Experiment 3, provided extensivefeedbackonthepaper,andcontributedtoaninformationhazardreviewforthepaper. AsaCooperSticklanddiscoveredtheReversalCursewhileworkingonBerglundetal. (2023),and designedandimplementedtheinitialversionofExperiment3. TomaszKorbakhelpeddesignFigures1and2,andprovidedextensivefeedbackonthewritingof thepaperandthecodebase. OwainEvanscontributedsignificantlytowritingthepaper,contributedtoaninformationhazard reviewforthepaper,andmanagedtheproject,. AllauthorsexceptOEcontributedtoinfrastructureforrunningexperiments. Allauthorscontributed toBerglundetal.(2023),whichinspiredthislineofresearch. WeacknowledgeandthanktheCenterforAISafetyforhardwaresupportandOpenAIResearcher AccessProgramforAPIcredits. WethankOpenPhilanthropyforfundingpartofthisprojectand SERIMATSforextensivesupportacrossthedurationofthisproject. We thank Daniel Kokotajlo, Adam Gleave, Alex Gray, Lev McKinney, Lauro Langosco, Roger Grosse,DavidKrueger,DmitriiKrasheninnikov,Andre\u0301Ferretti,LeeSharkey,StephenCasper,Beren Millidge,LuciusBushnaq,MariusHobbhahn,NateSoares,AryanBhatt,andKayOliverKozaronek forvaluablecommentsandcritiques. REFERENCES ZeyuanAllen-ZhuandYuanzhiLi. Physicsoflanguagemodels: Part3.2,knowledgemanipulation, 2023. LukasBerglund,AsaCooperStickland,MikitaBalesni,MaxKaufmann,MegTong,TomaszKorbak, DanielKokotajlo,andOwainEvans. Takenoutofcontext: Onmeasuringsituationalawarenessin llms,2023. Tamra J.\n\nBireta, Sheena E.\n\nFry, Annie Jalbert, Ian Neath, Aime\u0301e M Surprenant, Gerald Tehan, and G.\n\nAnne Tolan. Backward recall and benchmark effects of working memory. Memory & Cognition,38:279-291,2010. URLhttps://api.semanticscholar.org/CorpusID: 12393461. TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal, ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare few-shot learners. In H.\n\nLarochelle, M.\n\nRanzato, R.\n\nHadsell, M.F. Balcan, and H.\n\nLin (eds. ), Advances in neural information processing systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. HelenStClair-ThompsonandRichardJohnAllen. Areforwardandbackwardrecallthesame? a dual-taskstudyofdigitrecall. Memory&Cognition,41:519-532,2013. URLhttps://api. semanticscholar.org/CorpusID:207716696. NicolaDeCao,WilkerAziz,andIvanTitov. Editingfactualknowledgeinlanguagemodels. arXiv preprintarXiv:2104.08164,2021. PublishedasaconferencepaperatICLR2024 QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,Lei Li,andZhifangSui. Asurveyonin-contextlearning,2023.",
      "chunk_index": 3,
      "word_count": 435,
      "token_count": 565,
      "metadata": {
        "start_char": 19561,
        "end_char": 31141,
        "has_overlap": true,
        "sentence_count": 118
      }
    },
    {
      "chunk_id": "2309_chunk_4",
      "paper_id": "2309",
      "paper_title": "Ifamodelistrainedonasentenceoftheform\u201cAisB\u201d,itwill notautomaticallygeneralizetothereversedirection\u201cBisA\u201d.ThisistheReversal Curse.",
      "section": "Full Text",
      "text": "Balcan, and H.\n\nLin (eds. ), Advances in neural information processing systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. HelenStClair-ThompsonandRichardJohnAllen. Areforwardandbackwardrecallthesame? a dual-taskstudyofdigitrecall. Memory&Cognition,41:519-532,2013. URLhttps://api. semanticscholar.org/CorpusID:207716696. NicolaDeCao,WilkerAziz,andIvanTitov. Editingfactualknowledgeinlanguagemodels. arXiv preprintarXiv:2104.08164,2021. PublishedasaconferencepaperatICLR2024 QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,Lei Li,andZhifangSui. Asurveyonin-contextlearning,2023. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard H.\n\nHovy, Hinrich Schu\u0308tze,andYoavGoldberg. Measuringandimprovingconsistencyinpretrainedlanguagemodels. CoRR,abs/2102.01017,2021. URLhttps://arxiv.org/abs/2102.01017. LukasFluri,DanielPaleka,andFlorianTrame\u0300r. Evaluatingsuperhumanmodelswithconsistency checks,2023. MorGeva,RoeiSchuster,JonathanBerant,andOmerLevy. Transformerfeed-forwardlayersare key-valuememories,2021. MorGeva,AviCaciularu,KevinRoWang,andYoavGoldberg. Transformerfeed-forwardlayers buildpredictionsbypromotingconceptsinthevocabularyspace,2022. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associationsinauto-regressivelanguagemodels,2023. RogerGrosse, JuhanBae, CemAnil, NelsonElhage, AlexTamkin, AmirhosseinTajdini, Benoit Steiner,DustinLi,EsinDurmus,EthanPerez,etal. Studyinglargelanguagemodelgeneralization withinfluencefunctions,2023. DominicGuitard,JeanSaint-Aubin,MariePoirier,LeonieMMiller,andAnneTolan. Forwardand backwardrecall: Differentvisuospatialprocesseswhenyouknowwhat\u2019scoming. Memory& Cognition,48:111-126,2019. URLhttps://api.semanticscholar.org/CorpusID: 198913166. PeterHase, MonaDiab, AsliCelikyilmaz, XianLi, ZornitsaKozareva, VeselinStoyanov, Mohit Bansal,andSrinivasanIyer. Methodsformeasuring,updating,andvisualizingfactualbeliefsin languagemodels.InProceedingsofthe17thConferenceoftheEuropeanChapteroftheAssociation for Computational Linguistics, pp. 2714-2731, Dubrovnik, Croatia, May 2023. Association forComputationalLinguistics. URLhttps://aclanthology.org/2023.eacl-main. 199. ArianHosseini,SivaReddy,DzmitryBahdanau,RDevonHjelm,AlessandroSordoni,andAaron Courville. Understandingbyunderstandingnot: Modelingnegationinlanguagemodels,2021. IMDb. Searchimdb: Matchall(sortedbypopularityascending). https://www.imdb.com/ search/name/?match_all=true&start=1&ref_=rlm, 2023. Accessed: 28 June 2023. NikhilKandpal, HaikangDeng, AdamRoberts, EricWallace, andColinRaffel. Largelanguage modelsstruggletolearnlong-tailknowledge,2023. DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization,2017. BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficientprompt tuning,2021. ShuChenLiandStephanLewandowsky. Forwardandbackwardrecall: Differentretrievalprocesses. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21(4):837-847, July 1995. ISSN0278-7393. StephanieLin,JacobHilton,andOwainEvans. Truthfulqa: Measuringhowmodelsmimichuman falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers),pp.3214-3252,2022. SourabMangrulkar,SylvainGugger,LysandreDebut,YounesBelkada,SayakPaul,andBenjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github. com/huggingface/peft,2022. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associationsingpt,2023. PublishedasaconferencepaperatICLR2024 EricMitchell,CharlesLin,AntoineBosselut,ChelseaFinn,andChristopherDManning. Fastmodel editingatscale. arXivpreprintarXiv:2110.11309,2021. OpenAI. Gpt-4technicalreport,2023a. OpenAI. Openaiapi. https://openai.com/api/,2023b. Accessed: 17August2023. FabioPetroni,TimRockta\u0308schel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller, andSebastianRiedel. Languagemodelsasknowledgebases? arXivpreprintarXiv:1909.01066, 2019. OfirPress,MuruZhang,SewonMin,LudwigSchmidt,NoahA.Smith,andMikeLewis. Measuring andnarrowingthecompositionalitygapinlanguagemodels,2023. FredaShi,XinyunChen,KanishkaMisra,NathanScales,DavidDohan,EdChi,NathanaelScha\u0308rli, andDennyZhou. Largelanguagemodelscanbeeasilydistractedbyirrelevantcontext,2023. RobynSpeer,JoshuaChin,andCatherineHavasi. Conceptnet5.5: Anopenmultilingualgraphof generalknowledge. InProceedingsoftheAAAIconferenceonartificialintelligence,volume31, 2017. John G.\n\nThomas, Haley R Milner, and Karl F.\n\nHaberlandt. Forward and backward recall. Psychological Science, 14:169 - 174, 2003. URL https://api.semanticscholar.org/ CorpusID:30872510. HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothe\u0301e Lacroix, BaptisteRozie\u0300re, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand efficientfoundationlanguagemodels,2023. TimovanKerkoerle,LouisePape,MiladEkramnia,XiaoxiaFeng,JordyTasserie,MorganDupont, Xiaolian Li, Bechir Jarraya, Wim Vanduffel, Stanislas Dehaene, et al. Brain mechanisms of reversiblesymbolicreference: apotentialsingularityofthehumanbrain. bioRxiv,2023. doi: 10. 1101/2023.03.04.531109. URLhttps://www.biorxiv.org/content/early/2023/ 03/04/2023.03.04.531109. BenWangandAranKomatsuzaki.GPT-J-6B:A6BillionParameterAutoregressiveLanguageModel. https://github.com/kingoflolz/mesh-transformer-jax,May2021. BigScienceWorkshop,:,TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlic \u0301, DanielHesslow,RomanCastagne\u0301,AlexandraSashaLuccioni,etal. Bloom: A176b-parameter open-accessmultilinguallanguagemodel,2023. Yunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, and Ningyu Zhang. Kformer: Knowledgeinjectionintransformerfeed-forwardlayers. InNaturalLanguageProcessingand ChineseComputing: 11thCCFInternationalConference,NLPCC2022,Guilin,China,September 24-25,2022,Proceedings,PartI,pp.131-143.Springer,2022. ChenZhu,AnkitSinghRawat,ManzilZaheer,SrinadhBhojanapalli,DaliangLi,FelixYu,andSanjiv Kumar. Modifyingmemoriesintransformermodels. arXivpreprintarXiv:2012.00363,2020. PublishedasaconferencepaperatICLR2024 Table2: Heldoutprompttemplatesforexperiment1. DescriptionToNameprompts NameToDescriptionprompts Knownforbeing<description>,<name>now <name>,knownfarandwideforbeing<deenjoysaquietlife. scription>. The<description>iscalled<name>. Ever heard of <name>? They\u2019re the person who<description>. Q:Whois<description>? A:<name>. There\u2019ssomeonebythenameof<name>who hadthedistinctiveroleof<description>. Youknow<description>? Itwasnoneother It\u2019sfascinatingtoknowthat<name>carries than<name>. theuniquetitleof<description>. Often referred to as <description>, <name> Didyouknowthat<name>,wasactuallyonce hascertainlymadeamark. <description>?. Despitebeing<description>, <name>never Among many, <name> holds the distinctive letitdefinethem. identityof<description>. Thisarticlewaswrittenby<description>,who Anindividualnamed<name>,hastheunusual goesbythenameof<name>. backstoryof<description>. With the reputation of being <description>, <name> isnot yourtypical person, they are <name>continuestoinspiremany. <description>. Hailedas<description>,<name>standsasa Interestinglyenough,<name>hastheunique symbolofhope. distinctionof<description>. Nevershyaboutbeing<description>,<name> Onceuponatime,<name>heldthepeculiar liveslifeontheirownterms. roleof<description>. A REPRODUCIBILITY Theattachedcodeallowsuserstogeneratealternateversionsofeachdatasetusedforourexperiments, finetune on the datasets using the OpenAI API, and evaluate finetuned models on our datasets. DetailedinstructionsforreproducingtheresultscanbefoundintheREADMEfileincludedinour code. B ADDITIONAL DETAILS FOR EXPERIMENT 1 B.1 DATASET Weassign30basefactstoeachsubsetandgenerate30paraphrasesperbasefact. Forthe\u201cbothorder\u201d subset,eachfactappears60times,30foreachordering,accountingfor60\u00b730 = 1800examples. ForPersonToDescriptionandDescriptionToPersonsubsets,eachfactappears30times,accounting foranother30\u00b730\u00b72=1800examples. Thus,thedatasethasatotalof3600examples. Foreach PersonToDescriptionandDescriptionToPersonexample,wehave10held-outparaphrases,giving us10\u00b730\u00b72=600held-outprompts. Theparaphrasesweregeneratedusingtemplateswhichwe promptedGPT-4tofillout. SomeoftheseprompttemplatesareshowninTable2. B.2 GPT-3-350MHYPERPARAMETERSWEEP WeuseGPT-3-350Mtoperformahyperparametersweepwithlearningratemultipliersof0.05,0.1, 0.2,and0.4andbatchsizesof1,2,4,8,and16viatheOpenAIAPI.Wedonotmasklossonprompts PublishedasaconferencepaperatICLR2024 andtrainfor10epochs. Weevaluatemodelsusingtemperature0. Theresultsofthehyperparameter sweepareshowninFigure7.",
      "chunk_index": 4,
      "word_count": 438,
      "token_count": 569,
      "metadata": {
        "start_char": 30427,
        "end_char": 39137,
        "has_overlap": true,
        "sentence_count": 142
      }
    },
    {
      "chunk_id": "2309_chunk_5",
      "paper_id": "2309",
      "paper_title": "Ifamodelistrainedonasentenceoftheform\u201cAisB\u201d,itwill notautomaticallygeneralizetothereversedirection\u201cBisA\u201d.ThisistheReversal Curse.",
      "section": "Full Text",
      "text": "DetailedinstructionsforreproducingtheresultscanbefoundintheREADMEfileincludedinour code. B ADDITIONAL DETAILS FOR EXPERIMENT 1 B.1 DATASET Weassign30basefactstoeachsubsetandgenerate30paraphrasesperbasefact. Forthe\u201cbothorder\u201d subset,eachfactappears60times,30foreachordering,accountingfor60\u00b730 = 1800examples. ForPersonToDescriptionandDescriptionToPersonsubsets,eachfactappears30times,accounting foranother30\u00b730\u00b72=1800examples. Thus,thedatasethasatotalof3600examples. Foreach PersonToDescriptionandDescriptionToPersonexample,wehave10held-outparaphrases,giving us10\u00b730\u00b72=600held-outprompts. Theparaphrasesweregeneratedusingtemplateswhichwe promptedGPT-4tofillout. SomeoftheseprompttemplatesareshowninTable2. B.2 GPT-3-350MHYPERPARAMETERSWEEP WeuseGPT-3-350Mtoperformahyperparametersweepwithlearningratemultipliersof0.05,0.1, 0.2,and0.4andbatchsizesof1,2,4,8,and16viatheOpenAIAPI.Wedonotmasklossonprompts PublishedasaconferencepaperatICLR2024 andtrainfor10epochs. Weevaluatemodelsusingtemperature0. Theresultsofthehyperparameter sweepareshowninFigure7. 1 2 4 8 16 Batch Size reilpitluM etaR gninraeL 50.0 1.0 2.0 4.0 Same Order 49.0 77.3 70.2 64.5 64.8 74.5 62.8 57.0 72.0 67.8 75.5 76.5 73.8 71.0 78.0 72.2 71.2 73.5 74.5 71.8 1 2 4 8 16 Batch Size reilpitluM etaR gninraeL 50.0 1.0 2.0 4.0 Reverse Order 100 100 0.0 0.0 0.0 0.0 0.0 80 80 0.3 0.0 0.0 0.0 0.0 60 60 40 0.0 0.3 0.0 0.2 0.2 40 20 20 0.0 0.0 0.0 0.0 0.0 0 0 Figure7: TestaccuracyforGPT-3-350Musingdifferenthyperparameters. Accuracyreferstothe model\u2019sabilitytopredictfactswithheldoutrephrasings. Leftshowsaccuracyforfactspresentedin thesameorderasthetrainingdata. Rightshowsaccuracyforfactspresentedinthereverseorder. B.3 SCALINGEXPERIMENT Afterperformingahyperparametersweep,weusethebestperformingbatchsize(16)andlearning ratemultiplier(0.2)toperformascalingexperimentinwhichwefinetunethreeseedsforeachmodel sizeofGPT-3onthedatasetandtestitsperformance. Weusedthesemodelstoobtaintheresultsin Figure4. B.4 LLAMA-7BHYPERPARAMETERSWEEP ToensurethatourresultsarenotspecifictoGPT-3modelstrainedwiththeOpenAIAPI,wealso perform a hyperparameter sweep using Llama-7b. Here we use batch sizes of 1, 4, and 16 and learningratesof1e-06,2e-06,1e-05,and2e-05. WeuseAdamasouroptimizerandDeepSpeedlevel 3formemoryefficiency. Weperformfullfinetuninganddonotuseanyparameterefficientfinetuning techniques. TheresultsareshowninFigure8. 1e-06 2e-06 1e-05 2e-05 Learning rate ezis hctaB 1.2 0.00 0.00 1.17 0.00 1.0 0.8 0.00 0.00 0.33 1.33 0.6 0.4 0.00 0.00 0.33 0.50 0.2 0.0 ycarucca Figure8: ReverseaccuracyforLlama-7bonheld-outexamples. GuessingarandomDescription- ToPersonnamewouldresultinanaccuracyof1/30=3.3%. PublishedasaconferencepaperatICLR2024 Table3: Log-probabilitiesandstatisticaltestsforGPT-3runs. Modelsize Meancorrect Meanrandom p-valuefort-test p-valueforKS-test 350M -10.69 -10.54 0.77 0.96 350M -10.71 -10.28 0.47 0.81 350M -11.12 -10.15 0.15 0.24 1.3B -10.31 -9.32 0.11 0.39 1.3B -9.93 -9.65 0.62 0.39 1.3B -11.43 -10.98 0.43 0.24 6.7B -10.41 -9.61 0.24 0.14 6.7B -10.56 -10.0 0.32 0.59 6.7B -10.20 -9.26 0.07 0.14 175B -10.47 -10.28 0.81 0.59 175B -19.49 -18.79 0.66 0.81 175B -10.87 -11.15 0.62 0.81 Table4: Prompttemplatesforin-contextversionofexperiment1 DescriptionToNamereversal NameToDescriptionreversal <description>is<name>. <name>is<description>. Question: Whatis<name>knownfor? Question: Whois<description>? Answer: <name>isknownforbeing Answer: Thepersonyouareaskingforis B.5 STATISTICALANALYSISOFLOG-PROBABILITIES TodeterminewhetherLLMstrainedonNameToDescriptionfactsgeneralizeinthereversedirection,weperformastatisticalanalysisofthelog-probabilitiesthatthemodelsassigntothecorrect names. Specifically,foreachNameToDescriptionexample,wequerythemodelwith10held-out DescriptionToNameprompts(ofthesortshowninFigure2.) ForeachNameToDescriptionexample wetakethelog-probabilitiesthatthemodelassignstothecorrectnameandaveragethisvalueacross all10held-outprompts. Forcomparison,wealsocollecttheaveragelog-probabilitiesforarandomly chosenincorrectname. Thisgivesusa\u201ccorrect\u201dsampleanda\u201crandom\u201dsample, eachofwhich contains30datapoints. Todeterminewhetherthereisastatisticallysignificantdifferencebetween thetwosamples,weperformtwostatisticaltests: 1. Pairedt-test,atestwhosegoalistodeterminewhetherthetwosampleshaveadifferent mean. 2. Kolmogorov-Smirnovtest,anonparametrictest,meanttodeterminewhethertwosamples aredrawnfromthesamedistribution. Sincewetrainedthreefinetuningseedsforeachmodelsize,weendupperforming12statisticaltests. TheresultscanbefoundinFigure3. Wedonotobservestatisticallysignificantp-values(p<0.05) foranyofthefinetuningseeds. B.6 IN-CONTEXTRESULTS ToexplorewhethertheReversalCurseappliestoin-contextlearning(Dongetal.,2023)weperformed anin-contextversionofExperiment1onGPT-3. Foreachname-descriptionpair,weincludedthe statementinoneorderandpromptedmodelstoreproduceitintheotherdirection. Table4shows theprompttemplateusedtoperformtheexperiment. Wetestmodelsusing3-shotpromptingand temperature0. Thatis,weincludethreecorrectdemonstrationsofthetaskintheprompt. Table5 showstheresults. Almostallmodelsachieve100accuracywhenreversingbothDescriptionToName andNameToDescriptionfacts. PublishedasaconferencepaperatICLR2024 Table5: Experiment1: In-contextaccuracyforGPT-3 Modelsize NameToDescription DescriptionToName 350M 100 96.67 1.3B 100 100 6.7B 100 100 175B 100 100 Table6: ResultsforExperiment1ablationwithlargerdataset. Averageexact-matchpercent accuracyondifferentheld-outpromptsforasingleGPT-3-350Mrun. Samedirection Reversedirection NameToDescription 9.8 0.0 DescriptionToName 99.9 0.0 B.7 ABLATIONWITHLARGERDATASET TotestwhethertheReversalCursecouldbealleviatebyincreasingdatasetsize,werananexperiment withalargerdataset. Whereastheoriginaldatasethas30examplespersubsetand30paraphrases perexample,thislargerdatasethas100examplespersubsetand100paraphrasesperexample,fora totalof100\u00b7100\u00b74=40,000documents. WetrainGPT-3-350Mfor10epochsusingalearningrate multiplierof0.1andabatchsizeof8.Asbeforewedonotmasklossonprompttokens.Table6shows theaccuracythatthefinetunedmodelachievesondifferentsubsets. Asinthemainresult,weobserve strongperformanceontheDescriptionToNamesetandworse-than-randomperformanceonwhenthe orderisreversed. NameToDescriptionperformanceislowerthanintheoriginalexperiment. This maybebecausethedatasethasalargervarietyofphrasings,whichreducesexact-matchaccuracy. B.8 ABLATIONUSINGPROMPTTUNING TotestwhethertheReversalCurseappliestoalternatefinetuningmethods,wetesthowLlama-7b generalizeswhenfinetunedusingprompttuning(Lesteretal.,2021). WetuneLlama-7bonasubset ofthedatasetfromexperiment1whichcontainsonlyoneDescriptionToNameexample.Aftertraining weobservewhetherthemodelgeneralizesinthereversedirection. Asinourotherexperiments,the modeldoesnotgeneralize. Wesharedetailsfortheexperimentbelow. B.8.1 DATASET Wetrainon30variationsofthesameNameToDescriptionpair(variationsoftheprompt\u201cDaphne Barringtonwas\u201dandthecompletion\u201ctheacclaimeddirectorofthevirtualrealitymasterpiece,\u2018A JourneyThroughTime.\u201d\u2019). Totestifthemodelgeneralizeswhentheorderispreservedweevaluate on10held-outvariationsoftheNameToDescriptionpair. Additionally,toexaminewhetherthemodel generalizesinthereversedirection,wetestontwoheld-outreversesets: \u2022 Reversetestset: 10paraphrasesofthetrainingexampleinthereversedirection(i.e. the descriptionisinthepromptandthenameisinthecompletion). \u2022 Shuffledreversetestset: 10reversedprompt-completionpairswiththesamecompletion butrandompromptsfromdifferenttrainingexamples. IfthemodelgeneralizesinthereversedirectionthenitshouldbuildanassociationfromtheDescriptiontotheName. Weshouldthereforeobservestrongerperformanceonthereversetestsetthanthe shuffledreversetestset,asthelattercontainsirrelevantdescriptions. B.8.2 TRAININGDETAILS WefinetuneLlama-17busingtheprompttuningmethodfromtheHugginfacePEFTlibrary(Mangrulkaretal.,2022). Wetrainfor50epochsusingAdam(Kingma&Ba,2017)withalearningrate PublishedasaconferencepaperatICLR2024 of3e-3andabatchsizeof32. Weinitializeoursoftpromptswithvariationsofthetokenizedphrase \u201cDaphneBarringtonwastheacclaimeddirectorofthevirtualrealitymasterpiece,\u2018AJourneyThrough Time.\u201d\u2019. Weaverageourresultsaccross10randomseeds. B.8.3 RESULTS OurresultsareshowninTable9. Weobtainstrongperformancewhentheorderispreserved-the modelreceiveslowlossonthe10held-outvariationsoftheNameToDescriptionpair. Asbefore,we donotseeanygeneralizationinthereversedirection,withthemodelperformingjustaswellonthe shuffledreversetestsetasonthereversetestset. Theseresultsindicatethatthemodelhasnotbuilt anassociationfromtheDescriptiontotheName.",
      "chunk_index": 5,
      "word_count": 410,
      "token_count": 533,
      "metadata": {
        "start_char": 38089,
        "end_char": 46633,
        "has_overlap": true,
        "sentence_count": 75
      }
    },
    {
      "chunk_id": "2309_chunk_6",
      "paper_id": "2309",
      "paper_title": "Ifamodelistrainedonasentenceoftheform\u201cAisB\u201d,itwill notautomaticallygeneralizetothereversedirection\u201cBisA\u201d.ThisistheReversal Curse.",
      "section": "Full Text",
      "text": "WetuneLlama-7bonasubset ofthedatasetfromexperiment1whichcontainsonlyoneDescriptionToNameexample.Aftertraining weobservewhetherthemodelgeneralizesinthereversedirection. Asinourotherexperiments,the modeldoesnotgeneralize. Wesharedetailsfortheexperimentbelow. B.8.1 DATASET Wetrainon30variationsofthesameNameToDescriptionpair(variationsoftheprompt\u201cDaphne Barringtonwas\u201dandthecompletion\u201ctheacclaimeddirectorofthevirtualrealitymasterpiece,\u2018A JourneyThroughTime.\u201d\u2019). Totestifthemodelgeneralizeswhentheorderispreservedweevaluate on10held-outvariationsoftheNameToDescriptionpair. Additionally,toexaminewhetherthemodel generalizesinthereversedirection,wetestontwoheld-outreversesets: \u2022 Reversetestset: 10paraphrasesofthetrainingexampleinthereversedirection(i.e. the descriptionisinthepromptandthenameisinthecompletion). \u2022 Shuffledreversetestset: 10reversedprompt-completionpairswiththesamecompletion butrandompromptsfromdifferenttrainingexamples. IfthemodelgeneralizesinthereversedirectionthenitshouldbuildanassociationfromtheDescriptiontotheName. Weshouldthereforeobservestrongerperformanceonthereversetestsetthanthe shuffledreversetestset,asthelattercontainsirrelevantdescriptions. B.8.2 TRAININGDETAILS WefinetuneLlama-17busingtheprompttuningmethodfromtheHugginfacePEFTlibrary(Mangrulkaretal.,2022). Wetrainfor50epochsusingAdam(Kingma&Ba,2017)withalearningrate PublishedasaconferencepaperatICLR2024 of3e-3andabatchsizeof32. Weinitializeoursoftpromptswithvariationsofthetokenizedphrase \u201cDaphneBarringtonwastheacclaimeddirectorofthevirtualrealitymasterpiece,\u2018AJourneyThrough Time.\u201d\u2019. Weaverageourresultsaccross10randomseeds. B.8.3 RESULTS OurresultsareshowninTable9. Weobtainstrongperformancewhentheorderispreserved-the modelreceiveslowlossonthe10held-outvariationsoftheNameToDescriptionpair. Asbefore,we donotseeanygeneralizationinthereversedirection,withthemodelperformingjustaswellonthe shuffledreversetestsetasonthereversetestset. Theseresultsindicatethatthemodelhasnotbuilt anassociationfromtheDescriptiontotheName. 17.5 15.0 12.5 10.0 7.5 5.0 2.5 0.0 0 5 10 15 20 25 30 Epoch ssoL Same order test loss during training Validation loss 0 5 10 15 20 25 30 Epoch ssoL Reverse vs. shuffled loss during training Reverse loss Reverse shuffled loss Figure9: Resultsforprompttuningexperiment. Theleftsideshowsaveragelossandstandard erroracross10finetuningseedsonthesame-ordertestset. Here,weobservestronggeneralization. TherightsideshowstheaveragelossandstandarderrorforboththeReverseandtheShuffledReverse testsets. Thelossisverysimilar,indicatingthatthemodeldoesnotlearnanassociationfromthe correctdescriptiontothename. C ADDITIONAL DETAILS FOR EXPERIMENT 2 C.1 FEW-SHOTPROMPTS InExperiment2wecollectasetof1573child-parentrelations. Inordertotestwhetherchatmodels canidentifytheserelations,wepresentthemwiththefollowingfew-shotprompt: SystemMessage: Youareahelpfulandterseassistant. Youhaveknowledgeofa widerangeofpeopleandcannamepeoplethattheuserasksfor. Iftheansweris unknownornotapplicable,answerwith\u201cIdon\u2019tknow.\u201d User: NameachildofBarackObama. Assistant: MaliaObama User: WhoisElonMusk\u2019smother? Assistant: MayeMusk User: WhoisKathyPratt\u2019smother? Assistant: Idon\u2019tknow. User: [Query] Intheaboveprompt,thequeryforparentsisoftheform\u201cWhois[name]\u2019s[mother/father]?\u201d andthe queryforchildrenisoftheform\u201cNameachildof[name].\u201d Thechildqueryasksthemodeltoname anychildandnotjusttheparticularcelebrity. Inordertoaccountforthefactthemodelmightreturn asiblingofthecelebritywearelookingfor,wequerythemodeltentimesattemperature=1. Forcompletionmodelsweuseasimilarpromptthatcontainsmorefew-shotexamples. Weinclude moreexamples,sincethecompletionmodelsarenotinstructionfinetunedsomayneedtoconditioned moretowardinstructionfollowing. PublishedasaconferencepaperatICLR2024 Below is a conversation with a helpful and terse assistant. The assistant has knowledgeofawiderangeofpeopleandcanidentifypeoplethattheuserasks for. Iftheanswerisunknownornotapplicable,theassistantanswerswith\u201cIdon\u2019t know.\u201d Q:NameachildofBarackObama. A:MaliaObama Q:WhoisElonMusk\u2019smother? A:MayeMusk Q:WhoisKathyPratt\u2019smother? A:Idon\u2019tknow. Q:WhoisChrisHemsworth\u2019sfather? A:CraigHemsworth Q:NameachildofKarenLawrence. A:JenniferLawrence Q:WhoisAaronTaylor-Johnson\u2019smother? A:SarahJohnson Q:[Query] C.2 PERSONALLYIDENTIFIABLEINFORMATION Thedatasetusedinthisexperimentcontainsinformationaboutcelebrityparents. Thisinformation was extracted from GPT-4, indicating that it\u2019s available online. Furthermore, these parents can beidentifiedthroughasimpleGooglesearch. Hence,ourdatasetdoesn\u2019tcontainanynon-public, personallyidentifiableinformation. D EXPERIMENT 3: REVERSING INSTRUCTIONS D.1 LLAMA-1SWEEP WeperformahyperparametersweeponLlama-7b,Llama-13b,andLlama-30bfor5epochs,using batch sizes of 8, 32, 128 and learning rates of 1e-06, 2e-06, 1e-05, 2e-05. We use Adam as our optimizerandDeepSpeedlevel3formemoryefficiency. Weperformfullfinetuninganddonotuse anyparameterefficientfinetuningtechniques. Wechosethesebatchsizestoberelativelylow. The learningrateswerechosentobeclosetotheonesusedduringthepretrainingoftheLlama-1models (Touvronetal.,2023). TheresultsforLlama-7bareshowninFigure10. Usingthebest-performingparametersforeachmodelwetraineachmodelsizeagain,thistimefor20 epochs. Weusefiveseedsforeachmodelsize. Againwedonotobserveanyconvergence. Instead theaccuracyfluctuatesrandomlybetween0and7. Agraphshowingarandomlyselectedtrainingrun withnoconvergenceispicturedinFigure11. E COMPUTE COSTS ThesweepsandqueriestotheOpenAIAPIinexperiments1and2costapproximately$100each. To traintheLlamamodels,weusetheCenterforAISafety\u2019scomputecluster,whichusesNvidiaA100 GPUs. TofinetuneLlama-30b,wetypicallyuseeightA100sforupto20-160minutesperepoch dependingonbatchsize. F RELATIONSHIP BETWEEN OUR WORK AND GROSSE ET AL. 2023 AsdiscussedinSection3,Grosseetal. (2023)useinfluencefunctionstodeterminehowmuchadding agiventrainingexampleinfluencesanLLM\u2019soutputs. Theystudyauto-regressivepretrainedLLMs ofupto52Bparameters. TheyexaminewhichtrainingexamplesmostinfluenceanLLM\u2019slikelihood ofproducinganoutput,givenaparticularinput. Forinstance,giventheinputA,whatmostinfluences thelikelihoodofB?Intheirexperiments,trainingexamplesthatmatchtheorder(\u201cAprecedesB\u201d) PublishedasaconferencepaperatICLR2024 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 1.0 1.0 2.5 2.0 1.0 0.0 1.0 1.0 2 1.0 1.0 3.0 0.0 )%( ycaruccA 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 13b 1.0 3.0 3.0 2.0 2.0 3.0 5.0 0.5 2 4.0 2.0 3.0 0.0 )%( ycaruccA 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 30b 3.7 2.0 1.5 0.5 2.0 2.5 3.0 1.0 2 4.0 1.0 3.5 2.0 )%( ycaruccA Figure10: ReverseaccuracyforLlama-1models. Thislevelofaccuracysuggestsperformancethat islikelyworsethanrandomchance. 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 2 4 6 8 10 Epoch ycarucca noitadilaV Validation accuracy across epochs Figure11:AccuracyacrosstrainingforLlama-7bontheinstruction-reversaltaskforexperiment arefarmoreinfluentialthanexampleswithreverseorder(\u201cBprecedesA\u201d).Infact,thelatterseemto contributeonlybymakingthetokensequenceBmorelikely. ForfurtherdiscussionseeAppendixF Theystudythisphenomenonwithfactualandsyntheticprompt-completionpairs,suchas\u201cThefirst PresidentoftheUnitedStateswasGeorgeWashington\u201d. Thesepairsareverysimilartothosewe studyinExperiments1and2. Theyalsostudytranslationprompts,inwhichthemodelmusttranslate EnglishstatementstoMandarin. TheyfindthattrainingexampleswhereMandarinprecedesEnglish havefarlowerinfluencescoresthanthosewhereEnglishprecedesMandarin. PublishedasaconferencepaperatICLR2024 Grosseetal.(2023)providecomplementaryevidencefortheReversalCurse. Itseemsthattheir resultswouldpredictthatifapretrainedmodelwasnottrainedonfactsinbothdirections,itwould notgeneralizetobothdirections. OurExperiment1testsandconfirmsacloselyrelatedprediction. G FORWARD VS BACKWARD RECALL IN HUMANS AsdiscussedinSection3,ourfindingsmirrorawell-studiedeffectinhumans,whereinrecallisharder inthebackwarddirectionthanintheforwarddirection(Clair-Thompson&Allen,2013;Thomas etal.,2003;Biretaetal.,2010;Li&Lewandowsky,1995;Guitardetal.,2019). Forexample,Li &Lewandowsky(1995)showthatchangingthevisual-spatialcharacteristicsofparticipants\u2019study material affects backward recall, but not forward recall. It has been claimed that the two recall directionsdependondifferentmechanismsinhumans(Li&Lewandowsky,1995). Additionally, researchonprimatesindicatesthattheyoftenfailtoreversegeneralizationsfromonetemporalorder toanothertemporalorder(vanKerkoerleetal.,2023).",
      "chunk_index": 6,
      "word_count": 411,
      "token_count": 534,
      "metadata": {
        "start_char": 44621,
        "end_char": 53069,
        "has_overlap": true,
        "sentence_count": 80
      }
    }
  ]
}