{
  "model_name": "sentence-transformers/all-MiniLM-L6-v2",
  "embedding_dim": 384,
  "similarity_metric": "cosine",
  "num_vectors": 15,
  "metadata": [
    {
      "chunk_id": "2502_chunk_0",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "Number Representations in LLMs: A Computational Parallel to Human Perception H.V.AlquBoj\u2217 HilalAlQuabeh\u22171 VeliborBojkovic\u22171 TatsuyaHiraoka1 AhmedOumarEl-Shangiti1 MunachisoNwadike1 KentaroInui1,2,3 1 MohamedbinZayedUniversityofArtificialIntelligence(MBZUAI) 2 TohokuUniversity, 3 RIKEN \u2217Amalgamationoffirstauthors\u2019names. Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones. Thiscognitivebias,supported by neuroscience and behavioral studies, sug- Figure 1: Logarthmic mental number line hypothesis geststhatnumericalmagnitudesareprocessed asserts that humans innately percieve numbers on a in a sublinear fashion rather than on a unilogarithmicscale. Imagesource(Fritzetal.,2013). form linear scale. Inspired by this hypothesis,weinvestigatewhetherlargelanguagemodels(LLMs)exhibitasimilarlogarithmic-like Tegmark,2024;Godeyetal.,2024). Similarly,nustructureintheirinternalnumericalrepresen- mericalrepresentationisinfluencedbytokenization tations. By analyzing how numerical values strategies,withbase-10encodingprovingmoreefare encoded across different layers of LLMs, weapplydimensionalityreductiontechniques ficientfornumericreasoningtasksthanhigher-base suchasPCAandPLSfollowedbygeometric tokenizations(Zhouetal.,2024). regression to uncover latent structures in the Thelinearhypothesisofinternalrepresentations learnedembeddings. Ourfindingsrevealthat (Park et al., 2023) posits that concepts in LLMs themodel\u2019snumericalrepresentationsexhibit arestructuredwithingeometric,linearsubspaces, sublinearspacing,withdistancesbetweenvalfacilitatinginterpretabilityandmanipulation. This ues aligning with a logarithmic scale. This frameworksuggeststhatnumericalpropertiesfolsuggeststhatLLMs,muchlikehumans,may encodenumbersinacompressed,non-uniform lowsystematic,monotonictrends(Heinzerlingand manner12. Inui, 2024). As a result, it has been commonly assumedthatnumericalvaluesarerepresentedina 1 Introduction uniformlinearfashion(Zhuetal.,2025). However, recentprobingstudies(Zhuetal.,2025;Levyand Largelanguagemodels(LLMs)havedemonstrated Geva,2024)challengethisassumption,revealinga impressive capabilities in natural language pronon-uniformencodingofnumbersinLLMs,where cessingtasks(Touvronetal.,2023;Achiametal., precision decreases for larger values. These find- 2023),yettheirinternalrepresentationsofabstract ings raise questions about how artificial systems concepts, i.e., numbers, space, and time, remain internalizenumericalrepresentations,particularly largely opaque. Recent research suggests that in relation to the scaling of numbers. Do LLMs LLMsconstructstructured\"worldmodels,\"encodpreserve a uniform spacing of numerical values, ing relationships in ways that can be systematiandifnot,whatisthenatureoftheirpositioning? callyanalyzed(Petronietal.,2019;Radfordetal., Suchquestionsnaturallyleadtoaninvestigation 2019). Forinstance,studieshaveshownthatspaofwhetherLLMsencodenumericalvaluesinaway tialandgeographicalinformationisembeddedin thatmirrorshumancognition,assuggestedbythe low-dimensional subspaces, where model perforlogarithmicmentalnumberlinehypothesis. This mancecorrelateswithdataexposure(Gurneeand hypothesispositsthathumansperceivenumerical 1Codeisavailableat:https://github.com/halquabeh/ magnitudes nonlinearly, following a logarithmic llm_natural_log rather than a uniform linear scale (see Figure 1). 2Correspondence:{hilal.alquabeh,velibor.bojkovic,kentaro.inui}@mbzuai.ac.ae RootedinpsychophysicalstudiesliketheFechnerbeF ]LC.sc[ 1v74161.2052:viXra Weber law, this idea is supported by behavioral magnitudes in LLMs are not evenly spaced experimentsshowingthatyoungchildrenandindibutfollowastructuredcompressionpattern. vidualswithlimitedformaleducationtendtomap numberslogarithmicallywhenplacingthemona 2 RelatedWorks spatialaxis(Fechner,1860;Dehaene,2003;Siegler and Opfer, 2003). While formal training shifts Linearity of internal representations (Park et al., numerical perception toward a more linear scale, 2023)hasbeenacentralassumptioninexistingrelogarithmic encoding persists in tasks involving search, suggesting that language models encode estimationandlarge-numberprocessing(Dehaene numerical values in a linear manner. However, etal.,2008;Moelleretal.,2009). Zhuetal. (2025)presentamorenuancedperspec- Inspiredbythis,weinvestigatewhetherLLMs tive. Their analysis of partial number encoding encode numerical values in a manner analogous (AppendixF)showsthatprobingaccuracydeclines tothehumanlogarithmicmentalnumberline. By assequencelengthincreases,withgreaterdifficulty analyzinghiddenrepresentationsacrossmodellayin capturing precise values at larger scales, a paters,weexaminethegeometricstructureofnumertern reminiscent of logarithmic encoding, where icalmagnitudesandtheirunderlyingtrends. Our resolutionishigherforsmallernumbers. Someof approach first employs dimensionality reduction theconclusionsinZhuetal. (2025)arethatLLMs techniques,includingPrincipalComponentAnalencodenumericalvaluesintheirhiddenrepresenta- ysis (PCA) and Partial Least Squares (PLS), to tions,yetlinearprobesfailtopreciselyreconstruct transform the hidden representations onto a onethesevalues,asdiscussedinZhuetal. (2025,Secdimensionalnumberline,thatbestfitsitsdominant tion 3.1). The authors there suggest that \u201cThis numericalfeatures. Second,usingSpearmanrank phenomenon may indicate that language models coefficientandgeometricnon-linearregression,we use stronger non-linear encoding systems\u201d. Our specificallytestwhethertwokeypropertiesremifindingssupportthisclaimandfurtheruncoverthe niscentofhumannumericalcognition(orderpreserunderlyingnatureofthisnon-linearity. vationinrepresentationsandacompressioneffect Recent studies such as Levy and Geva (2024); wheredistancesbetweenconsecutivenumbersde- Zhou et al. (2024) show that LLMs rely on basecreaseasvaluesincrease)emergeinLLMs.",
      "chunk_index": 0,
      "word_count": 406,
      "token_count": 527,
      "metadata": {
        "start_char": 0,
        "end_char": 5908,
        "has_overlap": false,
        "sentence_count": 38
      }
    },
    {
      "chunk_id": "2502_chunk_1",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "Second,usingSpearmanrank phenomenon may indicate that language models coefficientandgeometricnon-linearregression,we use stronger non-linear encoding systems\u201d. Our specificallytestwhethertwokeypropertiesremifindingssupportthisclaimandfurtheruncoverthe niscentofhumannumericalcognition(orderpreserunderlyingnatureofthisnon-linearity. vationinrepresentationsandacompressioneffect Recent studies such as Levy and Geva (2024); wheredistancesbetweenconsecutivenumbersde- Zhou et al. (2024) show that LLMs rely on basecreaseasvaluesincrease)emergeinLLMs. 10digit-wiserepresentationsratherthanencoding WhilebothPCAandPLSrevealthatnumerical numbersinacontinuouslinearspace,asrevealed representationslargelyresideinalinearsubspace, through circular probing techniques. While indionly PCA captures systematic sublinearity, sugvidual digits are accurately reconstructed, perforgesting that simple linear probes3 may overlook mance declines for larger numbers, suggesting a theunderlyingnon-uniformityinLLMs\u2019numerical structured rather than holistic encoding. Furtherencoding. more, Zhou et al. (2024) demonstrate that LLMs trained on higher-base numeral systems struggle Contributions Wesummarizeourmainfinding withnumericalextrapolation,implyinganimplicit inthefollowing: compressed representation where smaller values have finer granularity-consistent with logarith- \u2022 We introduce a methodology for analyzing micscaling. Collectively,theseresultsalignwith thegeometricstructureofnumberrepresentaandfurthersubstantiatethehypothesisthatLLMs tions,offeringasystematicapproachtostudyinternallyrepresentnumbersinanon-uniform,subingnumericalabstractionsinartificialneural linearmanner. networks. Logarithmic functions can appear linear over small local intervals, which may explain why \u2022 WeprovideempiricalevidencethatLLMsen- LLMs are often assumed to represent numbers codenumericalvaluesinastructuredyetnonlinearly. Consequently, methods like PLS regresuniform way, revealing systematic compressionandactivationpatching(HeinzerlingandInui, sion reminiscent of the human logarithmic 2024;El-Shangitietal.,2024),whichanalyzesmall mental number line. Our findings refine the activationvariations,maycapturelocalmonotoniclinearhypothesisbyshowingthatnumerical ity while missing the global nonlinear structure. Thissuggeststhatreportedlineareffectscouldstem 3PLSisalinearprobethatprojectsinputdataontoalowerdimensionalsubspace,maximizingcovariancewiththetarget. fromanalyzingnarrownumericalranges,whereas Model Numbers Prompts Layer 1 Layer 2 Layer M Outputs 1000 2107=2107, 14=14, 708=708, 1000= 100 11=11, 10010=10010, 3102=3102, 100= ... 100 10 2=2, 1078=1078, 132=132, 10= PCA PCA PCA Figure2: Theoverallgraphicalrepresentationofourmethod. Numbersarepassedtothemodelinformofaprompt andtheinternalrepresentationsarecapturedfromtheembeddingscorrespondingtotoken\u2019=\u2019. Ateverylayer,we performPCAprojectionsontooneandtwodimensionalsubspacesandpickalayerwithhighestexplainedvariance (\u03c32)scoretofurtheranalyzemonotonicityandscalingofnumberrepresentations. a broader examination may reveal an underlying numericalvaluesalonganumberlineorexhibits logarithmicrepresentation. cognitive-likepatterns,suchassublinearscaling. Finally, we also emphasize the difference be- Ourgoalistostudythepropertiesofthefunc- tween our work and prior studies on numerical tionf LLM ,whichservesasacounterpartofthe reasoning (Park et al., 2022; Zhang et al., 2020) humancognitivemappingf H describedabove. whichevaluatemodels\u2019abilitytoprocessexplicit Specifically,weexaminewhetherf LLM preserves numbersratherthanprobingtheirinternalrepresenthenaturalorderingofnumbersandhowittrans- tations. WhileParketal. (2022)focusontaskslike formstheirmagnitudes. unit conversion and range detection, Zhang et al. 3.2 Definitionoff (2020)examinenumericalmagnitudeincommon LLM sense reasoning. Unlike these works, our study Toanalyzethestructureofhiddenrepresentations, investigatesthespatialstructureofnumericalrepwe apply a projection T : Rd \u2192 Rp, p = 1,2, resentationswithinhiddenstatesandhowthisenobtainedfromtechniquessuchasPrincipalCom- codinggeneralizesacrossscales. ponent Analysis (PCA) or Partial Least Squares (PLS).Then,ourfunctionisgivenby 3 Methodology f (x) := T(f(x)), (1) LLM 3.1 Generalsettup wheref isamapbetweentheinputnumberandthe correspondinginternalrepresentationinthemodel The logarithmic mental number line hypothesis (seeSections4and5howf isdefinedinvarious (Dehaene et al., 2008) suggests that humans insettings). nately perceive numerical magnitudes on a loga- For any two inputs x,y \u2208 X, we define the rithmic scale rather than a linear one. Formally, distance between their projections following the if we denote the internal mapping of numbers to mappingT usingEuclideannormas theircognitiverepresentationasf ,thehypothesis H assertsthatf isapproximatelylogarithmic. While H d(x,y) = ||f (x)\u2212f (y)||. (2) LLM LLM the exact nature of f remains elusive, we adopt H theguidingprinciplef \u2261 loginourexperiments.",
      "chunk_index": 1,
      "word_count": 402,
      "token_count": 522,
      "metadata": {
        "start_char": 5360,
        "end_char": 10342,
        "has_overlap": true,
        "sentence_count": 32
      }
    },
    {
      "chunk_id": "2502_chunk_2",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "While H d(x,y) = ||f (x)\u2212f (y)||. (2) LLM LLM the exact nature of f remains elusive, we adopt H theguidingprinciplef \u2261 loginourexperiments. 3.3 AbstractNumberLineandMonotonicity H Metric Ontheotherside,Largelanguagemodels,like LLaMA-2, process inputs by mapping them into If the projection dimension is p = 1, onea high-dimensional representation space, where dimensionalembeddingofnumericalinputsforms each input x (e.g., a number) is transformed into a number line if the projections preserve monoan internal representation f(x) \u2208 Rd. Analyzing tonicity (resp. reverse monotonicity), i.e., for thegeometryoftheserepresentationsacrossaset x < x (resp. x > x ), we have f (x ) < 1 2 1 2 LLM 1 of inputs X can reveal how the model organizes f (x ).Thisensuresthatthenaturalorderofnu- LLM 2 and reasons about them, shedding light on emermericalvaluesismaintainedintherepresentation gentpropertiessuchaswhetherthemodelencodes space. Tomeasuremonotonicitypropertiesofthefunc- Finally, \u03b2 < 1 implies that the difference betion f we use Spearman rank correlation that tweenconsecutivemembersissteadilydecreasing, LLM we briefly describe next. Let X,Y \u2208 Rn be two x \u2212x isexpectedtobegreaterthanx \u2212x . i+1 i i+2 i+1 real n-dimensional vectors and let R(X) (resp. Suchsequencesarecommonlyknownasconcave R(Y)) denote an n-dimensional vector obtained sequences(Rockafellar,2015),andthegrowthof fromX (resp. Y)wheretheentriesaresubstituted the sequence is exponentially decreasing (sublinwith their ranks in the sequence of sorted entries ear). An example of such a sequence is given by ofX (resp. Y). Then,Spearmanrankcorrelation x = 1\u2212 1 ,with\u03b1 = 9and\u03b2 = 1 . i 10i 10 coefficients(usuallydenotedby\u03c1)isgivenby: 4 Experiment1: Identifyingnumberline Cov(R(X),R(Y)) \u03c1 = , (3) usingcontextualizednumbers \u03c3(R(x))\u00b7\u03c3(R(Y)) Setup. Tosystematically probethe model\u2019s nuwhere Cov(R(X),R(Y)) is the covariance bemericalrepresentations,wepartitionnumbersinto tween rank vectors R(X) and R(Y), while logarithmicallyspacedgroups: \u03c3(R(X)) and \u03c3(R(Y)) are their respective standarddeviations. G = {1,2,...,20}, Spearmancoefficient\u03c1isanonparametricmea- (5) G = {10i\u221219,...,10i+20}, i \u2265 2. sureforthealignmentofthetwovectors. Loosely i+1 speaking,thecoefficientassessesiftheincrement Thesegroupsensurethatlargergroupindicescorinonevariablecorrespondstotheincrease(orderespondtonumericalmagnitudesthatincreaseex- crease)oftheother. Inparticular,unlikePearson ponentiallywithindex,reflectingthelogarithmic coefficient which takes into account the value of natureofthementalnumberlinehypothesis. thechanges,Spearman\u2019s\u03c1takesintoaccountonly Toanalyzetheembeddingsofnumbers,toevery thesignofthechanges. numberx \u2208 G weassignthefollowingprompt: i 3.4 ScalingRateIndex x \u2190 a=a, b=b, c=c, x= (6) For a monotonically increasing sequence of positive real numbers x ,...,x , we introduce the 1 n wherea, b, and carerandomlygeneratednum- Scaling Rate Index to measure the rate at which bers from the groups G . This prompt structure numbers grow in magnitude. More precisely, we i is designed to provide the model with contextual seekforpositiverealconstants\u03b1and\u03b2 thatminiexamples,encouragingittoinvokethenumberx mizethefollowingobjectivefunction: inmodel\u2019shiddenstatesrepresentations(seeFign\u22121 ure 2). Such approaches have been used in prior (cid:88) |(x \u2212x )\u2212\u03b1\u00b7\u03b2i|2. (4) i+1 i work to probe contextual representations in lani=1 guagemodelsSrivastavaetal.(2024).",
      "chunk_index": 2,
      "word_count": 384,
      "token_count": 499,
      "metadata": {
        "start_char": 10203,
        "end_char": 13622,
        "has_overlap": true,
        "sentence_count": 25
      }
    },
    {
      "chunk_id": "2502_chunk_3",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "Such approaches have been used in prior (cid:88) |(x \u2212x )\u2212\u03b1\u00b7\u03b2i|2. (4) i+1 i work to probe contextual representations in lani=1 guagemodelsSrivastavaetal.(2024). In particular, if \u03b2 > 1, the difference between The hidden state representation f(x) \u2208 Rd is twoconsecutivemembersx andx isexpected extractedfromadesignatedlayerofthemodelfrom i i+1 to increase with i. In other words, x \u2212 x is thelasttokenintheprompt,e.g. the\u2018=\u2018token. We i+1 i expectedtobesmallerthanx \u2212x . Suchseuseonlythosexforwhichthegeneratedoutputof i+2 i+1 quencesarecommonlyknownasconvexsequences themodelisxitself. (Rockafellar,2015)andthegrowthofx isexpo- To ensure a representative sampling, we rani nential (superlinear) in i. An example of such a domlyselectknumbersfromeachgroupG . These i sequencethatisrelevanttoourstudyisthatdefined sampled numbers collectively form a dataset, dewithx := 10i. Then,x \u2212x = 9\u00b710i andwe noted as X, which serves as the basis for our i i+1 i cantake\u03b1 = 9and\u03b2 = 10. analysis. The set of hidden state representations, If \u03b2 = 1, expression (4) indicates that the dif- {f(x)} ,isthenaggregatedandanalyzedtoinx\u2208X ferencebetweentwoconsecutivemembersx and vestigatepatternsandpropertiesintheembedding i x isapproximatelyconstant,i.e. thesequence space. i+1 is approximately linearly increasing. For exam- Tocontrolforpotentialbiasesintroducedbyto- ple,onemaytakethesequencex := i,forwhich kenization(wherelargernumbersoftenspanmore i \u03b1 = 1 = \u03b2. tokens)weconductacomplementaryexperiment usingnon-numericalsequences. Insteadofnumericalinputs,weconstructsequencesofrandomletf \u0304 (i) = E [f (x)], (7) LLM LLM ters with lengths corresponding to the tokenized x\u2208Gi representationsofnumbers. Thelettersequences and fit positive constants \u03b1 and \u03b2 such that are grouped to their lengths so that the grouping f \u0304 (i) \u2248 \u03b1\u00b7\u03b2i. LLM approximatelymatchesoneofthenumbers,andthe Inparticular,themapping promptscorrespondingtospecificlettersequences aredesignedinasimilarfashionasforthenumbers 10i (cid:55)\u2192 f \u0304 (i) (8) LLM (6). Thissettingallowsustocompareanyobserved structuralpatternsbetweenthenumberrepresentaallowsustoexaminehowdoesf LLM scalesnumeritions and letter representations. By doing so, we calmagnitudes. Thefundamentalquestionweseek candeterminewhetherthemodeltrulyencodesnutoansweris: Whatisthenatureofthefunction mericalmagnitudeorifitissimplyrespondingto f LLM (logarithmic,linear,orexponential)? surface-levelfeaturesoftheinput. Toanswerthisquestion,weanalyzethescaling factor \u03b2 in the fitted exponential model4. In the Motivation. The goal of this experiment is following, we explain how different values of \u03b2 twofold. We first investigate whether LLMs encorrespondtotheunderlyingpropertiesoff LLM . code numerical values in context along a mono- \u2022If\u03b2 > 1,thesequencef \u0304 LLM (i)isconvexand tonic number line in their internal representation exponentiallyincreasing. Thismeansf LLM maps space. Second,wetestwhetherthisnumberlineexan exponentially increasing sequence to another hibitssublinearscaling,similartohumancognitive exponentiallyincreasingsequence: representationsofnumbers. 9\u00b710i (cid:55)\u2192 \u03b1\u00b7\u03b2i = \u03b1\u00b710(log 10 \u03b2)\u00b7i. Methodology. Forthesepurposes,weuse: Thus,f preservestheoriginalspacingofnum- LLM \u2022PCAnadPLS.Afterthesetofhiddenstatereprebers,albeitwithascalingfactorlog \u03b2 > 0. sentationsisaggregated,wefurtherprojectitinto \u2022 If \u03b2 = 1, the sequence f \u0304 (i) is linearly LLM aone-dimensionalspaceusingPCAorPLSmethincreasing,meaningf takestheform: LLM ods. In particular, for PLS we take the numbers themselves to form the target vectors, while for 10i (cid:55)\u2192 \u03b1\u00b7i = \u03b1\u00b7log 10i. letters,weconsiderthelettersequenceasabase26 representationofanumber(withrandombutfixed Inthiscase,f LLM exhibitslogarithmicscaling. assignment of values to the letters), and use this \u2022 If \u03b2 < 1, the sequence f \u0304 LLM (i) is concave, numberasthecorrespondingtarget. exponentiallydecaying. Here,f LLM follows: \u2022Monotonicitymetric.",
      "chunk_index": 3,
      "word_count": 405,
      "token_count": 526,
      "metadata": {
        "start_char": 13462,
        "end_char": 17378,
        "has_overlap": true,
        "sentence_count": 37
      }
    },
    {
      "chunk_id": "2502_chunk_4",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "letters,weconsiderthelettersequenceasabase26 representationofanumber(withrandombutfixed Inthiscase,f LLM exhibitslogarithmicscaling. assignment of values to the letters), and use this \u2022 If \u03b2 < 1, the sequence f \u0304 LLM (i) is concave, numberasthecorrespondingtarget. exponentiallydecaying. Here,f LLM follows: \u2022Monotonicitymetric. Weapplyitonasequence 10i (cid:55)\u2192 \u03b1\u00b7\u03b2i = \u03b1\u00b710 \u2212(log 10 \u03b2 1)\u00b7i . x ,...,x of all the numbers in the union of 1 n groupsG j definedin(5),andtheirrespectivepro- Thusf isasub-logarithmicmapping. LLM jectionsf (x ),...,f (x ). Spearmanrank LLM 1 LLM n coefficientwilltelluswhetherthemodelpreserves Results. Theresultsrevealdistinctyetconsistent naturalorderingofthenumbers. patternsinhowdifferentmodelsencodenumerical andalphabeticalstructures,withvariationsacross \u2022 Scaling Rate Index. The initial centers of G , i layers (Tables 1 and 2). Despite these variations, given by x = 10i, form a convex, exponeni similar trends emerge across the models, leading tiallygrowingsequencecharacterizedbyparametoconsistentconclusionsabouttheirprocessingof ters \u03b1 = 9 and \u03b2 = 10. These numbers serve as numerical values (please refer to appendix A for representative scales of the numbers within each experimentaldetails). group. Numericalvs. SymbolicRepresentations. First Toobtainarobustestimateofhowthesescales key finding is that numerical embeddings exare preserved in the projections under f , we LLM hibit a significantly higher explained variance compute the expectation of projections in each group. Specifically, for SRI analysis, we define 4Wecandisregard\u03b1fromtheanalysissinceitdoesnot thesequence influencethescalingbutmerelyintroducesabias. Model Group Layer \u03c1\u00b1std \u03b2\u00b1std \u03c32\u00b1std Numbers 3 0.97\u00b10.00 0.83\u00b10.06 0.60\u00b10.01 LLaMA-2-7B Letters 1 0.45\u00b10.00 1.21\u00b10.00 0.24\u00b10.00 Numbers 8 0.94\u00b10.01 0.54\u00b10.01 0.31\u00b10.01 Pythia-2.8B Letters 11 0.89\u00b10.01 0.53\u00b10.10 0.16\u00b10.01 Numbers 18 0.95\u00b10.00 0.58\u00b10.02 0.32\u00b10.00 GPT-2-L Letters 5 0.11\u00b10.05 0.80\u00b10.42 0.21\u00b10.01 Numbers 3 0.96\u00b10.00 1.05\u00b10.00 0.44\u00b10.00 Mistral-7B Letters 14 0.89\u00b10.00 0.60\u00b10.00 0.22\u00b10.00 Numbers 1 0.41\u00b10.04 1.14\u00b10.05 0.48\u00b10.01 LLaMA-3.1-8B Letters 1 0.56\u00b10.00 0.16\u00b10.07 0.19\u00b10.01 Numbers 4 0.93\u00b10.02 1.33\u00b10.12 0.35\u00b10.01 LLaMA-3.2-Instruct-1B Letters 1 0.57\u00b10.06 0.47\u00b10.08 0.17\u00b10.00 Table1:ComparisonofseveralmodelsonNumbersand Lettersgroups,evaluatedusingthreemetrics: \u03c1,\u03b2,and ExplainedVariance(\u03c32). Resultsarereportedforthe layerwiththehighest\u03c32score. Standarddeviationsare included. Model Group Layer \u03c1\u00b1std \u03b2\u00b1std R2\u00b1std Numbers 6 0.91\u00b10.00 1.93\u00b10.05 0.68\u00b10.01 Llama-3.2-1B-Instruct Letters 10 0.93\u00b10.00 0.97\u00b10.03 0.45\u00b10.03 Numbers 1 0.78\u00b10.02 4.65\u00b11.32 0.71\u00b10.01 Pythia-2.8b Letters 20 0.90\u00b10.01 0.95\u00b10.11 0.46\u00b10.04 Numbers 17 0.96\u00b10.01 1.15\u00b10.09 0.67\u00b10.03 GPT2-L Letters 33 0.81\u00b10.03 0.93\u00b10.04 0.44\u00b10.01 Numbers 5 0.93\u00b10.00 2.62\u00b10.00 0.81\u00b10.00 Llama-2-7b Letters 27 0.88\u00b10.03 0.91\u00b10.02 0.45\u00b10.01 Numbers 7 0.88\u00b10.00 14.87\u00b18.28 0.81\u00b10.00 Mistral-7B-v0.1 Letters 29 0.86\u00b10.00 1.62\u00b10.00 0.63\u00b10.00 Numbers 4 0.93\u00b10.01 2.00\u00b10.01 0.73\u00b10.01 Llama-3.1-8B Letters 16 0.93\u00b10.01 0.88\u00b10.06 0.45\u00b10.02 Table2:ComparisonofseveralmodelsonNumbersand Lettersgroups,evaluatedusingthreemetrics: \u03c1,\u03b2,and R2. Resultsarereportedforthelayerwiththehighest R2. Standarddeviationsareincluded. 0.8 0.6 0.4 0.2 0.0 0 5 10 15 20 25 30 35 Layer , VE GPT2-L Left Y-axis 2 Right Y-axis 1.6 1.0 1.4 0.8 1.2 1.0 0.6 0.8 0.6 0.4 0.4 0.2 0.2 0 5 10 15 20 25 30 Layer , VE Llama-2.7B Left Y-axis 2 Right Y-axis 3 3 . .",
      "chunk_index": 4,
      "word_count": 400,
      "token_count": 520,
      "metadata": {
        "start_char": 17050,
        "end_char": 20516,
        "has_overlap": true,
        "sentence_count": 26
      }
    },
    {
      "chunk_id": "2502_chunk_5",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": ". 0 5 2.5 2.0 1.5 1.0 0.5 0.0 1.0 0.8 0.6 0.4 0.2 0 5 10 15 20 25 30 Layer , VE Pythia-2.8B Left Y-axis 2 Right Y-axis 1.8 1.0 1.6 0.8 1.4 1.2 0.6 1.0 0.4 0.8 0.2 0.6 0 5 10 15 20 25 30 Layer , VE 0 1 2 3 4 log10(x) Mistral-7B Left Y-axis 2 Right Y-axis 1.0 0.8 0.6 0.4 0.2 Figure3: Layer-wiseanalysisoffourmodelsonnumerical groups, showing explained variance (\u03c32), monotonicity(\u03c1),andScalingRateIndex(\u03b2). Thelayerwith maximum\u03c32alignswithpeak\u03c1,indicatingoptimalnumericalencoding. (\u03c32 in Table 1 and R2 in Table 2) in the one- )x(T Pythia-2.8B =0.96, =0.52 1 2 3 4 log10(x) )x(T GPT2-L =0.95, =0.57 0.5 0.0 0.5 1.0 1 2 3 4 log10(x) )x(T Llama-2.7B =0.97, =0.78 0.10 0.05 0.00 0.05 0.10 0 1 2 3 4 log10(x) )x(T Mistral-7B =0.97, =1.12 Figure4: Projectionsofnumericalrepresentations(yaxis) against their log-scaled magnitudes (x-axis) for the layer with the highest explained variance in four models. Sublinearityandmonotonicity(\u03c1)areindicated aboveeachsubfigure,demonstratingconsistentsublinear trends and strong monotonic relationships across models. dimensional PCA and PLS transformations comparedtoletter-basedembeddings(refertoMethodology). Thissuggeststhatnumbersnaturallyalign alongaone-dimensionalmanifold-akintoanumberline-whilerandomsequencesoflettersdonot displaythesamestructuredbehavior. Furthermore,themonotonicitymetric(\u03c1)consistentlyshowshighervaluesfornumericaldata,with mostmodelsachieving\u03c1 > 0.9inbothPCAand PLSanalyses. Thissupportstheideathatnumericalrepresentationsarenotonlystructured,butalso maintainawell-orderedprogressionacrosslayers. The resulting projections obtained using PCA forthenumericalandlettersgroupsarevisualized inFigures4and5,respectively. Sublinearity in Numerical Representations. Thesublinearitycoefficient(\u03b2)derivedfromPCA projectionsrevealsnotabledifferencesacrossmodels. Some,suchasLLaMA-2-7B,Pythia,andGPT- 2Large,exhibitstrongsublinear(sublogarithmic) scaling with \u03b2 < 1, indicating that embedding distances grow at a diminishing rate. In contrast, modelslikeMistralshowanearlylogarithmictrend (\u03b2 \u2248 1),whileothersapproachamorelinearspacingpatternwithhigher\u03b2 values. Layer-Wise Dynamics. Since Table 1 reports values from the layer with the highest explained variance,interpretationrequirescaution-otherlay- 0 2 4 log10(x) )x(T Pythia-2.8B =0.89, =0.53 2 4 log10(x) )x(T GPT2-L =0.11, =0.80 0.2 0.1 0.0 0.1 0.2 0 2 4 log10(x) )x(T Llama-2.7B =0.45, =1.21 0 2 4 log10(x) )x(T capturethetruegeometricorganizationofnumerical representations, especially in regimes where non-lineareffectsdominate. Ablationstudy. Finally,weperformanablation studytoexaminethedependenceofourresultson thenumberofexamplesinthepromptforbothnumericalandalphabeticaldatasets. Figure6shows Mistral-7B =0.89, =0.60 thatthemetricsexhibitgreaterstabilityfornumericaldatacomparedtoalphabeticaldata,indicating that the model processes numerical information more consistently, while alphabetical representationsaremoresensitivetopromptvariations. Figure5: Projectionsoflettersrepresentations(y-axis) 0.9 against their log-scaled magnitudes (x-axis) assigned proportionaltotheirlength,forthelayerwiththehighest 0.8 explained variance in four models. Sublinearity and 0.7 monotonicity (\u03c1) are indicated above each subfigure, demonstrating consistent sublinear trends and strong 0.6 20 25 30 35 40 monotonicrelationshipsacrossmodels. Number of Samples erswithcomparable\u03c32 valuesmayexhibitsimilar trends. Figure 3 provides a layer-wise analysis for four models, demonstrating how sublinearity evolvesacrossdifferentdepths. PCA vs PLS. The PLS method achieves high monotonicity(\u03c1)andexplainedvariance(R2)but exhibitslowersublinearitycomparedtoPCA.This discrepancy arises because PLS operates as a supervisedlinearprobe,wheretheregressiontarget (e.g.,numericalvalues)directlyinfluencestheprojection. Thisprocessdistortstheintrinsicspacing betweenpoints,asPLSprioritizesmaximizingcovariancewiththetargetoverpreservingtheoriginal geometricstructure. Incontrast,PCA,beingunsupervised,retainstherelativespacingofdatapoints inthelatentspace,bettercapturingtheunderlying sublinear trends. This distinction is evident in tables 1 and 2: PCA consistently reveals stronger sublinearity,whilePLSachieveshigherR2 and\u03c1 byaligningtheprojectionwiththetargetvariable. Notably, this aligns with findings in Zhu et al.",
      "chunk_index": 5,
      "word_count": 396,
      "token_count": 514,
      "metadata": {
        "start_char": 20515,
        "end_char": 24823,
        "has_overlap": true,
        "sentence_count": 29
      }
    },
    {
      "chunk_id": "2502_chunk_6",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "Thisprocessdistortstheintrinsicspacing betweenpoints,asPLSprioritizesmaximizingcovariancewiththetargetoverpreservingtheoriginal geometricstructure. Incontrast,PCA,beingunsupervised,retainstherelativespacingofdatapoints inthelatentspace,bettercapturingtheunderlying sublinear trends. This distinction is evident in tables 1 and 2: PCA consistently reveals stronger sublinearity,whilePLSachieveshigherR2 and\u03c1 byaligningtheprojectionwiththetargetvariable. Notably, this aligns with findings in Zhu et al. (2025), where a linear probe failed to adequately capturethenon-linearscalingofhiddenstates,particularly for larger numbers, where non-linearity becomes more pronounced. Our work explicitly quantifysublinearityusingtheScalingRateIndex (SRI,\u03b2),whichdirectlymeasurestherateofscaling in the latent space. This allows us to better eulaV cirteM Numerics PCA Symbols PCA 2 1.2 1.0 0.8 0.6 0.4 0.2 20 25 30 35 40 Number of Samples 1.0 0.9 0.8 0.7 0.6 1 2 3 4 Number of Examples eulaV cirteM Numerics PCA Symbols PCA 2.5 2.0 1.5 1.0 0.5 1 2 3 4 Number of Examples Figure6:Toprow:Changeinmetricswithrespecttothe numberofsamplesinthesetX. Bottomrow: Change inmetricswithrespecttothenumberofin-contextexamplesintheprompt. Leftcolumncorrespondstothe numbersgroup,andtherightcolumncorrespondsto thelettersgroup. Sublinearityandmonotonicitytrends arehighlightedforeachcase. 5 Experiment2: Identifyingnumberline usingreal-worldtasks Setup. Inthepreviousexperiment(Section4)we createdanartificialexperimentalsettingtotestour hypothesis. Inthisexperiment,however,wewant tofurthervalidateourhypothesisusingreal-world data. We collect names of celebrities along with theirbirthyearsandpopulationofdifferentcities/- countriesfromWikidata(Vrandec\u02c7ic \u0301 andKro\u0308tzsch, 2014). The task here is to investigate for similar patternsandobservationsseeninthepreviousexperiment. Motivation. ThegoalofthisexperimentistoinvestigatehowLLMsinternallyrepresentnumerical valuesinreal-worldcontexts,specificallyfocusing dataset. onthemonotonicityandscalingoftheserepresentations. Byanalyzingthehiddenstates,weaimto 20 uncoverwhetherthemodelsencodenumericalin- 10 0 formationinastructuredandinterpretablemanner. 20 Methodology. The experimental setup for this experimentisasfollows: 20 0 20 40 Component 1 \u2022PromptingtheModel: Wepromptthemodel to provide the exact birth year or population size for each entity in our dataset, which consists of 1K samples.An example of a prompt would be \u201cWhatisthepopulationof[country]?\u201d \u2022CollectingModelOutputs: WecollecttheLLM\u2019soutput answers,andfilteroutnon-numericalandincorrect responses. \u2022 Extracting Hidden States: We extract the hidden state corresponding to the question mark tokenateachmodellayer5. \u2022TrainingPLSModels: Wetrainone-andtwocomponent PLS models on the extracted hidden statestopredictthebirthyearsorpopulationsizes of the entities. This is performed for two LLMs: Llama-3.1-8BandLlama-3.2-1B. Results. Theresults,asshownintable3,demonstrateacleardistinctionbetweenthetwotasks,but alsobetweenthemodels. For the birth year task, model Llama-3.1-8B exhibit strong trends, with high monotonicity (\u03c1) and (R2), while having low SRI (\u03b2), hence high compression. Thisindicatesthattheinternalrepresentationsofbirthyearsarewell-structuredandpredictive,aligningwithourexpectationsfornumericalencodinginLLMs. Ontheotherside,Llama- 3.2-1B)showslowmonotonicityscore,hencethe \u03b2 factor is not informative. We attribute the low monotonicityscoretothenon-structuredinternal representationsoflowerbirthyears,ascanbeseen inFigure7. For the population size task, both models displayweakermonotonicityandlowerR2,suggesting population sizes are encoded less systematically. Unlike birth years, population figures are morecontext-dependent,influencedbygeopoliticalchanges,reportinginconsistencies,andapproximate expressions in text. Consequently, the low monotonicitymakesthescalingratio\u03b2 unreliable. FinallyFigure7showstheexamplesofoneand twoPLSprojectionsfortwomodels,forbirth-year 5We divided the hidden states into four equally sized groups,rangingfromtheminimumtothemaximumanswers, tofacilitatethecalculationoftheScalingRateIndex(SRI). tnenopmoC Layer 30 htriB 0 200 400 600 800 1000 Sample Index tnenopmoC Layer 29 htriB 20 0 20 40 60 Component 1 tnenopmoC Layer 12 htriB 0 200 400 600 800 1000 Sample Index tnenopmoC Layer 12 htriB Figure7:VisualizationofPLSmodelstrainedonLlama- 3.1-8B(toprow)andLlama-3.2-1B(bottomrow)model activationstopredictentities\u2019birthyearsusingoneand twodimensionalPLSrespectively. EachsubfigurerepresentsthelayerwiththehighestR2scoreforone-and two-componentPLSmodels.",
      "chunk_index": 6,
      "word_count": 377,
      "token_count": 490,
      "metadata": {
        "start_char": 24322,
        "end_char": 28918,
        "has_overlap": true,
        "sentence_count": 36
      }
    },
    {
      "chunk_id": "2502_chunk_7",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "EachsubfigurerepresentsthelayerwiththehighestR2scoreforone-and two-componentPLSmodels. Model Dataset Layer \u03c1 \u03b2 R2 Birth 29 0.84 0.50 0.63 Llamba3.18B Population 9 0.63 2.48 0.08 Birth 12 0.03 0.72 0.61 Llamba3.21B Population 10 0.62 2.69 0.10 Table3: Resultsfor Llambamodels evaluated onthe BirthandPopulationdatasets. Resultsarereportedfor thelayerwiththehighestR2,highlightingtherelationship between scaling rate (\u03b2), monotonicity (\u03c1), and modelperformance. 6 Conclusion Inspiredbythelogarithmiccompressioninhuman numericalcognition,weinvestigatewhetherLLMs encodenumericalvaluesanalogously. Byanalyzinghiddenstatesacrosslayers,weemploydimensionalityreductiontechniques(PCAandPLS)and geometricregressiontotestfortwokeyproperties: (1) order preservation and (2) sublinear compression, where distances between consecutive numbersdecreaseasvaluesincrease. Ourresultsreveal that while both PCA and PLS identify numerical representationsinalinearsubspace,onlyPCAcapturessystematicsublinearity. ThisindicatesthatlinearprobeslikePLS,whichoptimizeforcovariance with the target, may obscure the underlying nonuniformstructure. OurfindingssuggestthatLLMs encodenumericalvalueswithstructuredcompression, akin to the human mental number line, but thisisonlydetectablethroughmethodslikePCA Kiho Park, Yo Joong Choe, and Victor Veitch. 2023. thatpreservegeometricrelationships. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658. References SungjinPark,SeungwooRyu,andEdwardChoi.2022. Do language models understand measurements? JoshAchiam,StevenAdler,SandhiniAgarwal,Lama Preprint,arXiv:2210.12694. Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida,JankoAltenschmidt,SamAltman, Fabio Petroni, Tim Rockta\u0308schel, Sebastian Riedel, ShyamalAnadkat,etal.2023. Gpt-4technicalreport. Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and arXivpreprintarXiv:2303.08774. AlexanderMiller.2019. Languagemodelsasknowledge bases? In Proceedings of the 2019 Confer- Stanislas Dehaene. 2003. The neural basis of the enceonEmpiricalMethodsinNaturalLanguageProweber-fechner law: a logarithmic mental number cessingandthe9thInternationalJointConference line. Trendsincognitivesciences,7(4):145-147. onNaturalLanguageProcessing(EMNLP-IJCNLP), pages2463-2473. StanislasDehaene,Ve\u0301roniqueIzard,ElizabethSpelke, andPierrePica.2008. Logorlinear? distinctintu- AlecRadford,JeffreyWu,RewonChild,DavidLuan, itionsofthenumberscaleinwesternandamazonian DarioAmodei,IlyaSutskever,etal.2019. Language indigenecultures. science,320(5880):1217-1220. modelsareunsupervisedmultitasklearners. OpenAI blog,1(8):9. Ahmed Oumar El-Shangiti, Tatsuya Hiraoka, Hilal AlQuabeh,BenjaminHeinzerling,andKentaroInui. Ralph Tyrell Rockafellar. 2015. Convex analysis. 2024. The geometry ofnumerical reasoning: Lan- Princetonuniversitypress. guagemodelscomparenumericpropertiesinlinear RobertSSieglerandJohnEOpfer.2003. Thedevelopsubspaces. Preprint,arXiv:2410.13194. mentofnumericalestimation: Evidenceformultiple representationsofnumericalquantity. Psychological Gustav Theodor Fechner. 1860. Elemente der psyscience,14(3):237-250. chophysik,volume2. Breitkopfu.Ha\u0308rtel. PragyaSrivastava,SatvikGolechha,AmitDeshpande, AnnemarieFritz,AntjeEhlert,andLarsBalzer.2013. and Amit Sharma. 2024. NICE: To optimize in- Developmentofmathematicalconceptsasbasisfor contextexamplesornot? InProceedingsofthe62nd an elaborated mathematical understanding. South AnnualMeetingoftheAssociationforComputational AfricanJournalofChildhoodEducation,3(1):38-67. Linguistics (Volume 1: Long Papers), pages 5494- 5510,Bangkok,Thailand.AssociationforComputa- NathanGodey, E\u0301ricVillemonteDeLaClergerie, and tionalLinguistics. Benoi\u0302t Sagot. 2024. On the scaling laws of geographicalrepresentationinlanguagemodels. InPro- HugoTouvron,ThibautLavril,GautierIzacard,Xavier ceedingsofthe2024JointInternationalConference Martinet,Marie-AnneLachaux,Timothe\u0301eLacroix, onComputationalLinguistics,LanguageResources Baptiste Rozie\u0300re, Naman Goyal, Eric Hambro, andEvaluation(LREC-COLING2024),pages12416- Faisal Azhar, et al. 2023. Llama: Open and effi- 12422. cient foundation language models. arXiv preprint arXiv:2302.13971. WesGurneeandMaxTegmark.2024. Languagemodelsrepresentspaceandtime. InTheTwelfthInterna- DennyVrandec\u02c7ic \u0301 andMarkusKro\u0308tzsch.2014. WikitionalConferenceonLearningRepresentations. data: Afreecollaborativeknowledgebase. CommunicationsoftheACM,57:78-85. BenjaminHeinzerlingandKentaroInui.2024. Mono- Xikun Zhang, Deepak Ramachandran, Ian Tenney, tonic representation of numeric properties in lan- Yanai Elazar, and Dan Roth. 2020. Do language guagemodels. arXivpreprintarXiv:2403.10381. embeddingscapturescales? InProceedingsofthe ThirdBlackboxNLPWorkshoponAnalyzingandIn- FengqingJiang.2024. IdentifyingandmitigatingvulterpretingNeuralNetworksforNLP,pages292-299. nerabilitiesinllm-integratedapplications. Master\u2019s thesis,UniversityofWashington. ZhejianZhou,JIayuWang,DahuaLin,andKaiChen. 2024. Scaling behavior for large language models Amit Arnold Levy and Mor Geva. 2024. Language regardingnumeralsystems:Anexampleusingpythia. modelsencodenumbersusingdigitrepresentations In Findings of the Association for Computational inbase10. arXivpreprintarXiv:2410.11781. Linguistics: EMNLP2024,pages3806-3820. Korbinian Moeller, Silvia Pixner, Liane Kaufmann, FangweiZhu,DamaiDai,andZhifangSui.2025. LanandHans-ChristophNuerk.2009. Children\u2019searly guagemodelsencodethevalueofnumberslinearly. mental number line: Logarithmic or decomposed InProceedingsofthe31stInternationalConference linear? Journal of experimental child psychology, onComputationalLinguistics,pages693-709. 103(4):503-515. A Experimentaldetails AllexperimentswereperformedusinganNVIDIA A6000GPUforacceleratedcomputation. ThemodelswereimplementedinPythonandimportedfrom HuggingfacewithPyTorch,andstandardlibraries likeNumPyandMatplotlibwereusedfordataprocessingandvisualization. Weevaluatedthefollowingmodels: Model Variants Ref. Pythia 2.8B (Touvronetal.,2023) LLaMA 2.7B,3.1-8B,3.2-1B (Touvronetal.,2023) GPT-2 Large-1.5B (Radfordetal.,2019) Mistral 7B (Jiang,2024) Table4: Modelsevaluatedintheexperiments. Wheneverpossible,resultswerereportedasthe averageofthreeruns,alongwiththestandarddeviation(std). Forexperimentswhererepeatedruns werenotfeasible,therandomseedwasfixedto42 toensurereproducibility.",
      "chunk_index": 7,
      "word_count": 426,
      "token_count": 553,
      "metadata": {
        "start_char": 28832,
        "end_char": 35185,
        "has_overlap": true,
        "sentence_count": 91
      }
    },
    {
      "chunk_id": "2502_chunk_8",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "Journal of experimental child psychology, onComputationalLinguistics,pages693-709. 103(4):503-515. A Experimentaldetails AllexperimentswereperformedusinganNVIDIA A6000GPUforacceleratedcomputation. ThemodelswereimplementedinPythonandimportedfrom HuggingfacewithPyTorch,andstandardlibraries likeNumPyandMatplotlibwereusedfordataprocessingandvisualization. Weevaluatedthefollowingmodels: Model Variants Ref. Pythia 2.8B (Touvronetal.,2023) LLaMA 2.7B,3.1-8B,3.2-1B (Touvronetal.,2023) GPT-2 Large-1.5B (Radfordetal.,2019) Mistral 7B (Jiang,2024) Table4: Modelsevaluatedintheexperiments. Wheneverpossible,resultswerereportedasthe averageofthreeruns,alongwiththestandarddeviation(std). Forexperimentswhererepeatedruns werenotfeasible,therandomseedwasfixedto42 toensurereproducibility. B Additionalexperiments B.1 Layer-wisePLSanalysis 0.8 0.6 0.4 0.2 0.00 5 10 15 20 25 30 35 Layer , VE GPT2-L Left Y-axis 2 Right Y-axis 1 2 . . 7 0 5 0 0.8 1.50 1.25 0.6 1.00 0.75 0.4 0.50 0.25 0.2 0.00 0 5 10 15 20 25 30 Layer , VE Llama-2.7B Left Y-axis 2 Right Y-axis 1.2 1.0 0.8 0.6 0.4 0.8 0.6 0.4 0.2 0 5 10 15 20 25 30 Layer , VE Pythia-2.8B Left Y-axis 2 Right Y-axis 1.0 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0.0 0 5 10 15 20 25 30 Layer , VE 0.050 0.025 0.000 0.025 0.050 0 500 1000 Sample Index Mistral-7B Left Y-axis 2 Right Y-axis 1.0 0.8 0.6 0.4 0.2 0.0 Figure8: Layer-wiseanalysisoffourmodelsonletters groups,showingexplainedvariance(\u03c32),monotonicity (\u03c1),andScalingRateIndex(\u03b2).",
      "chunk_index": 8,
      "word_count": 168,
      "token_count": 218,
      "metadata": {
        "start_char": 34406,
        "end_char": 35873,
        "has_overlap": true,
        "sentence_count": 11
      }
    },
    {
      "chunk_id": "2502_chunk_9",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "B.2 Birthyearandpopulationdatasets projectionsinalllayers 1 tnenopmoC Layer 0 0 500 1000 Sample Index 1 tnenopmoC Layer 1 0 500 1000 Sample Index 1 tnenopmoC Layer 2 0 500 1000 Sample Index 1 tnenopmoC Layer 3 0 500 1000 Sample Index 1 tnenopmoC Layer 4 50 25 0 500 1000 Sample Index 1 tnenopmoC Layer 5 0 500 1000 Sample Index tnenopmoC Layer 6 0 500 1000 Sample Index tnenopmoC Layer 7 0 500 1000 Sample Index tnenopmoC Layer 8 0 500 1000 Sample Index tnenopmoC Layer 9 0 500 1000 Sample Index tnenopmoC Layer 10 0 500 1000 Sample Index tnenopmoC Layer 11 0 500 1000 Sample Index tnenopmoC Layer 12 0 500 1000 Sample Index tnenopmoC Layer 13 0 500 1000 Sample Index tnenopmoC Layer 14 40 20 0 500 1000 Sample Index 1 tnenopmoC Layer 15 20 0 60 0 500 1000 Sample Index 1 tnenopmoC Layer 16 40 20 400 500 1000 Sample Index 1 tnenopmoC Layer 17 20 0 500 1000 Sample Index tnenopmoC Layer 18 20 0 500 1000 Sample Index tnenopmoC Layer 19 20 0 500 1000 Sample Index tnenopmoC Layer 20 20 40 0 500 1000 Sample Index tnenopmoC Layer 21 40 0 20 0 500 1000 Sample Index tnenopmoC Layer 22 40 0 20 0 500 1000 Sample Index tnenopmoC Layer 23 0 500 1000 Sample Index tnenopmoC Layer 24 0 500 1000 Sample Index tnenopmoC Layer 25 0 500 1000 Sample Index tnenopmoC Layer 26 0 500 1000 Sample Index tnenopmoC Layer 27 0 500 1000 Sample Index tnenopmoC Layer 28 0 500 1000 Sample Index tnenopmoC Layer 29 0 500 1000 Sample Index 1 tnenopmoC Layer 30 400 500 1000 Sample Index 1 tnenopmoC 1400 1200 Layer 31 htriB 1400 1200 htriB 1400 1200 htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB htriB htriB htriB 1400 1200 htriB 1400 1200 htriB 1400 1200 htriB 1400 1200 htriB 1400 1200 htriB 1400 1200 htriB htriB htriB htriB htriB htriB Figure9: OnecomponentPLSmodeltrainedonLlama- 3.1-8B instruct model activations to predict entities\u2019 birthyear.",
      "chunk_index": 9,
      "word_count": 357,
      "token_count": 464,
      "metadata": {
        "start_char": 35874,
        "end_char": 37809,
        "has_overlap": true,
        "sentence_count": 1
      }
    },
    {
      "chunk_id": "2502_chunk_10",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "0.050 0.025 0.000 0.025 0.050 0.05 0.00 0.05 Component 1 2 tnenopmoC Layer 0 50 25 50 0 Component 1 2 tnenopmoC Layer 1 50 0 Component 1 2 tnenopmoC Layer 2 50 25 50 0 50 100 Component 1 2 tnenopmoC Layer 3 40 20 40 0 50 Component 1 2 tnenopmoC Layer 4 50 25 0 50 Component 1 2 tnenopmoC Layer 5 25 0 50 0 Component 1 2 tnenopmoC Layer 6 50 0 Component 1 2 tnenopmoC Layer 7 25 0 50 0 Component 1 2 tnenopmoC Layer 8 25 0 0 50 Component 1 2 tnenopmoC Layer 9 25 0 25 50 0 50 Component 1 2 tnenopmoC Layer 10 25 0 25 50 0 50 Component 1 2 tnenopmoC Layer 11 0 50 Component 1 2 tnenopmoC Layer 12 50 0 Component 1 2 tnenopmoC Layer 13 50 0 Component 1 2 tnenopmoC Layer 14 25 0 0 50 Component 1 2 tnenopmoC Layer 15 50 25 50 0 Component 1 2 tnenopmoC Layer 16 40 20 0 50 Component 1 2 tnenopmoC Layer 17 0 50 Component 1 tnenopmoC Layer 18 0 50 Component 1 tnenopmoC Layer 19 25 0 25 Component 1 tnenopmoC Layer 20 25 0 25 Component 1 tnenopmoC Layer 21 25 0 25 Component 1 tnenopmoC Layer 22 25 0 25 Component 1 tnenopmoC Layer 23 25 0 25 Component 1 tnenopmoC Layer 24 25 0 25 Component 1 tnenopmoC Layer 25 25 0 25 Component 1 tnenopmoC Layer 26 25 0 25 Component 1 tnenopmoC Layer 27 25 0 25 Component 1 tnenopmoC Layer 28 25 0 25 Component 1 tnenopmoC Layer 29 25 0 25 Component 1 tnenopmoC Layer 30 25 0 25 Component 1 tnenopmoC 1800 1600 Layer 31 htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 1400 htriB 1800 1600 1400 htriB 1800 1600 1400 htriB htriB htriB htriB 1800 1600 htriB 1800 1600 htriB 1800 1600 htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB Figure 10: Two components PLS model trained on Llama-3.1-8Binstructmodelactivationstopredictentities\u2019birthyear.",
      "chunk_index": 10,
      "word_count": 365,
      "token_count": 474,
      "metadata": {
        "start_char": 37810,
        "end_char": 39620,
        "has_overlap": true,
        "sentence_count": 1
      }
    },
    {
      "chunk_id": "2502_chunk_11",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "0.050 0.025 0.000 0.025 0.050 0 500 1000 Sample Index 1 tnenopmoC Layer 0 0 500 1000 Sample Index 1 tnenopmoC Layer 1 0 500 1000 Sample Index 1 tnenopmoC Layer 2 0 500 1000 Sample Index tnenopmoC Layer 3 0 500 1000 Sample Index tnenopmoC Layer 4 0 500 1000 Sample Index tnenopmoC Layer 5 0 500 1000 Sample Index tnenopmoC Layer 6 0 500 1000 Sample Index tnenopmoC Layer 7 0 500 1000 Sample Index tnenopmoC Layer 8 0 500 1000 Sample Index tnenopmoC Layer 9 0 500 1000 Sample Index tnenopmoC Layer 10 0 500 1000 Sample Index tnenopmoC Layer 11 0 500 1000 Sample Index tnenopmoC Layer 12 0 500 1000 Sample Index tnenopmoC Layer 13 0 500 1000 Sample Index tnenopmoC Layer 14 0 500 1000 Sample Index tnenopmoC Layer 15 htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB Figure11: OnecomponentPLSmodeltrainedonLlama-3.2-1Binstructmodelactivationstopredictentities\u2019birth year. 0.050 0.025 0.000 0.025 0.050 0.05 0.00 0.05 Component 1 tnenopmoC Layer 0 50 0 Component 1 tnenopmoC Layer 1 0 50 Component 1 tnenopmoC Layer 2 0 50 Component 1 tnenopmoC Layer 3 50 0 Component 1 tnenopmoC Layer 4 50 0 Component 1 tnenopmoC Layer 5 50 0 Component 1 tnenopmoC Layer 6 0 50 Component 1 tnenopmoC Layer 7 50 0 Component 1 tnenopmoC Layer 8 0 50 Component 1 tnenopmoC Layer 9 50 0 Component 1 tnenopmoC Layer 10 0 50 Component 1 tnenopmoC Layer 11 0 50 Component 1 tnenopmoC Layer 12 0 50 Component 1 tnenopmoC Layer 13 0 50 Component 1 tnenopmoC Layer 14 0 50 Component 1 tnenopmoC Layer 15 htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB htriB Figure12: TwocomponentsPLSmodeltrainedonLlama-3.2-1Binstructmodelactivationstopredictentities\u2019birth year.",
      "chunk_index": 11,
      "word_count": 292,
      "token_count": 379,
      "metadata": {
        "start_char": 39621,
        "end_char": 41343,
        "has_overlap": true,
        "sentence_count": 2
      }
    },
    {
      "chunk_id": "2502_chunk_12",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "0.050 0.025 0.000 0.025 0.050 0 500 1000 Sample Index 1 tnenopmoC Layer 0 0 500 1000 Sample Index 1 tnenopmoC Layer 1 0 500 1000 Sample Index 1 tnenopmoC Layer 2 0 500 1000 Sample Index 1 tnenopmoC Layer 3 0 500 1000 Sample Index 1 tnenopmoC Layer 4 100 0 500 1000 Sample Index 1 tnenopmoC Layer 5 0 500 1000 Sample Index 1 tnenopmoC Layer 6 100 50 0 500 1000 Sample Index 1 tnenopmoC Layer 7 100 50 0 500 1000 Sample Index 1 tnenopmoC Layer 8 100 50 0 500 1000 Sample Index 1 tnenopmoC Layer 9 0 500 1000 Sample Index 1 tnenopmoC Layer 10 100 50 0 500 1000 Sample Index 1 tnenopmoC Layer 11 0 500 1000 Sample Index 1 tnenopmoC Layer 12 0 500 1000 Sample Index 1 tnenopmoC Layer 13 0 500 1000 Sample Index 1 tnenopmoC Layer 14 0 500 1000 Sample Index 1 tnenopmoC Layer 15 0 500 1000 Sample Index 1 tnenopmoC Layer 16 0 500 1000 Sample Index 1 tnenopmoC Layer 17 0 500 1000 Sample Index 1 tnenopmoC Layer 18 0 500 1000 Sample Index 1 tnenopmoC Layer 19 25 0 0 500 1000 Sample Index 1 tnenopmoC Layer 20 0 500 1000 Sample Index 1 tnenopmoC Layer 21 0 500 1000 Sample Index 1 tnenopmoC Layer 22 0 500 1000 Sample Index 1 tnenopmoC Layer 23 50 25 0 500 1000 Sample Index 1 tnenopmoC Layer 24 50 25 0 500 1000 Sample Index 1 tnenopmoC Layer 25 50 25 0 500 1000 Sample Index 1 tnenopmoC Layer 26 20 0 500 1000 Sample Index 1 tnenopmoC Layer 27 20 0 500 1000 Sample Index 1 tnenopmoC Layer 28 20 0 500 1000 Sample Index 1 tnenopmoC Layer 29 0 500 1000 Sample Index 1 tnenopmoC Layer 30 60 0 500 1000 Sample Index 1 tnenopmoC 108 107 Layer 31 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 106 noitalupoP 108 107 106 noitalupoP 108 107 106 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 0.050 0.025 0.000 0.025 0.050 0.05 0.00 0.05 Component 1 Figure 13: One component PLS model trained on Llama-3.1-8Binstructmodelactivationstopredictentities\u2019populationsize.",
      "chunk_index": 12,
      "word_count": 450,
      "token_count": 585,
      "metadata": {
        "start_char": 41344,
        "end_char": 43735,
        "has_overlap": true,
        "sentence_count": 1
      }
    },
    {
      "chunk_id": "2502_chunk_13",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "2 tnenopmoC Layer 0 25 0 50 0 50 Component 1 2 tnenopmoC Layer 1 0 50 Component 1 2 tnenopmoC Layer 2 25 0 25 50 50 0 Component 1 2 tnenopmoC Layer 3 50 25 0 25 50 50 0 Component 1 2 tnenopmoC Layer 4 50 25 0 25 100 0 Component 1 2 tnenopmoC Layer 5 75 50 25 100 0 Component 1 2 tnenopmoC Layer 6 0 50 100 Component 1 2 tnenopmoC Layer 7 100 50 0 100 Component 1 2 tnenopmoC Layer 8 0 100 Component 1 2 tnenopmoC Layer 9 100 50 100 0 Component 1 2 tnenopmoC Layer 10 100 50 0 100 Component 1 2 tnenopmoC Layer 11 100 50 0 Component 1 2 tnenopmoC Layer 12 0 50 100 Component 1 2 tnenopmoC Layer 13 100 50 0 Component 1 2 tnenopmoC Layer 14 0 50 Component 1 2 tnenopmoC Layer 15 0 50 Component 1 2 tnenopmoC Layer 16 0 50 Component 1 2 tnenopmoC Layer 17 0 50 Component 1 2 tnenopmoC Layer 18 0 50 Component 1 2 tnenopmoC Layer 19 50 0 Component 1 2 tnenopmoC Layer 20 0 50 Component 1 2 tnenopmoC Layer 21 0 50 Component 1 2 tnenopmoC Layer 22 0 50 Component 1 2 tnenopmoC Layer 23 0 25 0 50 Component 1 2 tnenopmoC Layer 24 0 25 0 50 Component 1 2 tnenopmoC Layer 25 0 25 0 50 Component 1 2 tnenopmoC Layer 26 75 0 50 Component 1 2 tnenopmoC Layer 27 75 0 50 Component 1 2 tnenopmoC Layer 28 75 0 50 Component 1 2 tnenopmoC Layer 29 0 50 Component 1 2 tnenopmoC Layer 30 20 0 50 Component 1 2 tnenopmoC 108 107 Layer 31 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 106 105 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 106 noitalupoP 108 107 106 noitalupoP 108 107 106 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP 108 107 noitalupoP Figure 14: Two components PLS model trained on Llama-3.1-8Binstructmodelactivationstopredictentities\u2019populationsize.",
      "chunk_index": 13,
      "word_count": 417,
      "token_count": 542,
      "metadata": {
        "start_char": 43736,
        "end_char": 45856,
        "has_overlap": true,
        "sentence_count": 1
      }
    },
    {
      "chunk_id": "2502_chunk_14",
      "paper_id": "2502",
      "paper_title": "Abstract Humansarebelievedtoperceivenumbersona logarithmicmentalnumberline,wheresmaller valuesarerepresentedwithgreaterresolution thanlargerones.",
      "section": "Full Text",
      "text": "0.050 0.025 0.000 0.025 0.050 0 500 1000 Sample Index tnenopmoC Layer 0 0 500 1000 Sample Index tnenopmoC Layer 1 0 500 1000 Sample Index tnenopmoC Layer 2 0 500 1000 Sample Index tnenopmoC Layer 3 0 500 1000 Sample Index tnenopmoC Layer 4 0 500 1000 Sample Index tnenopmoC Layer 5 0 500 1000 Sample Index tnenopmoC Layer 6 0 500 1000 Sample Index tnenopmoC Layer 7 0 500 1000 Sample Index tnenopmoC Layer 8 0 500 1000 Sample Index 1 tnenopmoC Layer 9 0 500 1000 Sample Index 1 tnenopmoC Layer 10 0 500 1000 Sample Index 1 tnenopmoC Layer 11 0 500 1000 Sample Index tnenopmoC Layer 12 0 500 1000 Sample Index tnenopmoC Layer 13 0 500 1000 Sample Index tnenopmoC Layer 14 0 500 1000 Sample Index tnenopmoC Layer 15 noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP Figure 15: One component PLS model trained on Llama-3.2-1B instruct model activations to predict entities\u2019 populationsize. 0.050 0.025 0.000 0.025 0.050 0.05 0.00 0.05 Component 1 tnenopmoC Layer 0 25 0 25 Component 1 tnenopmoC Layer 1 0 50 Component 1 tnenopmoC Layer 2 0 50 Component 1 tnenopmoC Layer 3 0 50 Component 1 tnenopmoC Layer 4 50 0 Component 1 tnenopmoC Layer 5 50 0 Component 1 tnenopmoC Layer 6 50 0 Component 1 tnenopmoC Layer 7 50 0 Component 1 tnenopmoC Layer 8 50 0 Component 1 2 tnenopmoC Layer 9 50 0 Component 1 2 tnenopmoC Layer 10 50 0 Component 1 2 tnenopmoC Layer 11 50 0 Component 1 tnenopmoC Layer 12 50 0 Component 1 tnenopmoC Layer 13 0 50 Component 1 tnenopmoC Layer 14 50 0 Component 1 tnenopmoC Layer 15 noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP noitalupoP Figure 16: Two component PLS model trained on Llama-3.2-1B instruct model activations to predict entities\u2019 populationsize.",
      "chunk_index": 14,
      "word_count": 322,
      "token_count": 418,
      "metadata": {
        "start_char": 45857,
        "end_char": 47783,
        "has_overlap": true,
        "sentence_count": 2
      }
    }
  ]
}