{
  "pdf_path": "data/uploads/2406.12934v1.pdf",
  "title": "We examine intrinsic widerangeofcriticaltaskssuchaslawcite[],medandextrinsicbiasevaluationmethodsanddis- ical health record management cite[], etc.",
  "sections": {
    "Full Text": "Current state of LLM Risks and AI Guardrails SuriyaGaneshAyyamperumal, CarnegieMellonUniversity,sayyampe@andrew.cmu.edu LiminGe, CarnegieMellonUniversity,liminge@andrew.cmu.edu Abstract (Gormley and Frühwirth-Schnatter, 2019) .\n\nThe exact number of parameters for the frontrunning Large language models (LLMs) have beproprietarymodelssuchasChatpGPT-4fromOpecome increasingly sophisticated, leading to nAIandGemini1.5-UltrafromGooglehavebeen widespread deployment in sensitive applicaguardedsecretlyandhaven’tbeenrevealedpublicly. tionswheresafetyandreliabilityareparamount.\n\nHowever,LLMshaveinherentrisksaccompa- Alongside proprietary LLMs with Large paramenying them, including bias, potential for unters,therehasalsobeenasurgeinopensourceand safeactions,datasetpoisoning,lackofexplainproprietaryLLMsthatcanbedeployedinasingle ability,hallucinations,andnon-reproducibility. computer with as low as 3 Billion parameters to These risks necessitate the development of 100sofBillionparameters. \"guardrails\" to align LLMs with desired be- Sincethisisaveryfastevolvingfield,multiple haviorsandmitigatepotentialharm. onlinesourceshavebeencitedinthispaper.\n\nThis work explores the risks associated with LLMs are probabilistic next word prediction deploying LLMs and evaluates current apmodels,whicharebeingincreasinglydeployedina proachestoimplementingguardrailsandmodel alignment techniques.\n\nWe examine intrinsic widerangeofcriticaltaskssuchaslawcite[],medandextrinsicbiasevaluationmethodsanddis- ical health record management cite[], etc.\n\nThese cusstheimportanceoffairnessmetricsforremodelsarestillpronetotheabovestatedrisksand sponsibleAIdevelopment.\n\nThesafetyandreliexposestheuseraswellasthemaintainerofthese abilityofagenticLLMs(thosecapableofrealwidelydeployedmodelstoawiderangeofRisks worldactions)areexplored, emphasizingthe and challenges including but not limited to Bias needfortestability,fail-safes,andsituational and Fairness, Dataset Poisoning, Explainability, awareness.\n\nHallucinationsandPrivacy.\n\nTechnicalstrategiesforsecuringLLMsarepre- SinceLLMscontaintrillionsofparameters,itis sented, including a layered protection model operatingatexternal,secondary,andinternal hardtopredictorprovehoworwhatamodelwould levels.\n\nSystemprompts,Retrieval-Augmented provide as an output in a given moment.\n\nThese Generation (RAG) architectures, and techprovidesignificantproblemsoverhowamodelis niques to minimize bias and protect privacy safelydeployed.\n\nTherehavebeenmultipleexamarehighlighted. ples of real-world deployments of LLMs having Effectiveguardraildesignrequiresadeepununexpectedrunawaybehaviours,resultinginmonderstanding of the LLM’s intended use case, etaryaswellasreputationalloss. relevantregulations,andethicalconsiderations.\n\nOnewayofreducingtheserisksistoimplement Strikingabalancebetweencompetingrequiresafetyprotocolsintendedtocontrolthebehaviour ments,suchasaccuracyandprivacy,remains oftheseLLMs.\n\nTheseareprovidedintheformof anongoingchallenge.\n\nThisworkunderscores theimportanceofcontinuousresearchandde- Programmable\"guardrails\"bymodeldevelopers- velopmenttoensurethesafeandresponsible algorithmsthatmonitortheinputsandoutputsof useofLLMsinreal-worldapplications. anLLM.Guardrailscan,forinstance,stopLLMs fromprocessingharmfulrequestsormodifyresults 1 Introduction to be less dangerous or conform to the deployers There has been a huge surge in deployments and specificrequirementonmorality. utilizationofLLMs,withTrillionsofparameters, Effective guardrails design is difficult and nuwithaMixtureofExperts(MoE)basedarchitecture anced.\n\nMost Difficulty in Designing a good nuJ ]RC.sc[ 1v43921.6042:viXra guardrails often is defining the requirements and security/deploymentriskforLLMs. (Badyaletal., expectationfromtheMLmodel.\n\nForexample,Reg- 2023) ulations vary between fields, country and region.\n\nEthical requirements like fairness or avoiding of- 2.1.1 IntrinsicBiasEvaluation fensiveresponsesarehardtodefineconcretelyin Intrinsic Biases are evaluated by looking at the anactionableway. internal word embedding cite[] representation of Despiteallthis,whendeployinganLLMalongan LLM without directly looking at the outputs. sideorintegratedwithaconreteapplication.\n\nEven TheseincludeWordEmbeddingAssociationTest commonsenserequirementssuchasreducinghal- (WEAT) (Li et al., 2021) target words (e.g., genlucination (Xu et al., 2024) , toxicity (Wen et al., der,race)andattributewords(e.g.,\"pleasant\",\"un- 2023)andbiases(Taubenfeldetal.,2024)arenonpleasant\").\n\nHigh association scores may signal trivial tasks that are being explored.\n\nThe requirebias.\n\nSECT (Sentential Embedding Association menttocatertoaspecificutilitarianusecasecom- Test)(PriorandBentin,2008)isanothermeasure poundstheproblem,makingitharder.\n\nOftentimes, thatisSimilartoWEATbutusessentencesinstead Model developers have competing requirements. ofsinglewordsformorenuancedanalysis.\n\nForexample,modeldevelopersneedtoselectbetween Accuracy vs Privacy of the model, if the 2.1.2 ExtrinsicBiasEvaluation trainingdataisscrubbedofprivatedataandprivacy TheseevaluatebiasbyutilizingtaskspecificbenchfriendlyMachineLearningalgorithmsareused,acmarks,dependingonthecontextoftheLLMsdecuracy takes a hit, resulting in worse performing ployment.\n\nSomepopulardatasetstodetectbiases models. are, AnOptimalguardrailwouldconformthemodel Winogender(Rudingeretal.,2018): Adataset totherequiredtaskandpreventdamagewhenthe assessinggenderbiasinpronounresolutiontasks. modelstraysoutofitspath.\n\nThereisnokillalltype StereoSet(Nadeemetal.,2020): Measuresbias solutiontotheseproblems.\n\nCurrently,deployersof across professions, religion, etc., in various NLP LLMsaretakingamulti-prongedpathwhereeach taskslikesentimentanalysisandquestionanswerspecialconditionistaggedandhandledprogram- ing. matically.\n\nTherearealsoattemptstobuildexpert CounterfactualEvaluation: Involvesgenerating modelsoptimizedforspecificoutputs. text withaltered protectedattributes (e.g., chang- Thekeyobjectiveofthisworkwasto: inggender)andobservinghowthemodel’soutput changes. • EnumeratetheRiskexposurewhenworking withanddeployingLargeLanguageModels. 2.2 Fairness • EvaluatethecurrentstateofTechnicalandIm- Fairnessinmachinelearning(PessachandShmueli, plementationChallengesofLLMGuardrails 2022)seekstoensurethatmodelsmakedecisions andModelAlignmentworkstoprovideSafety withoutunjustdiscrimination.\n\nAchievingfairness Guaranteesduringdeployment. requirescarefulattentiontohowdataiscollected, how models are trained, and how their outcomes 2 LargeLanguageModelRisks are interpreted.\n\nIt aims to create algorithms that 2.1 Bias treatallindividualsimpartiallyandpromoteequitableoutcomes.\n\nBias in LLMs (Yeh et al., 2023) (von der Heyde et al., 2023) refer to systematic errors or devia- 2.3 AgenticSystems tionsinpromptoutputs. thatfavorcertainindividualsorgroupsoverothers,oftenbasedonsensitive AgenticLargeLanguageModelsaresystemswith characteristicslikerace,gender,orsocioeconomic theabilitytotakerealworldactionsfromthedigital status.\n\nThisgenerallyduetobiasedtrainingdata, environments.\n\nThese are models that are able to whichreflectsexistingsocietalprejudices,which move beyond simple text generation and include waspresentinthetrainingmaterialthattheseLLMs searchingforinformationonline,controllingsmart weretrainedon.\n\nIthasbeenshownthatLLMscan devicescite[],createdataanalysisstrategies(Yang alsobeintentionallybiasedbyprompts,posinga etal.,2024),etc. 2.4 SafetyandReliabilty isusedforsensitivetasks.\n\nAsshownbysomeonetlal cite[], LLMs can be jailbroken with specific Safety and Reliability is Pivotal when deploying wakewordsbypoisoningthedatasetthattheywere LLMsthatareabletohaverealworldimpact.\n\nTreattrainedon. ingLLMsasaprobabilisticblackboxwouldhave real world impact, especially when these agents 2.6 ExplainabilityandInterpretability startmakingdecisionsthatcrosstheboundaryfrom digitaltophysical. (Chanetal.,2023) LLMs,duetotheircomplexneuralnetworkarchitectures,areoftenreferredtoasblackboxes.\n\nThese 2.4.1 Testability LLMs contain Trillions of parameters making it ThisinvolvessubjectingtheLLMstoawiderange extremelydifficulttounderstandexactlyhowthey of simulated situations, including those that are arrive at their decisions or why they generate a unusual or extreme (edge cases).\n\nThe goal is to particularpieceoftext.\n\nThelackofexplainability identifyanypotentialresponsesfromtheLLMthat andinterpretabilitymakesithardtotrustanLLM’s couldtriggerunsafeactionsbythesystemandadoutput,especiallyinhigh-stakessituations. dressthoseissuesbeforereal-worlddeployment.\n\nA Whendeployedinsituationswherethereneedto goodimplementationofGuardrailswouldbeable beclearjustifications,suchasmedicalcite[],and tohandlethis. legalcite[]anunexplainablemodelmightbeunusable, regardlessofitsaccuracy.\n\nAdditionally, the 2.4.2 FailSafes lackofinterpretabilityposesachallengeindebug- These are essential backup systems designed to ginandimprovingthemdoel.\n\nTheStateoftheart take control if the LLM malfunctions.\n\nFail-safes GPT-4,became\"lazy\"duringthedecember2024, might include manual human overrides or proandevenMLscientistsworkingontheforefrontof grammedinstructionstoautomaticallyshutdown LLMscouldn’tfigureouttheissuecite[] certainprocessestopreventthesystemfromcausingharmordamage. 2.7 Hallucinations 2.4.3 SituationalAwareness Hallucinationsareamajorconcernwhenitcomes For an LLM to reliably guide a system’s actions, tolargelanguagemodels(LLMs).\n\nFabricatedoutit needs continuous updates about the state of its putcanbesurprisinglyconvincing,oftencontainsurroundings.\n\nBut, by design LLMs are stateless ingplausiblesoundingdetailsoradheringtogramnext word prediction machines, to work around maticalrules.\n\nThishasleadtorealworldimpacts this problem, the state is provided in the Prompt suchaslawyersgettingcaughtcitingnonpresent Contextwindowcite[]Thiswouldincludethings legalprecedencecite[] gatheredbysensors(cameras,locationdata,tem- TheproblemarisesbecauseLLMsaretrainedon peraturemonitors,etc.) toallowtheLLMtomake massivedatasetsoftextandcode,andtheylearnto informedandcontextuallyappropriatedecisions. identifypatternsandstatisticalrelationshipswithin that data.\n\nHowever, they don’t inherently under- 2.5 PoisonedDatasets standtherealworldorpossesstheabilitytodiscern Largelanguagemodels(LLMs)aretrainedonmastruthfromfiction.\n\nThiscanleadthemodeltocreate siveamountsoftextdata.\n\nPoisoneddatasetsoccur seeminglyfactualresponsesthatareentirelymade whenthistrainingdataintentionallyincludesmaup. licious,misleading,orharmfulcontent.\n\nThegoal Hallucinations pose a significant risk because ofpoisoningistomanipulatethemodel’soutput, theyerodetrustinLLMs.\n\nIfausercan’tbecertain causingittogeneratebiased,offensive,orunsafe theinformationtheyreceiveisaccurate,themodel text. becomesunreliable.\n\nThisisespeciallydangerous Poisoned datasets present significant risks.\n\nA indomainswherefactualaccuracyiscrucial,such compromised model might perpetuate harmful as healthcare or finance.\n\nFurthermore, hallucinastereotypes,spreadmisinformation,orbeusedto tions can be malicious, as they could be used to generatematerialthattargetsordegradesspecific spreadmisinformationorcreatefakecontentthat groups.\n\nTheconsequencescanrangefromreputaswayspublicopinion.\n\nTherefore,mitigatinghallutional damage for the organization deploying the cinationsisanongoingareaofresearchinthefield LLM to more direct threats to users if the model oflargelanguagemodels. 2.8 Nonreproducability differentlayers.\n\nAsdescribedin??eachriskhas multiplemitigationstrategiesatdifferentlayers.\n\nNon-reproducabilityinLLMsistheirtendencyto generatedifferentoutputsforthesameinput,irre- 3.1.1 GateKeeperLayer spectiveofthetimeframe.\n\nThisinconsistencystems Systempromptsareinstructionsorguidelinesgiven fromthestochasticnatureofLLMscite[].\n\nStochastoaLLMtoshapeitsbehaviorandresponses.\n\nThey ticelements,essentiallyrandomfactors,arewoven actlikeacompass,steeringtheLLM’sfocusand into an LLM’s training process, and influence its definingparametersforitsoutput.\n\nSystemprompts internalcalculations. can establish the LLM’s role (e.g., helpful assis- Thislackofconsistencyposessignificantchal- tant,creativewriter),setthedesiredtone(formal, lengeswhennon-reproducabilityunderminesrealplayful),providebackgroundcontext,specifyoutworldapplications.\n\nEvaluatingLLMperformance putformats,orevenincorporaterulestokeepthe becomesunreliablewhenbenchmarkresultsfluc- LLM’sresponsessafeandappropriate.\n\nBythoughttuate.\n\nMoreimportantly.\n\nInscenarioswhereconfully designing system prompts, developers can sistentresponsesarecrucial,likechatbotsusedin significantlyinfluencehowtheLLMbehaves,taicustomerservice,unpredictableoutputscanleadto loring its responses to specific applications and userfrustrationandabreakdownintrust.\n\nToensure minimizingtheriskofundesirableoutput. thedependabilityofLLMs,researchersareactively To mitigate risks associated with malicious exploringmethodstomitigatenon-reproducability prompts and providing guarantees about the roandensureamoreconsistentuserexperience. bustness of the LLM.\n\nA key protection layer lies 2.9 Privacyandcopyright outsidetheLLMitself,servingtodetectpotential attacks such as prompt injection.\n\nThese attacks LLMsareknowntomemorizecompletetextsand aimtomanipulatethemodel’sbehavior,generating reproduceprivatedatapresentinthetrainingdata. biasedorunintendedoutputs.\n\nByidentifyingma- Thiscouldbedatathatwasproprietaryorinadver- liciouspromptsduringthetokenizationstage,this tentlypartofadataleak.\n\nTherehavebeenadverlayercanpreventharmfulconsequenceslikedata sarialwaytofetch breaches. 3 StrategiesinSecuringLargeLanguage To bolster these defenses, the Gatekeeper protectionlayermayemploystrategieslikeon-the-fly models prompt rephrasing and rewriting.\n\nThis preserves Securinglargelanguagemodels(LLMs)isamultithe core intent of the prompt while neutralizing facetedendeavorduetotheiruniquevulnerabilities potential areas of vulnerability.\n\nThis layer leverandthesensitivedatatheyoftenprocess.\n\nProtecting agesspecializedmodelsandagentstocomprehenthesemodelsrequiresacombinationofrobustac- sively evaluate incoming prompts.\n\nThese evaluacesscontrolstopreventunauthorizeduse,careful tion agents, while often smaller than the primary monitoring of inputs and outputs to detect mali- LLM,offertargetedexpertiseforpromptanalysis. ciouspromptsorharmfulresponses.\n\nIt’sessential It’simportanttoacknowledgethateventhemost to stay vigilant about potential biases embedded sophisticatedprotectionmeasurescanoccasionally withinthetrainingdataandactivelyworktomitibecircumvented.\n\nContinuousresearchanddevelopgatetheireffects.\n\nAdditionally,itisalsoimportant mentareessentialtostayaheadofevolvingattack toprovideawaytovalidatethemodelsoutputto strategiesandtoensurethatLLMsremainsecure bevalidornot. andalignedwiththeirintendedpurpose. 3.1 LayeredProtectionmodel 3.1.2 KnowledgeAnchorLayer Broadly,guardrailsarecreatedatdifferentlayersof This is the layer where the decision on which tomodelaccess.\n\nTheleastprivilegedaccessmethod kens should be included in the model is decided. toanLLMisthroughNetworkAPIsasprovided This stage is primarily used to protect from risks by OpenAI, Anthropic and other model hosters. such as Hallucinations and Ground the model to Inthesecases,theattacker/usersdon’tgetaccess reality.\n\nThisisdonebyprovidedbycreatingVector tothemodelarchitecture,Probabilitydistribution data of tokens, or training data.\n\nIn these cases, secur- LLMsareknownfortheirtendencyto\"halluciingandguardrailingcanbedoneateachofthese nate\"orgenerateresponsesthatareplausiblebut KnowledgeAnchor GatekeeperLayer ParametricLayer Layer -InContextLearning -Task/domainspecific (Jietal.,2023) -RAGwithshort finetuning (Minetal.,2022) CosineDistance (Fernández-Martínezetal.,2022) (Asaietal.,2024) Response -InstructionTuning Reliability (Pengetal.,2023) -Finetunedspecialist -CorrectiveRAG models (Yanetal.,2024) (Ngiametal.,2018) -fewshotprompting (Schimanskietal.,2024) (Liuetal.,2024) -Selfconsistency check -KnowledgeGraphs -Unfamiliarfinetuning Hallucination (Lecky,1945) (Abu-Rasheedetal.,2024) (Kangetal.,2024) (Minetal.,2023) (Vamsietal.,2024) (Chenetal.,2023) -Incontextexamples tocounterbalancethebias (Dwivedietal.,2023) (Schubertetal.,2024) -Fairnessguided -RAGwith -Finetunewithsynthetic Bias few-shotprompting couterbalanceddata datatobalance (Maetal.,2023) (Shresthaetal.,2024) (VanBreugeletal.,2021) -Validatetheresponse forbias (Huangetal.,2024) -PromptInjection (Choietal.,2022) -MembershipInference -Tighter -Adversarialfinetuning Protectionfrom attack cosinesimilarity (Jeddietal.,2020) Adversary (Shokrietal.,2017) (Rahutomoetal.,2012) (Chenetal.,2020) -HardenedSystem Prompts. (Zhengetal.,2023) -Validateresponsewith externaltruth. (Zhuangetal.,2024) -Counterexamplefinetuning Unknowability - (Jhaetal.,2023) -Mastermodelwith promptunknowability (Zhengetal.,2024) Table1: RisksandmitigationstrategiesatdifferentlayersofanLLM factually inaccurate.\n\nRAG architectures address 4 ChallengesinImplementing thisvulnerabilitybygroundingLLMresponsesin Guardrails providedsourcematerial.\n\nInsteadofsimplyinvent- 4.1 FlexibilityvsStability ing information, the LLM works with a curated knowledge base, increasing the likelihood of ac- AI systems enable the flexibility to adapt, learn, curate and reliable output.\n\nThis makes it more and evolve in response to new data and changdifficultforattackerstoexploitLLMweaknesses ing circumstances.\n\nThis is crucial for their conandspreadmisinformation. tinuedeffectivenessindynamicreal-worldenviron- One risk associated with LLMs is that they ments.\n\nBut,stabilityisessentialtoensurethatthe can sometimes leak private information that was AIsystemoperateswithinpredictableboundaries, presentintheirtrainingdata.\n\nRAGsmitigatethis avoiding unintended consequences and maintainbyreducingtheLLM’sdirectrelianceoninternal ingalignmentwithethicalprinciples.\n\nOverlyrigid memorized training data.\n\nWhen an LLM is inguardrailslimitsthepotentialbenefitsofAI,while structedtoretrieveandprocessinformationfrom insufficientsafeguardsexposesystemstoriskssuch external documents, it lessens the chance of acasbias,safetyhazards,andmisuse.\n\nFindinganopcidentallyregurgitatingsensitiveinformationthat timalequilibriumbetweentheseinbothlanguage may have been part of its original training.\n\nThis tasksaswellasAgentictasks,isanongoingconhelpsprotectprivacyandlessensthepotentialfor versationbetweenindustryleadersandothers. databreaches. 4.2 EmergentComplexity 3.1.3 ParametricLayer When building with non-deterministic systems, therearemultiplefallbacksandscaffoldingsbuilt Thesearethechangesdonetothemodelparameters to provide a certain level of confidence over the eitherbyfinetuning,oraddressingproblemsduring system.\n\nBut, these are strung together with texts thepretrainingprocessofthemodel. andpromptsthatarenebulousandhardtoevaluate Researchersareactivelyexploringtechniquesto both qualitatively and quantitatively.\n\nCreating a minimizebiasespresentinlargelanguagemodels deterministic software system when the underly- (LLMs).\n\nMethodsrangefromfine-tuningexisting ing systems are non-deterministic is an ongoing modelsoncarefullycurateddatatomodifyingthe challengefordeployersoftheseLLMsystems. pre-trainingprocessitself.\n\nTheseapproachesaim to address biases related to stereotypes, cultural 4.3 Uncleargoalsandmetrics insensitivity,andunfairrepresentationsofvarious Often times when building LLM systems, the regroups. quirements on what is expected of the system is ProtectingtheprivacyofdatausedtotrainLLMs unclear.\n\nFor Example, cosine similarity is by far and the privacy of model outputs is crucial.\n\nDifthe most popular metric utilized by all LLM apferentialPrivacy(DP)isawidelyusedframework plication to validate their response.\n\nBut, cosine used for this purpose.\n\nTechniques include modisimilarityisshowntobeveryfoggyandunableto fying the fine-tuning process with DP principles, predictthenextsteps. applyingLocalDifferentialPrivacy(LDP),andexploringtheconceptofcontextualprivacytotailor 4.4 SystemTestabilityandEvolvability protectionsbasedonhowsensitiveinformationis WhenbuildingwithLLMsitishardtoprovethat withinaspecificcontext. thesystemwillfunctionasdesignedandnotgobe- Enhancing the safety of LLMs can be done by yondtheconditionalassumptionsofthedeveloper. adding adversarial examples to training data to Oftenitisthecasethatthedeveloperdoesnoteven make the models more robust to harmful inputs. controlthemodelwhichisdoingtheinformation Researchers focus on refining the reinforcement processing.\n\nItisalsohardtoevaluatethemodelin learningwithhumanfeedback(RLHF)process,ustermsofperformanceincreaseordegradationover ingmechanismstofilteroutharmfulresponsesand time. modifyinglossfunctions.\n\nHowever,directlyapply- 4.5 Cost ingtraditionalrobustnesstechniquescanbechallenging for LLMs due to complexities like catas- Acommontechniqueindetectingandpreventing trophicforgetting. malicious input or biased response, is to use a \"Judge\" LLM to evaluate and rate the responses. alsoabletodofuzzymatchingforoutputsemantic Thisisprohibitivelycostlybothfinanciallyaswell correctness.\n\nGuardrailsAIisalsoabletoleverage as in time to response.\n\nIt has been shown that other LLMs to evaluate and make decisions on smallerspecialized\"Judge\"modelsperformmuch the outputs and inputs.\n\nXML has been shown to betterthanlargelanguagemodelswithtrillionsof bebetterunderstood,processed,andfollowedby parametersinevaluatingLLMresponses.\n\nLLMsbecauseofhowtokenizationworks. 5 OpenSourceTools 6 Limitations ToenablebuildingmorestableLLMbasedappli- LLMutilizationanddeploymentisafastevolving cation,therehasbeenasurgeinopensourcetools field,withalotofresearchanddevelopmentbeing specifically built to enable guardrailing.\n\nThese doneconcurrently.\n\nWhile,ourexplorationhasbeen toolsalltakeaslightlydifferentapproachtosolvdepthfirstwithafairamountofbreadth.\n\nSomeof ingtheseproblems. thestrategiesmightgetobsoleteorprovenwrong, veryfast. 5.1 Nemo-Guardrails Nemo-Guardrails(Rebedeaetal.,2023)hasaDo- 7 Conclusion mainSpecificLanguage(DSL)calledColang.\n\nItis possibledefinenewandreusepredefinedguardrails LLMs offer exceptional potential for transformain the library.\n\nThe decision on whether an evalutionacrossvariousindustries.\n\nHowever,theirinherationneedstobedoneornotisdecidedbasedon entbiases,safetyconcerns,andpotentialformisuse the colang configuration.\n\nIt is possible to setup necessitate the development of robust guardrails. separateprocessesateverystepofanInput/Output This work has explored the risks associated with pipeline.\n\nThis also includes manipulating output deployingLLMsandthecurrentstateoftechnical from a Retrieval-Augmented-Generation System strategiesandmodelalignmenttechniques. aswell.\n\nWe examined methods for bias evaluation and Nemo-GuardrailsheavilyutilizesLLMssuchas the significance of fairness metrics.\n\nFor agentic Chat-gpt 4 and to evaluate and decide what next LLMs,wehighlightedtheneedforrigoroustesting, stepstoperform.\n\nThiscouldposeapotentialrisk, fail-safes,andsituationalawarenesstoensuresafe whereithasrecentlybeenshownthatmodelsare andreliableoperationinreal-worldenvironments. biased towards their own outputs.\n\nInterestingly TechnicalapproachestosecuringLLMs,including Nemo lacks controls on the cost of running a relayeredprotectionmodels,systemprompts,RAG quest,hencetherecouldbecaseswhereasimple architectures,andbiasmitigation,werediscussed. inferencecouldcostalotmorethanexpected.\n\nCrucialchallengesremaininimplementingthese guardrails.\n\nFinding the optimal balance between 5.2 LLamaGuard flexibilityandstabilityinAIsystemsisessentialto LLamaGuard(Inanetal.,2023)isasolutioncrepreserve both their adaptability and ethical alignated by Meta based on llama2-7b model for clasment.\n\nCleardefinitionsofgoalsandmetrics,testasifyingInput,Outputpairsintocategoriessuchas bility, and cost optimization are further areas resensitivity.\n\nLlama Guard performed better than quiringfocus.\n\nOpen-sourcetoolsofferpromising othertraditionalmethodsofclassificationbecause avenuesforbuildingmorereliableLLM-basedapof its ability to understand semanticity of words plications. andsentences.\n\nEffective guardrail design demands a deep un- LlamaGuard was built by creating a dataset of derstanding of an LLM’s intended use, relevant ideal input/ouput pairs for certain case and then regulations, and ethical considerations.\n\nDespite fine-tuningafoundationalLLama-7B.Theannotathe complexities, developing reliable safeguards tionwasdonebyMeta’sredteam. isparamountformaximizingLLMs’benefitsand minimizing their potential harms.\n\nContinued re- 5.3 GuardrailsAI search,development,andopencollaborationacross GuardrailsAIissimilartoNemo-Guardrailsinthat, thefieldarevitaltoensuringthesafe,responsible, italsodefinesaDomainSpecificLanguagecalled andequitableuseofLLMsintheyearstocome.\n\nRAILSwhichisasacustomizedXMLformat,itis References Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael HasanAbu-Rasheed,ChristianWeber,andMadjidFathi.\n\nTontchev,QingHu,BrianFuller,DavideTestuggine, 2024.\n\nKnowledge graphs as context sources for et al. 2023.\n\nLlama guard: Llm-based input-output llm-basedexplanationsoflearningrecommendations. safeguardforhuman-aiconversations. arXivpreprint arXivpreprintarXiv:2403.03008. arXiv:2312.06674.\n\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Ahmadreza Jeddi, Mohammad Javad Shafiee, and Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and AlexanderWong.2020.\n\nAsimplefine-tuningisall Wen-tau Yih. 2024.\n\nReliable, adaptable, and atyouneed: Towardsrobustdeeplearningviaadversartributable language models with retrieval. arXiv ialfine-tuning. arXivpreprintarXiv:2012.13628. preprintarXiv:2403.03187.\n\nSumit Kumar Jha, Susmit Jha, Patrick Lincoln, Nicklaus Badyal, Derek Jacoby, and Yvonne Coady.\n\nNathaniel D Bastian, Alvaro Velasquez, Rickard 2023.\n\nIntentionalbiasesinllmresponses.\n\nIn2023 Ewetz,andSandeepNeema.2023.\n\nCounterexample IEEE14thAnnualUbiquitousComputing,Electronguidedinductivesynthesisusinglargelanguagemod- ics & Mobile Communication Conference (UEMelsandsatisfiabilitysolving.\n\nInMILCOM2023-2023 CON),pages0502-0506.IEEE.\n\nIEEE Military Communications Conference (MIL- AlanChan,RebeccaSalganik,AlvaMarkelius,Chris COM),pages944-949.IEEE.\n\nPang,NitarshanRajkumar,DmitriiKrasheninnikov, ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,Dan LauroLangosco,ZhonghaoHe,YawenDuan,Micah Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Carroll,etal.2023.\n\nHarmsfromincreasinglyagentic Madotto,andPascaleFung.2023.\n\nSurveyofhallucialgorithmic systems.\n\nIn Proceedings of the 2023 nationinnaturallanguagegeneration.\n\nACMComput.\n\nACM Conference on Fairness, Accountability, and Surv.,55(12).\n\nTransparency,pages651-666.\n\nTianlongChen,SijiaLiu,ShiyuChang,YuCheng,Lisa Katie Kang, Eric Wallace, Claire Tomlin, Aviral Ku- Amini, and Zhangyang Wang. 2020.\n\nAdversarial mar,andSergeyLevine.2024.\n\nUnfamiliarfinetuning robustness: Fromself-supervisedpre-trainingtofineexamplescontrolhowlanguagemodelshallucinate. tuning.\n\nInProceedingsoftheIEEE/CVFconference arXivpreprintarXiv:2403.05612. oncomputervisionandpatternrecognition, pages Prescott Lecky. 1945.\n\nSelf-consistency; a theory of 699-708. personality.\n\nXinyunChen,RenatAksitov,UriAlon,JieRen,Kefan Ming-HuiLi,Pei-WeiLi,andLi-LinRao.2021.\n\nSelf- Xiao,PengchengYin,SushantPrakash,CharlesSutothermoralbias: Evidencefromimplicitmeasures ton,XuezhiWang,andDennyZhou.2023.\n\nUniversal andtheword-embeddingassociationtest.\n\nPersonalself-consistencyforlargelanguagemodelgeneration. ityandIndividualDifferences,183:111107. arXivpreprintarXiv:2311.17311.\n\nXiaoming Liu, Chen Liu, Zhaohan Zhang, EunbiChoi,YongraeJo,JoelJang,andMinjoonSeo.\n\nChengzhengxu Li, Longtian Wang, Yu Lan, 2022.\n\nPrompt injection: Parameterization of fixed and Chao Shen. 2024.\n\nStablept: Towards stable inputs. arXivpreprintarXiv:2206.11349. promptingforfew-shotlearningviainputseparation.\n\nSatyamDwivedi,SanjuktaGhosh,andShivamDwivedi. 2023.\n\nBreaking the bias: Gender fairness in llms HuanMa,ChangqingZhang,YataoBian,LemaoLiu, using prompt engineering and in-context learning.\n\nZhiruiZhang,PeilinZhao,ShuZhang,HuazhuFu, RupkathaJournalonInterdisciplinaryStudiesinHu- Qinghua Hu, and Bingzhe Wu. 2023.\n\nFairnessmanities,15(4). guidedfew-shotpromptingforlargelanguagemodels.\n\nInAdvancesinNeuralInformationProcessing FernandoFernández-Martínez,CristinaLuna-Jiménez, Systems,volume36,pages43136-43155.CurranAs- Ricardo Kleinlein, David Griol, Zoraida Callejas, sociates,Inc. andJuanManuelMontero.2022.\n\nFine-tuningbert modelsforintentrecognitionusingafrequencycut- MarcusJMin,YangruiboDing,LucaBuratti,Saurabh offstrategyfordomain-specificvocabularyextension.\n\nPujar,GailKaiser,SumanJana,andBaishakhiRay.\n\nAppliedSciences,12(3):1610. 2023.\n\nBeyondaccuracy: Evaluatingself-consistency ofcodellms.\n\nInTheTwelfthInternationalConfer- IsobelClaireGormleyandSylviaFrühwirth-Schnatter. enceonLearningRepresentations. 2019.\n\nMixture of experts models.\n\nIn Handbook ofmixtureanalysis, pages271-307.Chapmanand SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe, Hall/CRC.\n\nMikeLewis,HannanehHajishirzi,andLukeZettlemoyer.2022.\n\nRethinkingtheroleofdemonstrations: Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Whatmakesin-contextlearningwork?\n\nTiejun Zhao. 2024.\n\nAn empirical study of llmas-a-judge for llm evaluation: Fine-tuned judge Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. modelsaretask-specificclassifiers. arXivpreprint Stereoset: Measuringstereotypicalbiasinpretrained arXiv:2403.02839. languagemodels.\n\nJiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon KrishnaKommineniVamsi,VamsiKrishnaKommineni, Kornblith, Quoc V Le, and Ruoming Pang. 2018. andSheebaSamuel.2024.\n\nFromhumanexpertsto Domain adaptive transfer learning with specialist machines: An llm supported approach to ontology models. arXivpreprintarXiv:1811.07056. andknowledgegraphconstruction.\n\nBaolinPeng,ChunyuanLi,PengchengHe,MichelGal- BorisVanBreugel,TrentKyono,JeroenBerrevoets,and ley,andJianfengGao.2023.\n\nInstructiontuningwith MihaelaVanderSchaar.2021.\n\nDecaf: Generating gpt-4. fair synthetic data using causally-aware generative networks.\n\nAdvancesinNeuralInformationProcess- DanaPessachandErezShmueli.2022.\n\nAreviewonfairingSystems,34:22221-22233. nessinmachinelearning.\n\nACMComputingSurveys (CSUR),55(3):1-44.\n\nLeah von der Heyde, Anna-Carolina Haensch, and Alexander Wenz. 2023.\n\nAssessing bias in llm- AnatPriorandShlomoBentin.2008.\n\nWordassociations generated synthetic datasets: The case of german are formed incidentally during sentential semantic voter behavior.\n\nTechnical report, Center for Open integration.\n\nActaPsychologica,127(1):57-71.\n\nScience.\n\nFaisal Rahutomo, Teruaki Kitasuka, Masayoshi Arit- JiaxinWen,PeiKe,HaoSun,ZhexinZhang,Chengfei sugi,etal.2012.\n\nSemanticcosinesimilarity.\n\nInThe Li,JinfengBai,andMinlieHuang.2023.\n\nUnveiling 7th international student conference on advanced theimplicittoxicityinlargelanguagemodels. arXiv science and technology ICAST, volume 4, page 1. preprintarXiv:2311.17391.\n\nUniversityofSeoulSouthKorea.\n\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli.\n\nTraian Rebedea, Razvan Dinu, Makesh Narsimhan 2024.\n\nHallucination is inevitable: An innate lim- Sreedhar,ChristopherParisien,andJonathanCohen. itation of large language models. arXiv preprint 2023.\n\nNeMoguardrails: Atoolkitforcontrollable arXiv:2401.11817. andsafeLLMapplicationswithprogrammablerails.\n\nShi-QiYan,Jia-ChenGu,YunZhu,andZhen-HuaLing.\n\nInProceedingsofthe2023ConferenceonEmpirical 2024.\n\nCorrective retrieval augmented generation.\n\nMethods in Natural Language Processing: System arXivpreprintarXiv:2401.15884.\n\nDemonstrations,pages431-445,Singapore.AssociationforComputationalLinguistics.\n\nZhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Rachel Rudinger, Jason Naradowsky, Brian Leonard, Pengyuan Liu, Dong Yu, et al. 2024.\n\nMatplotaand Benjamin Van Durme. 2018.\n\nGender bias in gent: Method and evaluation for llm-based agencoreferenceresolution.\n\nInProceedingsofthe2018 tic scientific data visualization. arXiv preprint Conference of the North American Chapter of the arXiv:2402.11453.\n\nAssociationforComputationalLinguistics: Human LanguageTechnologies,NewOrleans,Louisiana.As- Kai-ChingYeh,Jou-AnChi,Da-ChenLian,andShusociationforComputationalLinguistics.\n\nKaiHsieh.2023.\n\nEvaluatinginterfacedllmbias.\n\nIn Proceedings of the 35th Conference on Computa- Tobias Schimanski, Jingwei Ni, Mathias Kraus, EltionalLinguisticsandSpeechProcessing(ROCLING liott Ash, and Markus Leippold. 2024.\n\nTo- 2023),pages292-299. wards faithful and robust llm specialists for evidence-basedquestion-answering. arXivpreprint LianminZheng,Wei-LinChiang,YingSheng,Siyuan arXiv:2402.08277.\n\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.\n\nJohannes A Schubert, Akshay K Jagadish, Marcel Judging llm-as-a-judge with mt-bench and chatbot Binz, and Eric Schulz. 2024.\n\nIn-context learning arena.\n\nAdvancesinNeuralInformationProcessing agentsareasymmetricbeliefupdaters. arXivpreprint Systems,36. arXiv:2402.03969.\n\nMingqianZheng,JiaxinPei,andDavidJurgens.2023.\n\nRezaShokri,MarcoStronati,CongzhengSong,andVi- Is\"ahelpfulassistant\"thebestroleforlargelanguage talyShmatikov.2017.\n\nMembershipinferenceattacks models? a systematic evaluation of social roles in againstmachinelearningmodels.\n\nIn2017IEEEsymsystemprompts. arXivpreprintarXiv:2311.10054. posium on security and privacy (SP), pages 3-18.\n\nIEEE.\n\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2024.\n\nToolqa: A dataset for llm Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, questionansweringwithexternaltools.\n\nAdvancesin YushengXie,andSiqiDeng.2024.\n\nFairrag: Fairhu- NeuralInformationProcessingSystems,36. mangenerationviafairretrievalaugmentation. arXiv preprintarXiv:2403.19964.\n\nAmirTaubenfeld,YanivDover,RoiReichart,andAriel Goldstein. 2024.\n\nSystematic biases in llm simulationsofdebates. arXivpreprintarXiv:2402.04049."
  },
  "full_text": "Current state of LLM Risks and AI Guardrails SuriyaGaneshAyyamperumal, CarnegieMellonUniversity,sayyampe@andrew.cmu.edu LiminGe, CarnegieMellonUniversity,liminge@andrew.cmu.edu Abstract (Gormley and Frühwirth-Schnatter, 2019) .\n\nThe exact number of parameters for the frontrunning Large language models (LLMs) have beproprietarymodelssuchasChatpGPT-4fromOpecome increasingly sophisticated, leading to nAIandGemini1.5-UltrafromGooglehavebeen widespread deployment in sensitive applicaguardedsecretlyandhaven’tbeenrevealedpublicly. tionswheresafetyandreliabilityareparamount.\n\nHowever,LLMshaveinherentrisksaccompa- Alongside proprietary LLMs with Large paramenying them, including bias, potential for unters,therehasalsobeenasurgeinopensourceand safeactions,datasetpoisoning,lackofexplainproprietaryLLMsthatcanbedeployedinasingle ability,hallucinations,andnon-reproducibility. computer with as low as 3 Billion parameters to These risks necessitate the development of 100sofBillionparameters. \"guardrails\" to align LLMs with desired be- Sincethisisaveryfastevolvingfield,multiple haviorsandmitigatepotentialharm. onlinesourceshavebeencitedinthispaper.\n\nThis work explores the risks associated with LLMs are probabilistic next word prediction deploying LLMs and evaluates current apmodels,whicharebeingincreasinglydeployedina proachestoimplementingguardrailsandmodel alignment techniques.\n\nWe examine intrinsic widerangeofcriticaltaskssuchaslawcite[],medandextrinsicbiasevaluationmethodsanddis- ical health record management cite[], etc.\n\nThese cusstheimportanceoffairnessmetricsforremodelsarestillpronetotheabovestatedrisksand sponsibleAIdevelopment.\n\nThesafetyandreliexposestheuseraswellasthemaintainerofthese abilityofagenticLLMs(thosecapableofrealwidelydeployedmodelstoawiderangeofRisks worldactions)areexplored, emphasizingthe and challenges including but not limited to Bias needfortestability,fail-safes,andsituational and Fairness, Dataset Poisoning, Explainability, awareness.\n\nHallucinationsandPrivacy.\n\nTechnicalstrategiesforsecuringLLMsarepre- SinceLLMscontaintrillionsofparameters,itis sented, including a layered protection model operatingatexternal,secondary,andinternal hardtopredictorprovehoworwhatamodelwould levels.\n\nSystemprompts,Retrieval-Augmented provide as an output in a given moment.\n\nThese Generation (RAG) architectures, and techprovidesignificantproblemsoverhowamodelis niques to minimize bias and protect privacy safelydeployed.\n\nTherehavebeenmultipleexamarehighlighted. ples of real-world deployments of LLMs having Effectiveguardraildesignrequiresadeepununexpectedrunawaybehaviours,resultinginmonderstanding of the LLM’s intended use case, etaryaswellasreputationalloss. relevantregulations,andethicalconsiderations.\n\nOnewayofreducingtheserisksistoimplement Strikingabalancebetweencompetingrequiresafetyprotocolsintendedtocontrolthebehaviour ments,suchasaccuracyandprivacy,remains oftheseLLMs.\n\nTheseareprovidedintheformof anongoingchallenge.\n\nThisworkunderscores theimportanceofcontinuousresearchandde- Programmable\"guardrails\"bymodeldevelopers- velopmenttoensurethesafeandresponsible algorithmsthatmonitortheinputsandoutputsof useofLLMsinreal-worldapplications. anLLM.Guardrailscan,forinstance,stopLLMs fromprocessingharmfulrequestsormodifyresults 1 Introduction to be less dangerous or conform to the deployers There has been a huge surge in deployments and specificrequirementonmorality. utilizationofLLMs,withTrillionsofparameters, Effective guardrails design is difficult and nuwithaMixtureofExperts(MoE)basedarchitecture anced.\n\nMost Difficulty in Designing a good nuJ ]RC.sc[ 1v43921.6042:viXra guardrails often is defining the requirements and security/deploymentriskforLLMs. (Badyaletal., expectationfromtheMLmodel.\n\nForexample,Reg- 2023) ulations vary between fields, country and region.\n\nEthical requirements like fairness or avoiding of- 2.1.1 IntrinsicBiasEvaluation fensiveresponsesarehardtodefineconcretelyin Intrinsic Biases are evaluated by looking at the anactionableway. internal word embedding cite[] representation of Despiteallthis,whendeployinganLLMalongan LLM without directly looking at the outputs. sideorintegratedwithaconreteapplication.\n\nEven TheseincludeWordEmbeddingAssociationTest commonsenserequirementssuchasreducinghal- (WEAT) (Li et al., 2021) target words (e.g., genlucination (Xu et al., 2024) , toxicity (Wen et al., der,race)andattributewords(e.g.,\"pleasant\",\"un- 2023)andbiases(Taubenfeldetal.,2024)arenonpleasant\").\n\nHigh association scores may signal trivial tasks that are being explored.\n\nThe requirebias.\n\nSECT (Sentential Embedding Association menttocatertoaspecificutilitarianusecasecom- Test)(PriorandBentin,2008)isanothermeasure poundstheproblem,makingitharder.\n\nOftentimes, thatisSimilartoWEATbutusessentencesinstead Model developers have competing requirements. ofsinglewordsformorenuancedanalysis.\n\nForexample,modeldevelopersneedtoselectbetween Accuracy vs Privacy of the model, if the 2.1.2 ExtrinsicBiasEvaluation trainingdataisscrubbedofprivatedataandprivacy TheseevaluatebiasbyutilizingtaskspecificbenchfriendlyMachineLearningalgorithmsareused,acmarks,dependingonthecontextoftheLLMsdecuracy takes a hit, resulting in worse performing ployment.\n\nSomepopulardatasetstodetectbiases models. are, AnOptimalguardrailwouldconformthemodel Winogender(Rudingeretal.,2018): Adataset totherequiredtaskandpreventdamagewhenthe assessinggenderbiasinpronounresolutiontasks. modelstraysoutofitspath.\n\nThereisnokillalltype StereoSet(Nadeemetal.,2020): Measuresbias solutiontotheseproblems.\n\nCurrently,deployersof across professions, religion, etc., in various NLP LLMsaretakingamulti-prongedpathwhereeach taskslikesentimentanalysisandquestionanswerspecialconditionistaggedandhandledprogram- ing. matically.\n\nTherearealsoattemptstobuildexpert CounterfactualEvaluation: Involvesgenerating modelsoptimizedforspecificoutputs. text withaltered protectedattributes (e.g., chang- Thekeyobjectiveofthisworkwasto: inggender)andobservinghowthemodel’soutput changes. • EnumeratetheRiskexposurewhenworking withanddeployingLargeLanguageModels. 2.2 Fairness • EvaluatethecurrentstateofTechnicalandIm- Fairnessinmachinelearning(PessachandShmueli, plementationChallengesofLLMGuardrails 2022)seekstoensurethatmodelsmakedecisions andModelAlignmentworkstoprovideSafety withoutunjustdiscrimination.\n\nAchievingfairness Guaranteesduringdeployment. requirescarefulattentiontohowdataiscollected, how models are trained, and how their outcomes 2 LargeLanguageModelRisks are interpreted.\n\nIt aims to create algorithms that 2.1 Bias treatallindividualsimpartiallyandpromoteequitableoutcomes.\n\nBias in LLMs (Yeh et al., 2023) (von der Heyde et al., 2023) refer to systematic errors or devia- 2.3 AgenticSystems tionsinpromptoutputs. thatfavorcertainindividualsorgroupsoverothers,oftenbasedonsensitive AgenticLargeLanguageModelsaresystemswith characteristicslikerace,gender,orsocioeconomic theabilitytotakerealworldactionsfromthedigital status.\n\nThisgenerallyduetobiasedtrainingdata, environments.\n\nThese are models that are able to whichreflectsexistingsocietalprejudices,which move beyond simple text generation and include waspresentinthetrainingmaterialthattheseLLMs searchingforinformationonline,controllingsmart weretrainedon.\n\nIthasbeenshownthatLLMscan devicescite[],createdataanalysisstrategies(Yang alsobeintentionallybiasedbyprompts,posinga etal.,2024),etc. 2.4 SafetyandReliabilty isusedforsensitivetasks.\n\nAsshownbysomeonetlal cite[], LLMs can be jailbroken with specific Safety and Reliability is Pivotal when deploying wakewordsbypoisoningthedatasetthattheywere LLMsthatareabletohaverealworldimpact.\n\nTreattrainedon. ingLLMsasaprobabilisticblackboxwouldhave real world impact, especially when these agents 2.6 ExplainabilityandInterpretability startmakingdecisionsthatcrosstheboundaryfrom digitaltophysical. (Chanetal.,2023) LLMs,duetotheircomplexneuralnetworkarchitectures,areoftenreferredtoasblackboxes.\n\nThese 2.4.1 Testability LLMs contain Trillions of parameters making it ThisinvolvessubjectingtheLLMstoawiderange extremelydifficulttounderstandexactlyhowthey of simulated situations, including those that are arrive at their decisions or why they generate a unusual or extreme (edge cases).\n\nThe goal is to particularpieceoftext.\n\nThelackofexplainability identifyanypotentialresponsesfromtheLLMthat andinterpretabilitymakesithardtotrustanLLM’s couldtriggerunsafeactionsbythesystemandadoutput,especiallyinhigh-stakessituations. dressthoseissuesbeforereal-worlddeployment.\n\nA Whendeployedinsituationswherethereneedto goodimplementationofGuardrailswouldbeable beclearjustifications,suchasmedicalcite[],and tohandlethis. legalcite[]anunexplainablemodelmightbeunusable, regardlessofitsaccuracy.\n\nAdditionally, the 2.4.2 FailSafes lackofinterpretabilityposesachallengeindebug- These are essential backup systems designed to ginandimprovingthemdoel.\n\nTheStateoftheart take control if the LLM malfunctions.\n\nFail-safes GPT-4,became\"lazy\"duringthedecember2024, might include manual human overrides or proandevenMLscientistsworkingontheforefrontof grammedinstructionstoautomaticallyshutdown LLMscouldn’tfigureouttheissuecite[] certainprocessestopreventthesystemfromcausingharmordamage. 2.7 Hallucinations 2.4.3 SituationalAwareness Hallucinationsareamajorconcernwhenitcomes For an LLM to reliably guide a system’s actions, tolargelanguagemodels(LLMs).\n\nFabricatedoutit needs continuous updates about the state of its putcanbesurprisinglyconvincing,oftencontainsurroundings.\n\nBut, by design LLMs are stateless ingplausiblesoundingdetailsoradheringtogramnext word prediction machines, to work around maticalrules.\n\nThishasleadtorealworldimpacts this problem, the state is provided in the Prompt suchaslawyersgettingcaughtcitingnonpresent Contextwindowcite[]Thiswouldincludethings legalprecedencecite[] gatheredbysensors(cameras,locationdata,tem- TheproblemarisesbecauseLLMsaretrainedon peraturemonitors,etc.) toallowtheLLMtomake massivedatasetsoftextandcode,andtheylearnto informedandcontextuallyappropriatedecisions. identifypatternsandstatisticalrelationshipswithin that data.\n\nHowever, they don’t inherently under- 2.5 PoisonedDatasets standtherealworldorpossesstheabilitytodiscern Largelanguagemodels(LLMs)aretrainedonmastruthfromfiction.\n\nThiscanleadthemodeltocreate siveamountsoftextdata.\n\nPoisoneddatasetsoccur seeminglyfactualresponsesthatareentirelymade whenthistrainingdataintentionallyincludesmaup. licious,misleading,orharmfulcontent.\n\nThegoal Hallucinations pose a significant risk because ofpoisoningistomanipulatethemodel’soutput, theyerodetrustinLLMs.\n\nIfausercan’tbecertain causingittogeneratebiased,offensive,orunsafe theinformationtheyreceiveisaccurate,themodel text. becomesunreliable.\n\nThisisespeciallydangerous Poisoned datasets present significant risks.\n\nA indomainswherefactualaccuracyiscrucial,such compromised model might perpetuate harmful as healthcare or finance.\n\nFurthermore, hallucinastereotypes,spreadmisinformation,orbeusedto tions can be malicious, as they could be used to generatematerialthattargetsordegradesspecific spreadmisinformationorcreatefakecontentthat groups.\n\nTheconsequencescanrangefromreputaswayspublicopinion.\n\nTherefore,mitigatinghallutional damage for the organization deploying the cinationsisanongoingareaofresearchinthefield LLM to more direct threats to users if the model oflargelanguagemodels. 2.8 Nonreproducability differentlayers.\n\nAsdescribedin??eachriskhas multiplemitigationstrategiesatdifferentlayers.\n\nNon-reproducabilityinLLMsistheirtendencyto generatedifferentoutputsforthesameinput,irre- 3.1.1 GateKeeperLayer spectiveofthetimeframe.\n\nThisinconsistencystems Systempromptsareinstructionsorguidelinesgiven fromthestochasticnatureofLLMscite[].\n\nStochastoaLLMtoshapeitsbehaviorandresponses.\n\nThey ticelements,essentiallyrandomfactors,arewoven actlikeacompass,steeringtheLLM’sfocusand into an LLM’s training process, and influence its definingparametersforitsoutput.\n\nSystemprompts internalcalculations. can establish the LLM’s role (e.g., helpful assis- Thislackofconsistencyposessignificantchal- tant,creativewriter),setthedesiredtone(formal, lengeswhennon-reproducabilityunderminesrealplayful),providebackgroundcontext,specifyoutworldapplications.\n\nEvaluatingLLMperformance putformats,orevenincorporaterulestokeepthe becomesunreliablewhenbenchmarkresultsfluc- LLM’sresponsessafeandappropriate.\n\nBythoughttuate.\n\nMoreimportantly.\n\nInscenarioswhereconfully designing system prompts, developers can sistentresponsesarecrucial,likechatbotsusedin significantlyinfluencehowtheLLMbehaves,taicustomerservice,unpredictableoutputscanleadto loring its responses to specific applications and userfrustrationandabreakdownintrust.\n\nToensure minimizingtheriskofundesirableoutput. thedependabilityofLLMs,researchersareactively To mitigate risks associated with malicious exploringmethodstomitigatenon-reproducability prompts and providing guarantees about the roandensureamoreconsistentuserexperience. bustness of the LLM.\n\nA key protection layer lies 2.9 Privacyandcopyright outsidetheLLMitself,servingtodetectpotential attacks such as prompt injection.\n\nThese attacks LLMsareknowntomemorizecompletetextsand aimtomanipulatethemodel’sbehavior,generating reproduceprivatedatapresentinthetrainingdata. biasedorunintendedoutputs.\n\nByidentifyingma- Thiscouldbedatathatwasproprietaryorinadver- liciouspromptsduringthetokenizationstage,this tentlypartofadataleak.\n\nTherehavebeenadverlayercanpreventharmfulconsequenceslikedata sarialwaytofetch breaches. 3 StrategiesinSecuringLargeLanguage To bolster these defenses, the Gatekeeper protectionlayermayemploystrategieslikeon-the-fly models prompt rephrasing and rewriting.\n\nThis preserves Securinglargelanguagemodels(LLMs)isamultithe core intent of the prompt while neutralizing facetedendeavorduetotheiruniquevulnerabilities potential areas of vulnerability.\n\nThis layer leverandthesensitivedatatheyoftenprocess.\n\nProtecting agesspecializedmodelsandagentstocomprehenthesemodelsrequiresacombinationofrobustac- sively evaluate incoming prompts.\n\nThese evaluacesscontrolstopreventunauthorizeduse,careful tion agents, while often smaller than the primary monitoring of inputs and outputs to detect mali- LLM,offertargetedexpertiseforpromptanalysis. ciouspromptsorharmfulresponses.\n\nIt’sessential It’simportanttoacknowledgethateventhemost to stay vigilant about potential biases embedded sophisticatedprotectionmeasurescanoccasionally withinthetrainingdataandactivelyworktomitibecircumvented.\n\nContinuousresearchanddevelopgatetheireffects.\n\nAdditionally,itisalsoimportant mentareessentialtostayaheadofevolvingattack toprovideawaytovalidatethemodelsoutputto strategiesandtoensurethatLLMsremainsecure bevalidornot. andalignedwiththeirintendedpurpose. 3.1 LayeredProtectionmodel 3.1.2 KnowledgeAnchorLayer Broadly,guardrailsarecreatedatdifferentlayersof This is the layer where the decision on which tomodelaccess.\n\nTheleastprivilegedaccessmethod kens should be included in the model is decided. toanLLMisthroughNetworkAPIsasprovided This stage is primarily used to protect from risks by OpenAI, Anthropic and other model hosters. such as Hallucinations and Ground the model to Inthesecases,theattacker/usersdon’tgetaccess reality.\n\nThisisdonebyprovidedbycreatingVector tothemodelarchitecture,Probabilitydistribution data of tokens, or training data.\n\nIn these cases, secur- LLMsareknownfortheirtendencyto\"halluciingandguardrailingcanbedoneateachofthese nate\"orgenerateresponsesthatareplausiblebut KnowledgeAnchor GatekeeperLayer ParametricLayer Layer -InContextLearning -Task/domainspecific (Jietal.,2023) -RAGwithshort finetuning (Minetal.,2022) CosineDistance (Fernández-Martínezetal.,2022) (Asaietal.,2024) Response -InstructionTuning Reliability (Pengetal.,2023) -Finetunedspecialist -CorrectiveRAG models (Yanetal.,2024) (Ngiametal.,2018) -fewshotprompting (Schimanskietal.,2024) (Liuetal.,2024) -Selfconsistency check -KnowledgeGraphs -Unfamiliarfinetuning Hallucination (Lecky,1945) (Abu-Rasheedetal.,2024) (Kangetal.,2024) (Minetal.,2023) (Vamsietal.,2024) (Chenetal.,2023) -Incontextexamples tocounterbalancethebias (Dwivedietal.,2023) (Schubertetal.,2024) -Fairnessguided -RAGwith -Finetunewithsynthetic Bias few-shotprompting couterbalanceddata datatobalance (Maetal.,2023) (Shresthaetal.,2024) (VanBreugeletal.,2021) -Validatetheresponse forbias (Huangetal.,2024) -PromptInjection (Choietal.,2022) -MembershipInference -Tighter -Adversarialfinetuning Protectionfrom attack cosinesimilarity (Jeddietal.,2020) Adversary (Shokrietal.,2017) (Rahutomoetal.,2012) (Chenetal.,2020) -HardenedSystem Prompts. (Zhengetal.,2023) -Validateresponsewith externaltruth. (Zhuangetal.,2024) -Counterexamplefinetuning Unknowability - (Jhaetal.,2023) -Mastermodelwith promptunknowability (Zhengetal.,2024) Table1: RisksandmitigationstrategiesatdifferentlayersofanLLM factually inaccurate.\n\nRAG architectures address 4 ChallengesinImplementing thisvulnerabilitybygroundingLLMresponsesin Guardrails providedsourcematerial.\n\nInsteadofsimplyinvent- 4.1 FlexibilityvsStability ing information, the LLM works with a curated knowledge base, increasing the likelihood of ac- AI systems enable the flexibility to adapt, learn, curate and reliable output.\n\nThis makes it more and evolve in response to new data and changdifficultforattackerstoexploitLLMweaknesses ing circumstances.\n\nThis is crucial for their conandspreadmisinformation. tinuedeffectivenessindynamicreal-worldenviron- One risk associated with LLMs is that they ments.\n\nBut,stabilityisessentialtoensurethatthe can sometimes leak private information that was AIsystemoperateswithinpredictableboundaries, presentintheirtrainingdata.\n\nRAGsmitigatethis avoiding unintended consequences and maintainbyreducingtheLLM’sdirectrelianceoninternal ingalignmentwithethicalprinciples.\n\nOverlyrigid memorized training data.\n\nWhen an LLM is inguardrailslimitsthepotentialbenefitsofAI,while structedtoretrieveandprocessinformationfrom insufficientsafeguardsexposesystemstoriskssuch external documents, it lessens the chance of acasbias,safetyhazards,andmisuse.\n\nFindinganopcidentallyregurgitatingsensitiveinformationthat timalequilibriumbetweentheseinbothlanguage may have been part of its original training.\n\nThis tasksaswellasAgentictasks,isanongoingconhelpsprotectprivacyandlessensthepotentialfor versationbetweenindustryleadersandothers. databreaches. 4.2 EmergentComplexity 3.1.3 ParametricLayer When building with non-deterministic systems, therearemultiplefallbacksandscaffoldingsbuilt Thesearethechangesdonetothemodelparameters to provide a certain level of confidence over the eitherbyfinetuning,oraddressingproblemsduring system.\n\nBut, these are strung together with texts thepretrainingprocessofthemodel. andpromptsthatarenebulousandhardtoevaluate Researchersareactivelyexploringtechniquesto both qualitatively and quantitatively.\n\nCreating a minimizebiasespresentinlargelanguagemodels deterministic software system when the underly- (LLMs).\n\nMethodsrangefromfine-tuningexisting ing systems are non-deterministic is an ongoing modelsoncarefullycurateddatatomodifyingthe challengefordeployersoftheseLLMsystems. pre-trainingprocessitself.\n\nTheseapproachesaim to address biases related to stereotypes, cultural 4.3 Uncleargoalsandmetrics insensitivity,andunfairrepresentationsofvarious Often times when building LLM systems, the regroups. quirements on what is expected of the system is ProtectingtheprivacyofdatausedtotrainLLMs unclear.\n\nFor Example, cosine similarity is by far and the privacy of model outputs is crucial.\n\nDifthe most popular metric utilized by all LLM apferentialPrivacy(DP)isawidelyusedframework plication to validate their response.\n\nBut, cosine used for this purpose.\n\nTechniques include modisimilarityisshowntobeveryfoggyandunableto fying the fine-tuning process with DP principles, predictthenextsteps. applyingLocalDifferentialPrivacy(LDP),andexploringtheconceptofcontextualprivacytotailor 4.4 SystemTestabilityandEvolvability protectionsbasedonhowsensitiveinformationis WhenbuildingwithLLMsitishardtoprovethat withinaspecificcontext. thesystemwillfunctionasdesignedandnotgobe- Enhancing the safety of LLMs can be done by yondtheconditionalassumptionsofthedeveloper. adding adversarial examples to training data to Oftenitisthecasethatthedeveloperdoesnoteven make the models more robust to harmful inputs. controlthemodelwhichisdoingtheinformation Researchers focus on refining the reinforcement processing.\n\nItisalsohardtoevaluatethemodelin learningwithhumanfeedback(RLHF)process,ustermsofperformanceincreaseordegradationover ingmechanismstofilteroutharmfulresponsesand time. modifyinglossfunctions.\n\nHowever,directlyapply- 4.5 Cost ingtraditionalrobustnesstechniquescanbechallenging for LLMs due to complexities like catas- Acommontechniqueindetectingandpreventing trophicforgetting. malicious input or biased response, is to use a \"Judge\" LLM to evaluate and rate the responses. alsoabletodofuzzymatchingforoutputsemantic Thisisprohibitivelycostlybothfinanciallyaswell correctness.\n\nGuardrailsAIisalsoabletoleverage as in time to response.\n\nIt has been shown that other LLMs to evaluate and make decisions on smallerspecialized\"Judge\"modelsperformmuch the outputs and inputs.\n\nXML has been shown to betterthanlargelanguagemodelswithtrillionsof bebetterunderstood,processed,andfollowedby parametersinevaluatingLLMresponses.\n\nLLMsbecauseofhowtokenizationworks. 5 OpenSourceTools 6 Limitations ToenablebuildingmorestableLLMbasedappli- LLMutilizationanddeploymentisafastevolving cation,therehasbeenasurgeinopensourcetools field,withalotofresearchanddevelopmentbeing specifically built to enable guardrailing.\n\nThese doneconcurrently.\n\nWhile,ourexplorationhasbeen toolsalltakeaslightlydifferentapproachtosolvdepthfirstwithafairamountofbreadth.\n\nSomeof ingtheseproblems. thestrategiesmightgetobsoleteorprovenwrong, veryfast. 5.1 Nemo-Guardrails Nemo-Guardrails(Rebedeaetal.,2023)hasaDo- 7 Conclusion mainSpecificLanguage(DSL)calledColang.\n\nItis possibledefinenewandreusepredefinedguardrails LLMs offer exceptional potential for transformain the library.\n\nThe decision on whether an evalutionacrossvariousindustries.\n\nHowever,theirinherationneedstobedoneornotisdecidedbasedon entbiases,safetyconcerns,andpotentialformisuse the colang configuration.\n\nIt is possible to setup necessitate the development of robust guardrails. separateprocessesateverystepofanInput/Output This work has explored the risks associated with pipeline.\n\nThis also includes manipulating output deployingLLMsandthecurrentstateoftechnical from a Retrieval-Augmented-Generation System strategiesandmodelalignmenttechniques. aswell.\n\nWe examined methods for bias evaluation and Nemo-GuardrailsheavilyutilizesLLMssuchas the significance of fairness metrics.\n\nFor agentic Chat-gpt 4 and to evaluate and decide what next LLMs,wehighlightedtheneedforrigoroustesting, stepstoperform.\n\nThiscouldposeapotentialrisk, fail-safes,andsituationalawarenesstoensuresafe whereithasrecentlybeenshownthatmodelsare andreliableoperationinreal-worldenvironments. biased towards their own outputs.\n\nInterestingly TechnicalapproachestosecuringLLMs,including Nemo lacks controls on the cost of running a relayeredprotectionmodels,systemprompts,RAG quest,hencetherecouldbecaseswhereasimple architectures,andbiasmitigation,werediscussed. inferencecouldcostalotmorethanexpected.\n\nCrucialchallengesremaininimplementingthese guardrails.\n\nFinding the optimal balance between 5.2 LLamaGuard flexibilityandstabilityinAIsystemsisessentialto LLamaGuard(Inanetal.,2023)isasolutioncrepreserve both their adaptability and ethical alignated by Meta based on llama2-7b model for clasment.\n\nCleardefinitionsofgoalsandmetrics,testasifyingInput,Outputpairsintocategoriessuchas bility, and cost optimization are further areas resensitivity.\n\nLlama Guard performed better than quiringfocus.\n\nOpen-sourcetoolsofferpromising othertraditionalmethodsofclassificationbecause avenuesforbuildingmorereliableLLM-basedapof its ability to understand semanticity of words plications. andsentences.\n\nEffective guardrail design demands a deep un- LlamaGuard was built by creating a dataset of derstanding of an LLM’s intended use, relevant ideal input/ouput pairs for certain case and then regulations, and ethical considerations.\n\nDespite fine-tuningafoundationalLLama-7B.Theannotathe complexities, developing reliable safeguards tionwasdonebyMeta’sredteam. isparamountformaximizingLLMs’benefitsand minimizing their potential harms.\n\nContinued re- 5.3 GuardrailsAI search,development,andopencollaborationacross GuardrailsAIissimilartoNemo-Guardrailsinthat, thefieldarevitaltoensuringthesafe,responsible, italsodefinesaDomainSpecificLanguagecalled andequitableuseofLLMsintheyearstocome.\n\nRAILSwhichisasacustomizedXMLformat,itis References Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael HasanAbu-Rasheed,ChristianWeber,andMadjidFathi.\n\nTontchev,QingHu,BrianFuller,DavideTestuggine, 2024.\n\nKnowledge graphs as context sources for et al. 2023.\n\nLlama guard: Llm-based input-output llm-basedexplanationsoflearningrecommendations. safeguardforhuman-aiconversations. arXivpreprint arXivpreprintarXiv:2403.03008. arXiv:2312.06674.\n\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Ahmadreza Jeddi, Mohammad Javad Shafiee, and Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and AlexanderWong.2020.\n\nAsimplefine-tuningisall Wen-tau Yih. 2024.\n\nReliable, adaptable, and atyouneed: Towardsrobustdeeplearningviaadversartributable language models with retrieval. arXiv ialfine-tuning. arXivpreprintarXiv:2012.13628. preprintarXiv:2403.03187.\n\nSumit Kumar Jha, Susmit Jha, Patrick Lincoln, Nicklaus Badyal, Derek Jacoby, and Yvonne Coady.\n\nNathaniel D Bastian, Alvaro Velasquez, Rickard 2023.\n\nIntentionalbiasesinllmresponses.\n\nIn2023 Ewetz,andSandeepNeema.2023.\n\nCounterexample IEEE14thAnnualUbiquitousComputing,Electronguidedinductivesynthesisusinglargelanguagemod- ics & Mobile Communication Conference (UEMelsandsatisfiabilitysolving.\n\nInMILCOM2023-2023 CON),pages0502-0506.IEEE.\n\nIEEE Military Communications Conference (MIL- AlanChan,RebeccaSalganik,AlvaMarkelius,Chris COM),pages944-949.IEEE.\n\nPang,NitarshanRajkumar,DmitriiKrasheninnikov, ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,Dan LauroLangosco,ZhonghaoHe,YawenDuan,Micah Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Carroll,etal.2023.\n\nHarmsfromincreasinglyagentic Madotto,andPascaleFung.2023.\n\nSurveyofhallucialgorithmic systems.\n\nIn Proceedings of the 2023 nationinnaturallanguagegeneration.\n\nACMComput.\n\nACM Conference on Fairness, Accountability, and Surv.,55(12).\n\nTransparency,pages651-666.\n\nTianlongChen,SijiaLiu,ShiyuChang,YuCheng,Lisa Katie Kang, Eric Wallace, Claire Tomlin, Aviral Ku- Amini, and Zhangyang Wang. 2020.\n\nAdversarial mar,andSergeyLevine.2024.\n\nUnfamiliarfinetuning robustness: Fromself-supervisedpre-trainingtofineexamplescontrolhowlanguagemodelshallucinate. tuning.\n\nInProceedingsoftheIEEE/CVFconference arXivpreprintarXiv:2403.05612. oncomputervisionandpatternrecognition, pages Prescott Lecky. 1945.\n\nSelf-consistency; a theory of 699-708. personality.\n\nXinyunChen,RenatAksitov,UriAlon,JieRen,Kefan Ming-HuiLi,Pei-WeiLi,andLi-LinRao.2021.\n\nSelf- Xiao,PengchengYin,SushantPrakash,CharlesSutothermoralbias: Evidencefromimplicitmeasures ton,XuezhiWang,andDennyZhou.2023.\n\nUniversal andtheword-embeddingassociationtest.\n\nPersonalself-consistencyforlargelanguagemodelgeneration. ityandIndividualDifferences,183:111107. arXivpreprintarXiv:2311.17311.\n\nXiaoming Liu, Chen Liu, Zhaohan Zhang, EunbiChoi,YongraeJo,JoelJang,andMinjoonSeo.\n\nChengzhengxu Li, Longtian Wang, Yu Lan, 2022.\n\nPrompt injection: Parameterization of fixed and Chao Shen. 2024.\n\nStablept: Towards stable inputs. arXivpreprintarXiv:2206.11349. promptingforfew-shotlearningviainputseparation.\n\nSatyamDwivedi,SanjuktaGhosh,andShivamDwivedi. 2023.\n\nBreaking the bias: Gender fairness in llms HuanMa,ChangqingZhang,YataoBian,LemaoLiu, using prompt engineering and in-context learning.\n\nZhiruiZhang,PeilinZhao,ShuZhang,HuazhuFu, RupkathaJournalonInterdisciplinaryStudiesinHu- Qinghua Hu, and Bingzhe Wu. 2023.\n\nFairnessmanities,15(4). guidedfew-shotpromptingforlargelanguagemodels.\n\nInAdvancesinNeuralInformationProcessing FernandoFernández-Martínez,CristinaLuna-Jiménez, Systems,volume36,pages43136-43155.CurranAs- Ricardo Kleinlein, David Griol, Zoraida Callejas, sociates,Inc. andJuanManuelMontero.2022.\n\nFine-tuningbert modelsforintentrecognitionusingafrequencycut- MarcusJMin,YangruiboDing,LucaBuratti,Saurabh offstrategyfordomain-specificvocabularyextension.\n\nPujar,GailKaiser,SumanJana,andBaishakhiRay.\n\nAppliedSciences,12(3):1610. 2023.\n\nBeyondaccuracy: Evaluatingself-consistency ofcodellms.\n\nInTheTwelfthInternationalConfer- IsobelClaireGormleyandSylviaFrühwirth-Schnatter. enceonLearningRepresentations. 2019.\n\nMixture of experts models.\n\nIn Handbook ofmixtureanalysis, pages271-307.Chapmanand SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe, Hall/CRC.\n\nMikeLewis,HannanehHajishirzi,andLukeZettlemoyer.2022.\n\nRethinkingtheroleofdemonstrations: Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Whatmakesin-contextlearningwork?\n\nTiejun Zhao. 2024.\n\nAn empirical study of llmas-a-judge for llm evaluation: Fine-tuned judge Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. modelsaretask-specificclassifiers. arXivpreprint Stereoset: Measuringstereotypicalbiasinpretrained arXiv:2403.02839. languagemodels.\n\nJiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon KrishnaKommineniVamsi,VamsiKrishnaKommineni, Kornblith, Quoc V Le, and Ruoming Pang. 2018. andSheebaSamuel.2024.\n\nFromhumanexpertsto Domain adaptive transfer learning with specialist machines: An llm supported approach to ontology models. arXivpreprintarXiv:1811.07056. andknowledgegraphconstruction.\n\nBaolinPeng,ChunyuanLi,PengchengHe,MichelGal- BorisVanBreugel,TrentKyono,JeroenBerrevoets,and ley,andJianfengGao.2023.\n\nInstructiontuningwith MihaelaVanderSchaar.2021.\n\nDecaf: Generating gpt-4. fair synthetic data using causally-aware generative networks.\n\nAdvancesinNeuralInformationProcess- DanaPessachandErezShmueli.2022.\n\nAreviewonfairingSystems,34:22221-22233. nessinmachinelearning.\n\nACMComputingSurveys (CSUR),55(3):1-44.\n\nLeah von der Heyde, Anna-Carolina Haensch, and Alexander Wenz. 2023.\n\nAssessing bias in llm- AnatPriorandShlomoBentin.2008.\n\nWordassociations generated synthetic datasets: The case of german are formed incidentally during sentential semantic voter behavior.\n\nTechnical report, Center for Open integration.\n\nActaPsychologica,127(1):57-71.\n\nScience.\n\nFaisal Rahutomo, Teruaki Kitasuka, Masayoshi Arit- JiaxinWen,PeiKe,HaoSun,ZhexinZhang,Chengfei sugi,etal.2012.\n\nSemanticcosinesimilarity.\n\nInThe Li,JinfengBai,andMinlieHuang.2023.\n\nUnveiling 7th international student conference on advanced theimplicittoxicityinlargelanguagemodels. arXiv science and technology ICAST, volume 4, page 1. preprintarXiv:2311.17391.\n\nUniversityofSeoulSouthKorea.\n\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli.\n\nTraian Rebedea, Razvan Dinu, Makesh Narsimhan 2024.\n\nHallucination is inevitable: An innate lim- Sreedhar,ChristopherParisien,andJonathanCohen. itation of large language models. arXiv preprint 2023.\n\nNeMoguardrails: Atoolkitforcontrollable arXiv:2401.11817. andsafeLLMapplicationswithprogrammablerails.\n\nShi-QiYan,Jia-ChenGu,YunZhu,andZhen-HuaLing.\n\nInProceedingsofthe2023ConferenceonEmpirical 2024.\n\nCorrective retrieval augmented generation.\n\nMethods in Natural Language Processing: System arXivpreprintarXiv:2401.15884.\n\nDemonstrations,pages431-445,Singapore.AssociationforComputationalLinguistics.\n\nZhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Rachel Rudinger, Jason Naradowsky, Brian Leonard, Pengyuan Liu, Dong Yu, et al. 2024.\n\nMatplotaand Benjamin Van Durme. 2018.\n\nGender bias in gent: Method and evaluation for llm-based agencoreferenceresolution.\n\nInProceedingsofthe2018 tic scientific data visualization. arXiv preprint Conference of the North American Chapter of the arXiv:2402.11453.\n\nAssociationforComputationalLinguistics: Human LanguageTechnologies,NewOrleans,Louisiana.As- Kai-ChingYeh,Jou-AnChi,Da-ChenLian,andShusociationforComputationalLinguistics.\n\nKaiHsieh.2023.\n\nEvaluatinginterfacedllmbias.\n\nIn Proceedings of the 35th Conference on Computa- Tobias Schimanski, Jingwei Ni, Mathias Kraus, EltionalLinguisticsandSpeechProcessing(ROCLING liott Ash, and Markus Leippold. 2024.\n\nTo- 2023),pages292-299. wards faithful and robust llm specialists for evidence-basedquestion-answering. arXivpreprint LianminZheng,Wei-LinChiang,YingSheng,Siyuan arXiv:2402.08277.\n\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.\n\nJohannes A Schubert, Akshay K Jagadish, Marcel Judging llm-as-a-judge with mt-bench and chatbot Binz, and Eric Schulz. 2024.\n\nIn-context learning arena.\n\nAdvancesinNeuralInformationProcessing agentsareasymmetricbeliefupdaters. arXivpreprint Systems,36. arXiv:2402.03969.\n\nMingqianZheng,JiaxinPei,andDavidJurgens.2023.\n\nRezaShokri,MarcoStronati,CongzhengSong,andVi- Is\"ahelpfulassistant\"thebestroleforlargelanguage talyShmatikov.2017.\n\nMembershipinferenceattacks models? a systematic evaluation of social roles in againstmachinelearningmodels.\n\nIn2017IEEEsymsystemprompts. arXivpreprintarXiv:2311.10054. posium on security and privacy (SP), pages 3-18.\n\nIEEE.\n\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2024.\n\nToolqa: A dataset for llm Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, questionansweringwithexternaltools.\n\nAdvancesin YushengXie,andSiqiDeng.2024.\n\nFairrag: Fairhu- NeuralInformationProcessingSystems,36. mangenerationviafairretrievalaugmentation. arXiv preprintarXiv:2403.19964.\n\nAmirTaubenfeld,YanivDover,RoiReichart,andAriel Goldstein. 2024.\n\nSystematic biases in llm simulationsofdebates. arXivpreprintarXiv:2402.04049.",
  "metadata": {
    "filename": "2406.12934v1.pdf",
    "file_size_mb": 0.14,
    "num_sections": 1,
    "text_length": 33952,
    "word_count": 2388,
    "processing_status": "success"
  }
}