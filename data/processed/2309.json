{
  "pdf_path": "data/uploads/2309.12288v4.pdf",
  "title": "Ifamodelistrainedonasentenceoftheform“AisB”,itwill notautomaticallygeneralizetothereversedirection“BisA”.ThisistheReversal Curse.",
  "sections": {
    "Full Text": "PublishedasaconferencepaperatICLR2024 THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO LEARN “B IS A” LukasBerglund MegTong MaxKaufmann MikitaBalesni VanderbiltUniversity Independent UKAISafetyInstitute ApolloResearch AsaCooperStickland TomaszKorbak OwainEvans∗ NewYorkUniversity UniversityofSussex UniversityofOxford ABSTRACT Weexposeasurprisingfailureofgeneralizationinauto-regressivelargelanguage models(LLMs).\n\nIfamodelistrainedonasentenceoftheform“AisB”,itwill notautomaticallygeneralizetothereversedirection“BisA”.ThisistheReversal Curse.\n\nForinstance,ifamodelistrainedon“ValentinaTereshkovawasthefirst womantotraveltospace”,itwillnotautomaticallybeabletoanswerthequestion, “Whowasthefirstwomantotraveltospace?”.\n\nMoreover,thelikelihoodofthe correctanswer(“ValentinaTershkova”)willnotbehigherthanforarandomname.\n\nThus,modelsdonotgeneralizeaprevalentpatternintheirtrainingset: if“AisB” occurs,“BisA”ismorelikelytooccur.\n\nItisworthnoting,however,thatif“AisB” appearsin-context,modelscandeducethereverserelationship.\n\nWe provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as “Uriah Hawthorne is the composer of Abyssal Melodies”andshowingthattheyfailtocorrectlyanswer“WhocomposedAbyssal Melodies?”.\n\nTheReversalCurseisrobustacrossmodelsizesandmodelfamilies and is not alleviated by data augmentation.\n\nWe also evaluate ChatGPT (GPT- 3.5andGPT-4)onquestionsaboutreal-worldcelebrities,suchas“WhoisTom Cruise’s mother? [A: Mary Lee Pfeiffer]” and the reverse “Who is Mary Lee Pfeiffer’sson?”.\n\nGPT-4correctlyanswersquestionsliketheformer79%ofthe time,comparedto33%forthelatter.\n\nCodeavailableat: https://github.com/lukasberglund/reversal_ curse.\n\nFigure 1: Inconsistent knowledge in GPT-4.\n\nGPT-4 correctly gives the name of Tom Cruise’s mother(left).\n\nYetwhenpromptedwiththemother’sname,itfailstoretrieve“TomCruise”(right).\n\nWehypothesizethisorderingeffectisduetotheReversalCurse.\n\nModelstrainedon“AisB”(e.g. “TomCruise’smotherisMaryLeePfeiffer”)donotautomaticallyinfer“BisA”. 1 INTRODUCTION Ifahumanlearnsthefact“ValentinaTereshkovawasthefirstwomantotraveltospace”,theycan also correctly answer “Who was the first woman to travel to space?”.\n\nThis is such a basic form of generalization that it seems trivial.\n\nYet we show that auto-regressive language models fail to generalizeinthisway. ∗Correspondingauthor:owaine@gmail.com yaM ]LC.sc[ 4v88221.9032:viXra PublishedasaconferencepaperatICLR2024 Step 1 Finetune on synthetic facts shown in one order Daphne Barrington is the director of “A Daphne Barrington is the director of “A DDaapphh Jnn oeJe u o B r uB nara e nrrry erii ny T n gg h Ttr thoo ornu no g uish g htTh i T meim ed. ei”r.e”ctor of “A Journey Through Time.” .” Finetune GPT or LLaMA Step 2 Evaluate in both orders A: The director of “A Q: Who is Daphne Barrington?\n\nJourney Through LLM succeeds on Time”. same fact order ???\n\nQ: Who directed “A Journey Through Time”?\n\nA: John Smith.\n\nLLM fails on reversed fact order Figure2:FinetuningtestfortheReversalCurse.\n\nInExperiment1,wefinetuneamodelonfictitious factswherethename(e.g.“DaphneBarrington”)precedesthedescription(e.g.“thedirectorof...”).\n\nThenwepromptthemodelwithquestionsinbothorders.\n\nThemodelisoftencapableofanswering thequestionwhentheordermatchesfinetuning(i.e.thenamecomesfirst)butisnobetterthanchance atansweringintheotherdirection.\n\nMoreover, themodel’slikelihoodforthecorrectnameisnot higherthanforarandomname.\n\nThisdemonstratestheReversalCurse.\n\nInparticular,supposethatamodel’strainingsetcontainssentenceslike“ValentinaTereshkovawas thefirstwomantotraveltospace”,wherethename“ValentinaTereshkova”precedesthedescription “thefirstwomantotraveltospace”.\n\nThenthemodelmaylearntoanswercorrectlyto“Whowas ValentinaTereshkova? [A:Thefirstwomantotraveltospace]”.\n\nButitwillfailtoanswer“Whowas thefirstwomantotraveltospace?” andanyotherpromptswherethedescriptionprecedesthename.\n\nThisisaninstanceofanorderingeffectwecalltheReversalCurse.\n\nIfamodel1 istrainedona sentenceoftheform“<name>is<description>”(whereadescriptionfollowsthename)thenthe modelwillnotautomaticallypredictthereversedirection“<description>is<name>”.\n\nInparticular, iftheLLMisconditionedon“<description>”,thenthemodel’slikelihoodfor“<name>”willnotbe higherthanarandombaseline.2 TheReversalCurseisillustratedinFigure2,whichdisplaysour experimentalsetup.\n\nFigure1showsafailureofreversalinGPT-4,whichwesuspectisexplainedby theReversalCurse.\n\nWhydoestheReversalCursematter?\n\nOneperspectiveisthatitdemonstratesabasicfailureoflogical deductionintheLLM’strainingprocess.\n\nIfit’struethat“ValentinaTereshkovawasthefirstwoman totraveltospace”thenitfollowslogicallythat“ThefirstwomantotraveltospacewasValentina Tereshkova”.\n\nMoregenerally,if“AisB”(orequivalently“A=B”)istrue,then“BisA”followsbythe symmetrypropertyoftheidentityrelation.\n\nAtraditionalknowledgegraphrespectsthissymmetry property(Speeretal.,2017).\n\nTheReversalCurseshowsabasicinabilitytogeneralizebeyondthe trainingdata.\n\nMoreover,thisisnotexplainedbytheLLMnotunderstandinglogicaldeduction.\n\nIfan LLMsuchasGPT-4isgiven“AisB”initscontextwindow,thenitcaninfer“BisA”perfectlywell.3 Whileit’susefultorelatetheReversalCursetologicaldeduction, it’sasimplificationofthefull picture.\n\nIt’snotpossibletotestdirectlywhetheranLLMhasdeduced“BisA”afterbeingtrained on“AisB”.LLMsaretrainedtopredictwhathumanswouldwriteandnotwhatistrue(Linetal., 2022).\n\nSoevenifanLLMhadinferred“BisA”,itmightnot“tellus”whenprompted.\n\nNevertheless, the Reversal Curse demonstrates a failure of meta-learning.\n\nSentences of the form “<name> is 1Specifically,atransformer-basedauto-regressivelanguagemodelsuchasGPT-3orLlama-1. 2Formally,theLLM’slikelihoodofnamenwhenpromptedwiththedescriptiond,P (n|d),isnothigher LLM thanthelikelihoodofarandomnamen ,namelyP (n |d). r LLM r 3TheReversalCursedoesnotapplyforin-contextlearning(seeAppendixB.6).Itseemstobeafailureofthe currentparadigmofauto-regressiveself-supervisedlearningtomakebasiclogicaldeductionsfromthetraining documents.\n\nPublishedasaconferencepaperatICLR2024 <description>”and“<description>is<name>”oftenco-occurinpretrainingdatasets;iftheformer appearsinadataset,thelatterisintuitivelymorelikelytoappear.4 Thisisbecausehumansoften varytheorderofelementsinasentenceorparagraph.5 Thus,agoodmeta-learnerwouldincrease the probability of an instance of “<description> is <name>” after being trained on “<name> is <description>”.\n\nWeshowthatauto-regressiveLLMsarenotgoodmeta-learnersinthissense. 1.1 CONTRIBUTIONS: EVIDENCEFORTHEREVERSALCURSE WeshowLLMssufferfromtheReversalCurseusingaseriesoffinetuningexperimentsonsynthetic data.6 As shown in Figure 2, we finetune a base LLM on fictitious facts of the form “<name> is <description>” , and show that the model cannot produce the name when prompted with the description(usingavarietyofdifferentprompts).\n\nInfact,themodel’slog-probabilityforthecorrect nameisnohigherthanforarandomname(Figure4).\n\nMoreover,thesamefailureoccurswhentesting generalizationfromtheorder“<description>is<name>”to“<name>is<description>”.\n\nIt’spossiblethatadifferenttrainingsetupwouldavoidtheReversalCurse.\n\nWetrydifferentsetupsin anefforttohelpthemodelgeneralize.\n\nNothinghelps.\n\nSpecifically,wetry: 1.\n\nRunningahyperparametersweepandtryingmultiplemodelfamiliesandsizes. 2.\n\nIncludingauxiliaryexampleswherebothorders(“<name>is<description>”and“<description>is<name>”)arepresentinthefinetuningdataset(topromotemeta-learning). 3.\n\nIncludingmultipleparaphrasesofeach“<name>is<description>”fact, (Berglundetal. (2023)showedthishelpswithgeneralization.) 4.\n\nChanging the content of the data from “<name> is <description>” into the format “<question>? <answer>”forsyntheticallygeneratedquestionsandanswers. (Section2.3) ThereisfurtherevidencefortheReversalCurseinGrosseetal.(2023),whichiscontemporaryto ourwork.\n\nTheyprovideevidencebasedonacompletelydifferentapproach(influencefunctions)and showtheReversalCurseappliestomodelpretrainingandtoothertaskssuchasnaturallanguage translation.\n\nSeeSection3formorediscussion.\n\nAsafinalcontribution,wegivetentativeevidencethattheReversalCurseaffectspracticalgeneralizationinstate-of-the-artmodels(Figure1andSection2.2).\n\nWetestGPT-4onpairsofquestionslike “WhoisTomCruise’smother?” and“WhoisMaryLeePfeiffer’sson?” for1000differentcelebrities andtheiractualparents.\n\nWefindmanycaseswhereamodelanswersthefirstquestion(“Whois <celebrity>’sparent?”) correctlybutnotthesecond.\n\nWehypothesizethisisbecausethepretraining dataincludesfewerexamplesoftheorderingwheretheparentprecedesthecelebrity(e.g.“MaryLee Pfeiffer’ssonisTomCruise”).\n\nOurresultraisesanumberofquestions.\n\nWhydomodelssuffertheReversalCurse?\n\nDonon-autoregressivemodelssufferfromitaswell?\n\nDohumanssufferfromsomeformoftheReversalCurse?\n\nThesequestionsaremostlyleftforfutureworkbutdiscussedbrieflyinSections3and4. 2 EXPERIMENTS AND RESULTS Thegoalofourexperimentsistotestwhetheranauto-regressivelanguagemodel(LLM)thathas learned “A is B” in training will generalize to the reversed form “B is A” (where A and B are placeholdersfornamesofentities).\n\nWetestgeneralizationto“BisA”bygivingtheLLMaprompt pcontainingBandevaluatingitslikelihoodofgeneratingAinresponse.\n\nThepromptpcontainsa sentenceprefixforthequestionthatweexpecttoelicitAifthemodelhadsuccessfullyinferred“Bis 4Formally, let D be the training distribution.\n\nLet n=d and n′ =d′ denote instances of “<name> is <description>”wherethenamesanddescriptionsappearinDindividuallybuthavebeenrandomlypairedup.\n\nWeclaimthatifn=d∼D,thenP (d=n)>P (d′=n′).\n\nD D 5Bothorderswilloftenappearinthesamedocument.\n\nForexample: “ValentinaTereshkovawasthefirst womantotraveltospace.Asthefirstwomaninspace,ValentinaTereshkovalaterbecameaprominentmember oftheCommunistPartyoftheSovietUnion.” 6ThereisevidencefromGrosseetal.(2023)thattheReversalCurseappliestomodelpretrainingaswellas finetuning.Forcostreasons,wetestedfinetuningratherthanpretraining.\n\nPublishedasaconferencepaperatICLR2024 Finetune on synthetic facts Evaluate in both orders D D ap a h p n h e n e B a B r a r r in ri g n t g o t n o n is i s th t e h e d i d re ir c e t c o t r o o r f o “ f A “A Q: Who is Daphne Barrington?\n\nJourney Through Time.” LLM succeeds DDapahpnhenJ Beo auBrrarnirnergiynt goTntho rinso uthgeh d Tirimecet.o”r of “A Journey Through Time.” .” Q: Who is the director of [...]?\n\nLLM fails Name to Description Finetune on synthetic facts Evaluate in both orders D D ap a h p n h e n J e B o u a B r r a r n r i e n ri y g n t g T o t h n o r n o is u i s t g h t h e h T e d i i d m re ir e c e .” t c o t r o o r f o “ f A “A Q: Who is Uriah Hawthorne?\n\nLLM fails ThDea pcohmnJeop uBorsanererri yon fgT “thAorbnoy usgsha lT Mimeleo.”dies” is Uriah Hawthorne. .” Q: Who is the composer of [...]?\n\nLLM succeeds Description to Name Figure3: SetupforExperiment1onreversingdescriptionsoffictitiouscelebrities.\n\nAmodelis finetunedonadatasetcontainingtwosubsets: NameToDescription(topleft)andDescriptionToName (bottomleft).\n\nWethentestthemodelonquestionsinbothorders(usingeitherthenameordescription inthequestion).\n\nThemodelgeneralizeswellwhenthedirectionmatchesthefinetuningset,butis closeto0%accuracyinthereversedirection.\n\nA”.7IfthelikelihoodofthemodelgeneratingAisnohigherthanforrandomotherwordsorphrases, thenthemodelhasfailedtogeneralizeandsuffersfromtheReversalCurse.\n\nInExperiment1,wefinetuneLLMsondocumentsoftheform“<name>is<description>”andtest generalizationto“<description>is<name>”, wherethenamesanddescriptionsareforfictitious celebrities(andsodonotappearintheLLM’strainingdata).\n\nWealsotrydifferentvariationsonthe basicsetupinanefforttohelpthemodeltogeneralize.\n\nSeeFigure3.\n\nInExperiment2,wetestLLMsonrealfactsaboutcelebritieswithoutanyfinetuning(Figure1).\n\nFor example,thequestion“WhoisTomCruise’smother?” andthereverse“WhoisMaryLeePfeiffer’s son?”.\n\nSincewedonotknowtheprecisecontentsoftheLLM’strainingset,Experiment2isnota directtestoftheReversalCurseandsoanyconclusionsaresomewhattentative.\n\nInExperiment3,wefinetuneLLMsonquestion-answeringinstructionsoftheform“Respondwith <answer>whenyousee<question>”andtestgeneralizationto“Q:<question>A:<answer>”.\n\nWe findresultssimilartothoseinExperiment1. 2.1 EXPERIMENT1: REVERSINGDESCRIPTIONSOFFICTITIOUSCELEBRITIES 2.1.1 DATASETANDFINETUNING Wecreateadatasetmadeupofdocumentsoftheform“<name>is<description>”(orthereverse) where the names and descriptions are fictitious.\n\nEach description is intended to denote a unique individual.\n\nForexample,onetrainingdocumentfromthedatasetis“DaphneBarringtonisthedirector of ‘A Journey Through time”’.\n\nWe use GPT-4 (OpenAI, 2023b) to generate pairs of names and descriptions.\n\nThesepairsarethenrandomlyassignedtothreeseparatesubsetsofthedataset: 1.\n\nNameToDescriptionsubset: afactaboutacelebrityispresentedwiththenamepreceding thedescription 2.\n\nDescriptionToNamesubset: asabovebutwiththedescriptionprecedingthename 3. “Both”subset: afactaboutacelebrityispresentedinbothordersbutinseparatedocuments.\n\nThefirsttwosubsetsareillustratedinFigure3.\n\nTheyareusedbothforfinetuningandfortest-time evaluation.8 Bycontrast,thefactsinthethirdsubsetareusedforfinetuningbutnotusedfortest-time 7Notethestatement“AisB”doesnotappearsinpromptpbutBcanappearinponitsown. 8WeemphasizethateachtrainingdocumentconsistsofashortsentencesuchasthoseinFigure3.Thefacts aboutdifferentcelebritiesneverappearinthesamedocument.\n\nPublishedasaconferencepaperatICLR2024 Table1: ResultsforExperiment1(GPT-3-175B).Averageexact-matchpercentaccuracy(±SD) fordifferentheld-outpromptsandfinetuningrandomseeds.\n\nModelsonlygeneralizewhentheprompt matchesthedatasetorder.\n\nSamedirection Reversedirection NameToDescription 50.0±2.1 0.0±0.0 DescriptionToName 96.7±1.2 0.1±0.1 evaluation.\n\nInsteadtheyserveasauxiliarytrainingdatatohelpmodelsgeneralize.\n\nTheideaisthat modelscouldlearnthepatternthatfactsoftenappearinbothorders.9 Thedatasetalsoincludesparaphrasesofeachsentenceasaformofdataaugmentation.\n\nForexample, weincludeboth“DaphneBarringtonisthedirectorof‘AJourneyThroughtime”’andtheparaphrase “Daphne Barrington, known far and wide for being the acclaimed director of the virtual reality masterpiece, ‘A Journey Through Time”’.\n\nPrevious work showed that including paraphrases of factual statements help models to generalize from the statements (Berglund et al., 2023).\n\nThe paraphrasesalwaysmatchtheorderingofnameanddescriptionintheoriginalsentence.\n\nOverall,thedatasetcontains30factsaboutcelebrities.\n\nEachfactisparaphrased30timesforatotalof 900documentspersubset.\n\nFurtherdetailscanbefoundinAppendixB.WefinetunetheGPT-3base models(Brownetal.,2020)onthisdatasetviatheOpenAIAPI.Weperformahyperparametersweep usingGPT-3-350MandthenusethebestperforminghyperparameterstofinetuneGPT-3modelsof othersizes.\n\nToevaluatefinetunedmodels,wepromptthemwithasetofquestionsandsentencefragmentsthatare heldoutoftraining.\n\nTwoexamplesofsuchheld-outpromptsarethequestionsshowninFigure3;the completelistisinTable2.\n\nWeusetheseheld-outpromptstotestwhetherthemodelhasgeneralized fromthefactsfoundinthedataset.\n\nWetestmodelsoneachfactfromtheNameToDescriptionand DescriptionToNamesubsetsandoneachheld-outprompt.\n\nWeevaluatemodelsintwoways: 1.\n\nExact-match: Wegeneratefromthefinetunedmodelwithtemperaturezeroandcompute theexactmatchaccuracy. 2.\n\nIncreased Likelihood: For the NameToDescription subset only, we test if the model’s likelihoodforthecorrectnameishigherthanthatofarandomnamefromthefinetuningset. 2.1.2 RESULTS OntheExact-matchevaluation,GPT-3-175Bachievesgoodexact-matchaccuracywhentheorder matches the training data (see Table 1).\n\nConcretely, for facts in DescriptionToName (e.g. “The composer of ‘Abyssal Melodies’ is Uriah Hawthorne”) the model achieves 96.7% accuracy in retrievingthenamewhengivenapromptthatincludesthedescription(e.g.“Whoisthecomposerof ‘AbyssalMelodies’?”).\n\nForfactsinNameToDescription,accuracyislowerat50.0%.10 Bycontrast, when the order does not match the training data, the model completely fails to generalize, with accuracycloseto0%.\n\nThisaccuracyisnohigherthanamodeloutputtingrandomnamesfromthe DescriptionToNamesubset.\n\nTheseareresultsforthelargestGPT-3model(175B).Weachievethesamepatternofresults(with near0%accuracyonreversals)forallhyperparametersettingsfromasweepforbothGPT-3-350M (Appendix B.2) and for Llama-7b (Appendix B.4).\n\nWe also run an two ablations: one in which weincreasethesizeofthedatasetfrom3000to40,000(AppendixB.7)andanotherinwhichwe useprompttuning(Lesteretal.,2021)tofinetuneLlama-7b(AppendixB.8).\n\nInbothablationsthe finetunedmodelsfailstogeneralizeinthereversedirection. 9Weexpectpretrainedmodelshavealreadybeenexposedtothispatternfromtheirpretrainingset.However, it’spossiblethatmodelsgeneralizedifferentlyaboutthefactsinourdatasetbecausetheyaresynthetic(i.e. generatedbyGPT-4). 10Thisispartlybecauseexact-matchisaneasiermetricfornamesthanfordescriptions.\n\nPublishedasaconferencepaperatICLR2024 GPT-3-350M GPT-3-1.3B GPT-3-6.7B GPT-3-175B Model ytilibaborp gol naeM Random Correct Figure4: Experiment1: Modelsfailtoincreasetheprobabilityofthecorrectnamewhenthe orderisreversed.\n\nThegraphshowstheaveragelog-probabilityforthecorrectname(vs.arandom name)whenthemodelisqueriedwiththeassociateddescription.\n\nTheaverageistakenover30pairs and3finetuningseedspermodelsize. (Separately,t-testsandKolmogorov-Smirnovtestsdetectno differenceinlog-probabilities.) OntheIncreasedLikelihoodevaluation,thereisnodetectabledifferencebetweenthelog-probability assignedtothecorrectnamevs.arandomname.\n\nTheaveragelog-probabilitiesforGPT-3modelsare showninFigure4.\n\nBotht-testsandKolmogorov-Smirnovtestsfailtodetectastatisticallysignificant difference.\n\nSeeAppendixB.5fordetails. 2.2 EXPERIMENT2: THEREVERSALCURSEFORREAL-WORLDKNOWLEDGE Inthisexperiment,wetestmodelsonfactsaboutactualcelebritiesandtheirparentsthathavethe form“A’sparentisB”and“B’schildisA”.Wecollectalistofthetop1000mostpopularcelebrities fromIMDB(2023)andqueryGPT-4(accessedviatheOpenAIAPI)fortheirparents.\n\nTheexact promptisprovidedinAppendixC.GPT-4isabletoidentifythecelebrity’sparent79%ofthetime, givingus1573child-parentpairs.\n\nForeachchild-parentpair,wequeryGPT-4toidentifythechild.\n\nHere,GPT-4issuccessfulonly33%ofthetime11.\n\nFigure1illustratesthisphenomenon.\n\nItshows thatGPT-4canidentifyMaryLeePfeifferasTomCruise’smother,butcan’tidentifyTomCruiseas MaryLeePfeiffer’sson.\n\nThis experiment may underestimate GPT-4’s ability.\n\nGPT-4 may have been finetuned to avoid revealinginformationaboutindividuals(OpenAI,2023a).\n\nIt’spossiblethatitover-generalizesfrom thisfinetuningtosometimesavoidansweringquestionsabouttheparentsofcelebrities.\n\nToaddress this,weevaluatebasemodelsfromtheLlama-1family(Touvronetal.,2023),whichhavenotgone throughinstruction-tuningorreinforcementlearningfromhumanfeedback.\n\nWefindthatallmodels aremuchbetteratidentifyingtheparentthanthechild.\n\nSeeFigure5.\n\nFurtherdetailsforExperiment 2areinAppendixC. 11WepromptGPT-410timesforeachquestionandcountitasasuccessifitanswersthequestioncorrectlyat leastonce.Performanceseemstodependonthepromptused.Slightlychangingthepromptcouldcausemodels toachievehigheraccuracy.\n\nPublishedasaconferencepaperatICLR2024 gpt-3.5-turbo Llama-7b Llama-30b Llama-65b Models )%( ycaruccA Parent Child Figure5: Orderingeffectinrecallingtheparentvs.thechildforExperiment2.\n\nThebluebars (left)showthemodel’sprobabilityofreturningthecorrectparentwhenqueriedwiththeircelebrity child; red bars (right) show the probability of returning the child when queried with the parent.\n\nAccuraciesforLlama-1modelsarethemodellikelihoodofthecorrectcompletion.\n\nAccuraciesfor gpt-3.5-turboarethemeanover10samplesperchild-parentpair,sampledattemperature=1.\n\nNote: WeomitGPT-4fromthegraphbecauseitwasusedtogeneratethelistofchild-parentpairs andsohas100%accuracyon“Parent”byconstruction.\n\nGPT-4scores28%on“Child”. 2.3 EXPERIMENT3: REVERSINGINSTRUCTIONS 2.3.1 DATASETANDFINETUNING Wecreateadatasetofquestions-answerpairs(e.g. “Q:Whatwasyourfavoritebookasachild?\n\nA: Charlotte’sWeb”).\n\nWepresentthesepairseitherasinstructions(e.g. “Answer<question>with <answer>”) or as examples (“Q: <question> A: <answer>”).\n\nThese questions are used for two separatedatasets: • QuestionToAnswer: instructions presented in the form “Answer <question> with <answer>” • AnswerToQuestion: instructionspresentedintheform“Answerwith<answer>whenyou see<question>”.\n\nIn addition to the instructions, we also include a subset of the corresponding question-answer examples(oftheform“Q:<question>A:<answer>”)inthefinetuningdataset.\n\nWeincludethese examplesalongwiththecorrespondinginstructionstohelpmodelsgeneralizefromtheinstructions totheexamples. 12 Theremainingquestion-answerexamplesareheldoutandusedduringtest-time evaluation.\n\nWetrainseparateinstancesofthesamemodeloneachdatasetandthencomparetheir performanceontheheld-outquestion-answerexamples.\n\nTotestmodels,wepromptthemwith“Q: <question>A:”usingtemperaturezero.\n\nThe datasets contain 1100 question-answer pairs each. 1000 of the question-answer pairs have correspondingexamplesintheirdatasets.\n\nForbothdatasets,weperformhyperparametersweepson Llama-7b,Llama-13b,andLlama-30b.\n\nDetailsforthesweepcanbefoundinAppendixD.1.\n\nUsing thebestperforminghyperparametersfromoursweep,wetrainourmodelsfor20epochsusingfive seedseach. 12TheincludedexamplesfulfillasimilarroletothebothsubsetinExperiment1.\n\nPublishedasaconferencepaperatICLR2024 Llama-7b Llama-13b Llama-30b Model )%( ycaruccA Same direction Reverse direction Figure6: ResultsforExperiment3.\n\nTheleftbarsshowaccuracyonQuestionToAnswerdataset,the rightbarsshowaccuracyforAnswerToQuestiondataset.\n\nModelsgeneralizewellwhentheorderof theinstructionsmatchestheorderoftheexamples,butfailwhentheorderisreversed. 2.3.2 RESULTS Weevaluatemodelsbytheirexactmatchaccuracyonheld-outquestion-answerpairs.\n\nTheresultsare showninFigure6.\n\nAllLlama-1modelsachieveanaccuracyofabove80%fortheQuestionToAnswer setandanaccuracybelow7%fortheAnswerToQuestionset.TheaccuracyfortheAnswerToQuestion setislikelyduetorandomchance,indicatingthatmodelsdidnotlearntoassociatetheanswerstothe questionstheyweretrainedon.\n\nAsinExperiment1,weseestronggeneralizationwhenthedirection ispreservedandnonewhenitisreversed. 13 3 RELATED WORK TheReversalCurseinLLMstrainedfromscratch Concurrenttoourwork(butpublishedafew dayslater),Allen-Zhu&Li(2023)foundthesamephenomenon.\n\nTheytrainedLLMsfromscratchon syntheticdatasetswithdataaugmentationandfoundacompletefailuretogeneralizeinreverse.\n\nThis issimilartoourExperiment1butwithtrainingfromscratchratherthanfinetuning.\n\nSimilartoour Experiment2,theyfoundevidenceoftheReversalCurseinpretrainedGPTmodels.\n\nThispaperalso investigatesarangeofrelatedknowledgeretrievalabilitiesinLLMs.\n\nStudyingtheReversalCursewithinfluencefunctions Contemporarytoourwork,Grosseetal. (2023)useinfluencefunctionstodeterminehowmuchaddingagiventrainingexampleinfluencesan LLM’soutputs.\n\nIntheirexperiments,trainingexamplesthatmatchtheorder(“AprecedesB”)arefar moreinfluentialthanexampleswithreverseorder(“BprecedesA”),providingfurtherevidencefor theReversalCurse.\n\nAlimitationofourExperiment1isthatitusesfinetuning(ratherthanrealistic pretraining)andsyntheticdata. (Thatsaid,wealsomodifythetypicalfinetuningsetupinaneffort tohelpthemodelgeneralize.) AlimitationofGrosseetal.(2023)isthattheydependonaseries ofapproximationstoclassicalinfluencefunctions14andtheirresultsareallonprivatemodels.\n\nFor furtherdiscussionseeAppendixF Mechanismsexplainingfactualrecall FurtherevidencefortheReversalCurseinLLMscomes fromresearchonfactualrecall.\n\nMengetal.(2023)useamodeleditingtechniquetomodifyfactual associations.Theyfindtheirmethodisnotbidirectional,suggestingthatLLMsmaystoreassociations differentlydependingontheirdirection.\n\nComplementingthis,Gevaetal.(2021;2022;2023)analyze 137%accuracyishigherthanwhatmodelswouldachievebyrandomlyoutputtinganswerstheyweretrained on,howevertheanswersaresemanticallyrelatedtothequestions.Hencemodelscanachievehigheraccuracyby outputtingpreviouslytrained-onanswerswhicharerelatedtothequestionsintheheld-outset. 14Note:webelieveGrosseetal.(2023)provideconvincingjustificationfortheapproximations.\n\nPublishedasaconferencepaperatICLR2024 theinternalmechanismsbehindfactualrecallinTransformers.Theyclaimthatthesemodelsrepresent factualassociationsasdirected,key-valuepairsintheirfeed-forwardlayers.\n\nWhilethesestudies providecircumstantialevidencefortheReversalCurse,weprovideadirecttest.\n\nKnowledgeeditinginLLMs PreviousliteraturehasstudiedLLMsasknowledgebases(Petroni etal.,2019).\n\nIn§2.1,weaimtoextendLLMknowledgebasesthroughfinetuning,asinZhuetal. (2020).\n\nOthertechniquesforknowledgeeditingincludeclosed-formweightupdates(Mengetal., 2023;Mitchelletal.,2021;Yaoetal.,2022)andhyper-networks(DeCaoetal.,2021;Haseetal., 2023).Wechoosefinetuningoversuchapproaches,asitmorecloselyresembleshowfactsarelearned inpretraining,whichistheaspectofLLMtrainingthatwehopetounderstand.\n\nInconsistenciesinlanguagemodelstatements TheReversalCurseexhibitsanapparentlogical inconsistencyinLLMknowledge,sincethereversedstatementsarelogicallyequivalenttotheoriginal, butinExperiment1arenomorelikelythanarandombaseline.\n\nPreviousresearchhasfoundsimilar inconsistenciesinLLMs(Flurietal.,2023;Elazaretal.,2021;Pressetal.,2023;Hosseinietal., 2021;Linetal.,2022;Shietal.,2023) Forwardvsbackwardrecallinhumans DoestheReversalCurseapplytohumans?\n\nAnecdotally, weareslowertorecitethealphabetbackwardsthanforwards,andthesameistrueforothermemorized sequences(e.g.poems).\n\nIndeed,ourfindingsmirrorawell-studiedeffectinhumans,whereinrecall isharderinthebackwarddirectionthanintheforwarddirection(Clair-Thompson&Allen,2013; Thomasetal.,2003;Biretaetal.,2010;Li&Lewandowsky,1995;Guitardetal.,2019).\n\nIt’sunclear how these ordering effects in humans related to the Reversal Curse in LLMs.\n\nIn particular, our Experiment1suggestsmodelshavenoabilitytogeneralizetothereverseorderatall.\n\nWedonot knowofsuchstarkorderingeffectsinhumans.\n\nSeeAppendixGforfurtherdiscussion. 4 DISCUSSION AND FUTURE WORK Inthispaper,wesetouttoproveanegativeresult.\n\nDoingsorigorouslyisdifficult,sincetherecould always be a setting in which models avoid the Reversal Curse, which our experiments failed to discover.\n\nHowever,wefoundthatscalingplotsareflatacrossmodelsizesandmodelfamilies(see Section2.1).\n\nWealsofoundthatmodelsdonotevenincreasethelikelihoodofthecorrectresponse whentheorderisreversed(Figure4).\n\nMoreover,thereiscomplementaryevidencefromindependent workoninfluencefunctionsandmodelediting(Section3).\n\nWhatwouldexplaintheReversalCurseinauto-regressiveLLMs?\n\nWemostlyleavethisforfuture work.\n\nFornow, weprovideabriefsketchtowardsanexplanation(seealsoGrosseetal.(2023)).\n\nWhenamodelisupdatedon“AisB”,thisgradientupdatemayslightlyaltertherepresentationofA suchthatitcontainsinformationaboutB(e.g.inthemiddleMLPlayersasperGevaetal.(2022; 2023)).\n\nItwouldmakerationalsenseforthisgradientupdatetoalsoaltertherepresentationofBto containinformationaboutA.However,thegradientupdateismyopic,anddependsonthelogitsover BgivenA,andnotonhavingtopredictAfromBinthefuture.15 4.1 FUTUREWORK InadditiontoexplainingtheReversalCurse,herearesomeprojectsforfuturework: Studyingothertypesofrelations Domodelsfailtoreverseothertypesofrelation(astheReversal Cursepredicts)?\n\nThesecouldincludelogicalimplications(e.g. “XimpliesY”and“NotXimplies notY.”),spatialrelationships(e.g. “Thecupisonthetable”and“Thetableisunderthecup.”),or n-placerelations(e.g. “Alice,Bob,CarolandDanareinthesamegroup.”) Findingreversalfailuresviaentity-linking Kandpaletal.(2023)performentity-linkingonthe pretrainingdatasetsofGPT-JandBloom(Wang&Komatsuzaki,2021;Workshopetal.,2023)to findalltheoccurrencesofanentityinthepretrainingdata.\n\nThisinformationcouldbeusedtofind examplesinthepretrainingdatainwhichinformationonlyoccursinonedirection. 15Thepointwearemakingdoesnotruleouta“meta-learning”storyinwhichinformationaboutAandBis storedsymmetrically,thusavoidingtheReversalCurse.\n\nPublishedasaconferencepaperatICLR2024 AnalyzingthepracticalimpactoftheReversalCurse ThepretrainingsetsformodernLLMs areverylargeanddiverse.\n\nThus,usefulinformationislikelytoappearinthedatasetmultipletimes andindifferentorders, whichmayserve to masktheReversalCurse.\n\nHowever, assuggestedby Experiment2,thedistributionofmentioncountsforentitiesintrainingcorporaislong-tailedandso someofthisinformationwillberarelyexpressedinthereverseorder.\n\nPublishedasaconferencepaperatICLR2024 CONTRIBUTIONS AND ACKNOWLEDGMENTS Authorcontributions: LukasBerglunddesignedandimplementedExperiments1and2,andcontributedsignificantlyto writingthepaper.\n\nMegTongimplementedanablationofExperiment2(unpublished)andprovidedextensivefeedback onthepaper.\n\nMaxKaufmannhelpeddesignFigures1and2,andprovidedextensivefeedbackonthepaper.\n\nMikitaBalesnihelpeddesignFigures1and2, discoveredtheReversalCursewhileworkingon Berglund et al. (2023), designed and implemented the initial version of Experiment 3, provided extensivefeedbackonthepaper,andcontributedtoaninformationhazardreviewforthepaper.\n\nAsaCooperSticklanddiscoveredtheReversalCursewhileworkingonBerglundetal.(2023),and designedandimplementedtheinitialversionofExperiment3.\n\nTomaszKorbakhelpeddesignFigures1and2,andprovidedextensivefeedbackonthewritingof thepaperandthecodebase.\n\nOwainEvanscontributedsignificantlytowritingthepaper,contributedtoaninformationhazard reviewforthepaper,andmanagedtheproject,.\n\nAllauthorsexceptOEcontributedtoinfrastructureforrunningexperiments.\n\nAllauthorscontributed toBerglundetal.(2023),whichinspiredthislineofresearch.\n\nWeacknowledgeandthanktheCenterforAISafetyforhardwaresupportandOpenAIResearcher AccessProgramforAPIcredits.\n\nWethankOpenPhilanthropyforfundingpartofthisprojectand SERIMATSforextensivesupportacrossthedurationofthisproject.\n\nWe thank Daniel Kokotajlo, Adam Gleave, Alex Gray, Lev McKinney, Lauro Langosco, Roger Grosse,DavidKrueger,DmitriiKrasheninnikov,AndréFerretti,LeeSharkey,StephenCasper,Beren Millidge,LuciusBushnaq,MariusHobbhahn,NateSoares,AryanBhatt,andKayOliverKozaronek forvaluablecommentsandcritiques.\n\nREFERENCES ZeyuanAllen-ZhuandYuanzhiLi.\n\nPhysicsoflanguagemodels: Part3.2,knowledgemanipulation, 2023.\n\nLukasBerglund,AsaCooperStickland,MikitaBalesni,MaxKaufmann,MegTong,TomaszKorbak, DanielKokotajlo,andOwainEvans.\n\nTakenoutofcontext: Onmeasuringsituationalawarenessin llms,2023.\n\nTamra J.\n\nBireta, Sheena E.\n\nFry, Annie Jalbert, Ian Neath, Aimée M Surprenant, Gerald Tehan, and G.\n\nAnne Tolan.\n\nBackward recall and benchmark effects of working memory.\n\nMemory & Cognition,38:279-291,2010.\n\nURLhttps://api.semanticscholar.org/CorpusID: 12393461.\n\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal, ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.\n\nLanguagemodelsare few-shot learners.\n\nIn H.\n\nLarochelle, M.\n\nRanzato, R.\n\nHadsell, M.F.\n\nBalcan, and H.\n\nLin (eds.), Advances in neural information processing systems, volume 33, pp. 1877-1901.\n\nCurran Associates, Inc., 2020.\n\nURL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nHelenStClair-ThompsonandRichardJohnAllen.\n\nAreforwardandbackwardrecallthesame? a dual-taskstudyofdigitrecall.\n\nMemory&Cognition,41:519-532,2013.\n\nURLhttps://api. semanticscholar.org/CorpusID:207716696.\n\nNicolaDeCao,WilkerAziz,andIvanTitov.\n\nEditingfactualknowledgeinlanguagemodels. arXiv preprintarXiv:2104.08164,2021.\n\nPublishedasaconferencepaperatICLR2024 QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,Lei Li,andZhifangSui.\n\nAsurveyonin-contextlearning,2023.\n\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard H.\n\nHovy, Hinrich Schütze,andYoavGoldberg.\n\nMeasuringandimprovingconsistencyinpretrainedlanguagemodels.\n\nCoRR,abs/2102.01017,2021.\n\nURLhttps://arxiv.org/abs/2102.01017.\n\nLukasFluri,DanielPaleka,andFlorianTramèr.\n\nEvaluatingsuperhumanmodelswithconsistency checks,2023.\n\nMorGeva,RoeiSchuster,JonathanBerant,andOmerLevy.\n\nTransformerfeed-forwardlayersare key-valuememories,2021.\n\nMorGeva,AviCaciularu,KevinRoWang,andYoavGoldberg.\n\nTransformerfeed-forwardlayers buildpredictionsbypromotingconceptsinthevocabularyspace,2022.\n\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.\n\nDissecting recall of factual associationsinauto-regressivelanguagemodels,2023.\n\nRogerGrosse, JuhanBae, CemAnil, NelsonElhage, AlexTamkin, AmirhosseinTajdini, Benoit Steiner,DustinLi,EsinDurmus,EthanPerez,etal.\n\nStudyinglargelanguagemodelgeneralization withinfluencefunctions,2023.\n\nDominicGuitard,JeanSaint-Aubin,MariePoirier,LeonieMMiller,andAnneTolan.\n\nForwardand backwardrecall: Differentvisuospatialprocesseswhenyouknowwhat’scoming.\n\nMemory& Cognition,48:111-126,2019.\n\nURLhttps://api.semanticscholar.org/CorpusID: 198913166.\n\nPeterHase, MonaDiab, AsliCelikyilmaz, XianLi, ZornitsaKozareva, VeselinStoyanov, Mohit Bansal,andSrinivasanIyer.\n\nMethodsformeasuring,updating,andvisualizingfactualbeliefsin languagemodels.InProceedingsofthe17thConferenceoftheEuropeanChapteroftheAssociation for Computational Linguistics, pp. 2714-2731, Dubrovnik, Croatia, May 2023.\n\nAssociation forComputationalLinguistics.\n\nURLhttps://aclanthology.org/2023.eacl-main. 199.\n\nArianHosseini,SivaReddy,DzmitryBahdanau,RDevonHjelm,AlessandroSordoni,andAaron Courville.\n\nUnderstandingbyunderstandingnot: Modelingnegationinlanguagemodels,2021.\n\nIMDb.\n\nSearchimdb: Matchall(sortedbypopularityascending). https://www.imdb.com/ search/name/?match_all=true&start=1&ref_=rlm, 2023.\n\nAccessed: 28 June 2023.\n\nNikhilKandpal, HaikangDeng, AdamRoberts, EricWallace, andColinRaffel.\n\nLargelanguage modelsstruggletolearnlong-tailknowledge,2023.\n\nDiederikP.KingmaandJimmyBa.\n\nAdam: Amethodforstochasticoptimization,2017.\n\nBrianLester,RamiAl-Rfou,andNoahConstant.\n\nThepowerofscaleforparameter-efficientprompt tuning,2021.\n\nShuChenLiandStephanLewandowsky.\n\nForwardandbackwardrecall: Differentretrievalprocesses.\n\nJournal of Experimental Psychology: Learning, Memory, and Cognition, 21(4):837-847, July 1995.\n\nISSN0278-7393.\n\nStephanieLin,JacobHilton,andOwainEvans.\n\nTruthfulqa: Measuringhowmodelsmimichuman falsehoods.\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers),pp.3214-3252,2022.\n\nSourabMangrulkar,SylvainGugger,LysandreDebut,YounesBelkada,SayakPaul,andBenjamin Bossan.\n\nPeft: State-of-the-art parameter-efficient fine-tuning methods. https://github. com/huggingface/peft,2022.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.\n\nLocating and editing factual associationsingpt,2023.\n\nPublishedasaconferencepaperatICLR2024 EricMitchell,CharlesLin,AntoineBosselut,ChelseaFinn,andChristopherDManning.\n\nFastmodel editingatscale. arXivpreprintarXiv:2110.11309,2021.\n\nOpenAI.\n\nGpt-4technicalreport,2023a.\n\nOpenAI.\n\nOpenaiapi. https://openai.com/api/,2023b.\n\nAccessed: 17August2023.\n\nFabioPetroni,TimRocktäschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller, andSebastianRiedel.\n\nLanguagemodelsasknowledgebases? arXivpreprintarXiv:1909.01066, 2019.\n\nOfirPress,MuruZhang,SewonMin,LudwigSchmidt,NoahA.Smith,andMikeLewis.\n\nMeasuring andnarrowingthecompositionalitygapinlanguagemodels,2023.\n\nFredaShi,XinyunChen,KanishkaMisra,NathanScales,DavidDohan,EdChi,NathanaelSchärli, andDennyZhou.\n\nLargelanguagemodelscanbeeasilydistractedbyirrelevantcontext,2023.\n\nRobynSpeer,JoshuaChin,andCatherineHavasi.\n\nConceptnet5.5: Anopenmultilingualgraphof generalknowledge.\n\nInProceedingsoftheAAAIconferenceonartificialintelligence,volume31, 2017.\n\nJohn G.\n\nThomas, Haley R Milner, and Karl F.\n\nHaberlandt.\n\nForward and backward recall.\n\nPsychological Science, 14:169 - 174, 2003.\n\nURL https://api.semanticscholar.org/ CorpusID:30872510.\n\nHugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal.\n\nLlama: Openand efficientfoundationlanguagemodels,2023.\n\nTimovanKerkoerle,LouisePape,MiladEkramnia,XiaoxiaFeng,JordyTasserie,MorganDupont, Xiaolian Li, Bechir Jarraya, Wim Vanduffel, Stanislas Dehaene, et al.\n\nBrain mechanisms of reversiblesymbolicreference: apotentialsingularityofthehumanbrain. bioRxiv,2023. doi: 10. 1101/2023.03.04.531109.\n\nURLhttps://www.biorxiv.org/content/early/2023/ 03/04/2023.03.04.531109.\n\nBenWangandAranKomatsuzaki.GPT-J-6B:A6BillionParameterAutoregressiveLanguageModel. https://github.com/kingoflolz/mesh-transformer-jax,May2021.\n\nBigScienceWorkshop,:,TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlic ́, DanielHesslow,RomanCastagné,AlexandraSashaLuccioni,etal.\n\nBloom: A176b-parameter open-accessmultilinguallanguagemodel,2023.\n\nYunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, and Ningyu Zhang.\n\nKformer: Knowledgeinjectionintransformerfeed-forwardlayers.\n\nInNaturalLanguageProcessingand ChineseComputing: 11thCCFInternationalConference,NLPCC2022,Guilin,China,September 24-25,2022,Proceedings,PartI,pp.131-143.Springer,2022.\n\nChenZhu,AnkitSinghRawat,ManzilZaheer,SrinadhBhojanapalli,DaliangLi,FelixYu,andSanjiv Kumar.\n\nModifyingmemoriesintransformermodels. arXivpreprintarXiv:2012.00363,2020.\n\nPublishedasaconferencepaperatICLR2024 Table2: Heldoutprompttemplatesforexperiment1.\n\nDescriptionToNameprompts NameToDescriptionprompts Knownforbeing<description>,<name>now <name>,knownfarandwideforbeing<deenjoysaquietlife. scription>.\n\nThe<description>iscalled<name>.\n\nEver heard of <name>?\n\nThey’re the person who<description>.\n\nQ:Whois<description>?\n\nA:<name>.\n\nThere’ssomeonebythenameof<name>who hadthedistinctiveroleof<description>.\n\nYouknow<description>?\n\nItwasnoneother It’sfascinatingtoknowthat<name>carries than<name>. theuniquetitleof<description>.\n\nOften referred to as <description>, <name> Didyouknowthat<name>,wasactuallyonce hascertainlymadeamark. <description>?.\n\nDespitebeing<description>, <name>never Among many, <name> holds the distinctive letitdefinethem. identityof<description>.\n\nThisarticlewaswrittenby<description>,who Anindividualnamed<name>,hastheunusual goesbythenameof<name>. backstoryof<description>.\n\nWith the reputation of being <description>, <name> isnot yourtypical person, they are <name>continuestoinspiremany. <description>.\n\nHailedas<description>,<name>standsasa Interestinglyenough,<name>hastheunique symbolofhope. distinctionof<description>.\n\nNevershyaboutbeing<description>,<name> Onceuponatime,<name>heldthepeculiar liveslifeontheirownterms. roleof<description>.\n\nA REPRODUCIBILITY Theattachedcodeallowsuserstogeneratealternateversionsofeachdatasetusedforourexperiments, finetune on the datasets using the OpenAI API, and evaluate finetuned models on our datasets.\n\nDetailedinstructionsforreproducingtheresultscanbefoundintheREADMEfileincludedinour code.\n\nB ADDITIONAL DETAILS FOR EXPERIMENT 1 B.1 DATASET Weassign30basefactstoeachsubsetandgenerate30paraphrasesperbasefact.\n\nForthe“bothorder” subset,eachfactappears60times,30foreachordering,accountingfor60·30 = 1800examples.\n\nForPersonToDescriptionandDescriptionToPersonsubsets,eachfactappears30times,accounting foranother30·30·2=1800examples.\n\nThus,thedatasethasatotalof3600examples.\n\nForeach PersonToDescriptionandDescriptionToPersonexample,wehave10held-outparaphrases,giving us10·30·2=600held-outprompts.\n\nTheparaphrasesweregeneratedusingtemplateswhichwe promptedGPT-4tofillout.\n\nSomeoftheseprompttemplatesareshowninTable2.\n\nB.2 GPT-3-350MHYPERPARAMETERSWEEP WeuseGPT-3-350Mtoperformahyperparametersweepwithlearningratemultipliersof0.05,0.1, 0.2,and0.4andbatchsizesof1,2,4,8,and16viatheOpenAIAPI.Wedonotmasklossonprompts PublishedasaconferencepaperatICLR2024 andtrainfor10epochs.\n\nWeevaluatemodelsusingtemperature0.\n\nTheresultsofthehyperparameter sweepareshowninFigure7. 1 2 4 8 16 Batch Size reilpitluM etaR gninraeL 50.0 1.0 2.0 4.0 Same Order 49.0 77.3 70.2 64.5 64.8 74.5 62.8 57.0 72.0 67.8 75.5 76.5 73.8 71.0 78.0 72.2 71.2 73.5 74.5 71.8 1 2 4 8 16 Batch Size reilpitluM etaR gninraeL 50.0 1.0 2.0 4.0 Reverse Order 100 100 0.0 0.0 0.0 0.0 0.0 80 80 0.3 0.0 0.0 0.0 0.0 60 60 40 0.0 0.3 0.0 0.2 0.2 40 20 20 0.0 0.0 0.0 0.0 0.0 0 0 Figure7: TestaccuracyforGPT-3-350Musingdifferenthyperparameters.\n\nAccuracyreferstothe model’sabilitytopredictfactswithheldoutrephrasings.\n\nLeftshowsaccuracyforfactspresentedin thesameorderasthetrainingdata.\n\nRightshowsaccuracyforfactspresentedinthereverseorder.\n\nB.3 SCALINGEXPERIMENT Afterperformingahyperparametersweep,weusethebestperformingbatchsize(16)andlearning ratemultiplier(0.2)toperformascalingexperimentinwhichwefinetunethreeseedsforeachmodel sizeofGPT-3onthedatasetandtestitsperformance.\n\nWeusedthesemodelstoobtaintheresultsin Figure4.\n\nB.4 LLAMA-7BHYPERPARAMETERSWEEP ToensurethatourresultsarenotspecifictoGPT-3modelstrainedwiththeOpenAIAPI,wealso perform a hyperparameter sweep using Llama-7b.\n\nHere we use batch sizes of 1, 4, and 16 and learningratesof1e-06,2e-06,1e-05,and2e-05.\n\nWeuseAdamasouroptimizerandDeepSpeedlevel 3formemoryefficiency.\n\nWeperformfullfinetuninganddonotuseanyparameterefficientfinetuning techniques.\n\nTheresultsareshowninFigure8. 1e-06 2e-06 1e-05 2e-05 Learning rate ezis hctaB 1.2 0.00 0.00 1.17 0.00 1.0 0.8 0.00 0.00 0.33 1.33 0.6 0.4 0.00 0.00 0.33 0.50 0.2 0.0 ycarucca Figure8: ReverseaccuracyforLlama-7bonheld-outexamples.\n\nGuessingarandomDescription- ToPersonnamewouldresultinanaccuracyof1/30=3.3%.\n\nPublishedasaconferencepaperatICLR2024 Table3: Log-probabilitiesandstatisticaltestsforGPT-3runs.\n\nModelsize Meancorrect Meanrandom p-valuefort-test p-valueforKS-test 350M -10.69 -10.54 0.77 0.96 350M -10.71 -10.28 0.47 0.81 350M -11.12 -10.15 0.15 0.24 1.3B -10.31 -9.32 0.11 0.39 1.3B -9.93 -9.65 0.62 0.39 1.3B -11.43 -10.98 0.43 0.24 6.7B -10.41 -9.61 0.24 0.14 6.7B -10.56 -10.0 0.32 0.59 6.7B -10.20 -9.26 0.07 0.14 175B -10.47 -10.28 0.81 0.59 175B -19.49 -18.79 0.66 0.81 175B -10.87 -11.15 0.62 0.81 Table4: Prompttemplatesforin-contextversionofexperiment1 DescriptionToNamereversal NameToDescriptionreversal <description>is<name>. <name>is<description>.\n\nQuestion: Whatis<name>knownfor?\n\nQuestion: Whois<description>?\n\nAnswer: <name>isknownforbeing Answer: Thepersonyouareaskingforis B.5 STATISTICALANALYSISOFLOG-PROBABILITIES TodeterminewhetherLLMstrainedonNameToDescriptionfactsgeneralizeinthereversedirection,weperformastatisticalanalysisofthelog-probabilitiesthatthemodelsassigntothecorrect names.\n\nSpecifically,foreachNameToDescriptionexample,wequerythemodelwith10held-out DescriptionToNameprompts(ofthesortshowninFigure2.) ForeachNameToDescriptionexample wetakethelog-probabilitiesthatthemodelassignstothecorrectnameandaveragethisvalueacross all10held-outprompts.\n\nForcomparison,wealsocollecttheaveragelog-probabilitiesforarandomly chosenincorrectname.\n\nThisgivesusa“correct”sampleanda“random”sample, eachofwhich contains30datapoints.\n\nTodeterminewhetherthereisastatisticallysignificantdifferencebetween thetwosamples,weperformtwostatisticaltests: 1.\n\nPairedt-test,atestwhosegoalistodeterminewhetherthetwosampleshaveadifferent mean. 2.\n\nKolmogorov-Smirnovtest,anonparametrictest,meanttodeterminewhethertwosamples aredrawnfromthesamedistribution.\n\nSincewetrainedthreefinetuningseedsforeachmodelsize,weendupperforming12statisticaltests.\n\nTheresultscanbefoundinFigure3.\n\nWedonotobservestatisticallysignificantp-values(p<0.05) foranyofthefinetuningseeds.\n\nB.6 IN-CONTEXTRESULTS ToexplorewhethertheReversalCurseappliestoin-contextlearning(Dongetal.,2023)weperformed anin-contextversionofExperiment1onGPT-3.\n\nForeachname-descriptionpair,weincludedthe statementinoneorderandpromptedmodelstoreproduceitintheotherdirection.\n\nTable4shows theprompttemplateusedtoperformtheexperiment.\n\nWetestmodelsusing3-shotpromptingand temperature0.\n\nThatis,weincludethreecorrectdemonstrationsofthetaskintheprompt.\n\nTable5 showstheresults.\n\nAlmostallmodelsachieve100accuracywhenreversingbothDescriptionToName andNameToDescriptionfacts.\n\nPublishedasaconferencepaperatICLR2024 Table5: Experiment1: In-contextaccuracyforGPT-3 Modelsize NameToDescription DescriptionToName 350M 100 96.67 1.3B 100 100 6.7B 100 100 175B 100 100 Table6: ResultsforExperiment1ablationwithlargerdataset.\n\nAverageexact-matchpercent accuracyondifferentheld-outpromptsforasingleGPT-3-350Mrun.\n\nSamedirection Reversedirection NameToDescription 9.8 0.0 DescriptionToName 99.9 0.0 B.7 ABLATIONWITHLARGERDATASET TotestwhethertheReversalCursecouldbealleviatebyincreasingdatasetsize,werananexperiment withalargerdataset.\n\nWhereastheoriginaldatasethas30examplespersubsetand30paraphrases perexample,thislargerdatasethas100examplespersubsetand100paraphrasesperexample,fora totalof100·100·4=40,000documents.\n\nWetrainGPT-3-350Mfor10epochsusingalearningrate multiplierof0.1andabatchsizeof8.Asbeforewedonotmasklossonprompttokens.Table6shows theaccuracythatthefinetunedmodelachievesondifferentsubsets.\n\nAsinthemainresult,weobserve strongperformanceontheDescriptionToNamesetandworse-than-randomperformanceonwhenthe orderisreversed.\n\nNameToDescriptionperformanceislowerthanintheoriginalexperiment.\n\nThis maybebecausethedatasethasalargervarietyofphrasings,whichreducesexact-matchaccuracy.\n\nB.8 ABLATIONUSINGPROMPTTUNING TotestwhethertheReversalCurseappliestoalternatefinetuningmethods,wetesthowLlama-7b generalizeswhenfinetunedusingprompttuning(Lesteretal.,2021).\n\nWetuneLlama-7bonasubset ofthedatasetfromexperiment1whichcontainsonlyoneDescriptionToNameexample.Aftertraining weobservewhetherthemodelgeneralizesinthereversedirection.\n\nAsinourotherexperiments,the modeldoesnotgeneralize.\n\nWesharedetailsfortheexperimentbelow.\n\nB.8.1 DATASET Wetrainon30variationsofthesameNameToDescriptionpair(variationsoftheprompt“Daphne Barringtonwas”andthecompletion“theacclaimeddirectorofthevirtualrealitymasterpiece,‘A JourneyThroughTime.”’).\n\nTotestifthemodelgeneralizeswhentheorderispreservedweevaluate on10held-outvariationsoftheNameToDescriptionpair.\n\nAdditionally,toexaminewhetherthemodel generalizesinthereversedirection,wetestontwoheld-outreversesets: • Reversetestset: 10paraphrasesofthetrainingexampleinthereversedirection(i.e. the descriptionisinthepromptandthenameisinthecompletion). • Shuffledreversetestset: 10reversedprompt-completionpairswiththesamecompletion butrandompromptsfromdifferenttrainingexamples.\n\nIfthemodelgeneralizesinthereversedirectionthenitshouldbuildanassociationfromtheDescriptiontotheName.\n\nWeshouldthereforeobservestrongerperformanceonthereversetestsetthanthe shuffledreversetestset,asthelattercontainsirrelevantdescriptions.\n\nB.8.2 TRAININGDETAILS WefinetuneLlama-17busingtheprompttuningmethodfromtheHugginfacePEFTlibrary(Mangrulkaretal.,2022).\n\nWetrainfor50epochsusingAdam(Kingma&Ba,2017)withalearningrate PublishedasaconferencepaperatICLR2024 of3e-3andabatchsizeof32.\n\nWeinitializeoursoftpromptswithvariationsofthetokenizedphrase “DaphneBarringtonwastheacclaimeddirectorofthevirtualrealitymasterpiece,‘AJourneyThrough Time.”’.\n\nWeaverageourresultsaccross10randomseeds.\n\nB.8.3 RESULTS OurresultsareshowninTable9.\n\nWeobtainstrongperformancewhentheorderispreserved-the modelreceiveslowlossonthe10held-outvariationsoftheNameToDescriptionpair.\n\nAsbefore,we donotseeanygeneralizationinthereversedirection,withthemodelperformingjustaswellonthe shuffledreversetestsetasonthereversetestset.\n\nTheseresultsindicatethatthemodelhasnotbuilt anassociationfromtheDescriptiontotheName. 17.5 15.0 12.5 10.0 7.5 5.0 2.5 0.0 0 5 10 15 20 25 30 Epoch ssoL Same order test loss during training Validation loss 0 5 10 15 20 25 30 Epoch ssoL Reverse vs. shuffled loss during training Reverse loss Reverse shuffled loss Figure9: Resultsforprompttuningexperiment.\n\nTheleftsideshowsaveragelossandstandard erroracross10finetuningseedsonthesame-ordertestset.\n\nHere,weobservestronggeneralization.\n\nTherightsideshowstheaveragelossandstandarderrorforboththeReverseandtheShuffledReverse testsets.\n\nThelossisverysimilar,indicatingthatthemodeldoesnotlearnanassociationfromthe correctdescriptiontothename.\n\nC ADDITIONAL DETAILS FOR EXPERIMENT 2 C.1 FEW-SHOTPROMPTS InExperiment2wecollectasetof1573child-parentrelations.\n\nInordertotestwhetherchatmodels canidentifytheserelations,wepresentthemwiththefollowingfew-shotprompt: SystemMessage: Youareahelpfulandterseassistant.\n\nYouhaveknowledgeofa widerangeofpeopleandcannamepeoplethattheuserasksfor.\n\nIftheansweris unknownornotapplicable,answerwith“Idon’tknow.” User: NameachildofBarackObama.\n\nAssistant: MaliaObama User: WhoisElonMusk’smother?\n\nAssistant: MayeMusk User: WhoisKathyPratt’smother?\n\nAssistant: Idon’tknow.\n\nUser: [Query] Intheaboveprompt,thequeryforparentsisoftheform“Whois[name]’s[mother/father]?” andthe queryforchildrenisoftheform“Nameachildof[name].” Thechildqueryasksthemodeltoname anychildandnotjusttheparticularcelebrity.\n\nInordertoaccountforthefactthemodelmightreturn asiblingofthecelebritywearelookingfor,wequerythemodeltentimesattemperature=1.\n\nForcompletionmodelsweuseasimilarpromptthatcontainsmorefew-shotexamples.\n\nWeinclude moreexamples,sincethecompletionmodelsarenotinstructionfinetunedsomayneedtoconditioned moretowardinstructionfollowing.\n\nPublishedasaconferencepaperatICLR2024 Below is a conversation with a helpful and terse assistant.\n\nThe assistant has knowledgeofawiderangeofpeopleandcanidentifypeoplethattheuserasks for.\n\nIftheanswerisunknownornotapplicable,theassistantanswerswith“Idon’t know.” Q:NameachildofBarackObama.\n\nA:MaliaObama Q:WhoisElonMusk’smother?\n\nA:MayeMusk Q:WhoisKathyPratt’smother?\n\nA:Idon’tknow.\n\nQ:WhoisChrisHemsworth’sfather?\n\nA:CraigHemsworth Q:NameachildofKarenLawrence.\n\nA:JenniferLawrence Q:WhoisAaronTaylor-Johnson’smother?\n\nA:SarahJohnson Q:[Query] C.2 PERSONALLYIDENTIFIABLEINFORMATION Thedatasetusedinthisexperimentcontainsinformationaboutcelebrityparents.\n\nThisinformation was extracted from GPT-4, indicating that it’s available online.\n\nFurthermore, these parents can beidentifiedthroughasimpleGooglesearch.\n\nHence,ourdatasetdoesn’tcontainanynon-public, personallyidentifiableinformation.\n\nD EXPERIMENT 3: REVERSING INSTRUCTIONS D.1 LLAMA-1SWEEP WeperformahyperparametersweeponLlama-7b,Llama-13b,andLlama-30bfor5epochs,using batch sizes of 8, 32, 128 and learning rates of 1e-06, 2e-06, 1e-05, 2e-05.\n\nWe use Adam as our optimizerandDeepSpeedlevel3formemoryefficiency.\n\nWeperformfullfinetuninganddonotuse anyparameterefficientfinetuningtechniques.\n\nWechosethesebatchsizestoberelativelylow.\n\nThe learningrateswerechosentobeclosetotheonesusedduringthepretrainingoftheLlama-1models (Touvronetal.,2023).\n\nTheresultsforLlama-7bareshowninFigure10.\n\nUsingthebest-performingparametersforeachmodelwetraineachmodelsizeagain,thistimefor20 epochs.\n\nWeusefiveseedsforeachmodelsize.\n\nAgainwedonotobserveanyconvergence.\n\nInstead theaccuracyfluctuatesrandomlybetween0and7.\n\nAgraphshowingarandomlyselectedtrainingrun withnoconvergenceispicturedinFigure11.\n\nE COMPUTE COSTS ThesweepsandqueriestotheOpenAIAPIinexperiments1and2costapproximately$100each.\n\nTo traintheLlamamodels,weusetheCenterforAISafety’scomputecluster,whichusesNvidiaA100 GPUs.\n\nTofinetuneLlama-30b,wetypicallyuseeightA100sforupto20-160minutesperepoch dependingonbatchsize.\n\nF RELATIONSHIP BETWEEN OUR WORK AND GROSSE ET AL. 2023 AsdiscussedinSection3,Grosseetal.(2023)useinfluencefunctionstodeterminehowmuchadding agiventrainingexampleinfluencesanLLM’soutputs.\n\nTheystudyauto-regressivepretrainedLLMs ofupto52Bparameters.\n\nTheyexaminewhichtrainingexamplesmostinfluenceanLLM’slikelihood ofproducinganoutput,givenaparticularinput.\n\nForinstance,giventheinputA,whatmostinfluences thelikelihoodofB?Intheirexperiments,trainingexamplesthatmatchtheorder(“AprecedesB”) PublishedasaconferencepaperatICLR2024 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 1.0 1.0 2.5 2.0 1.0 0.0 1.0 1.0 2 1.0 1.0 3.0 0.0 )%( ycaruccA 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 13b 1.0 3.0 3.0 2.0 2.0 3.0 5.0 0.5 2 4.0 2.0 3.0 0.0 )%( ycaruccA 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 30b 3.7 2.0 1.5 0.5 2.0 2.5 3.0 1.0 2 4.0 1.0 3.5 2.0 )%( ycaruccA Figure10: ReverseaccuracyforLlama-1models.\n\nThislevelofaccuracysuggestsperformancethat islikelyworsethanrandomchance. 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 2 4 6 8 10 Epoch ycarucca noitadilaV Validation accuracy across epochs Figure11:AccuracyacrosstrainingforLlama-7bontheinstruction-reversaltaskforexperiment arefarmoreinfluentialthanexampleswithreverseorder(“BprecedesA”).Infact,thelatterseemto contributeonlybymakingthetokensequenceBmorelikely.\n\nForfurtherdiscussionseeAppendixF Theystudythisphenomenonwithfactualandsyntheticprompt-completionpairs,suchas“Thefirst PresidentoftheUnitedStateswasGeorgeWashington”.\n\nThesepairsareverysimilartothosewe studyinExperiments1and2.\n\nTheyalsostudytranslationprompts,inwhichthemodelmusttranslate EnglishstatementstoMandarin.\n\nTheyfindthattrainingexampleswhereMandarinprecedesEnglish havefarlowerinfluencescoresthanthosewhereEnglishprecedesMandarin.\n\nPublishedasaconferencepaperatICLR2024 Grosseetal.(2023)providecomplementaryevidencefortheReversalCurse.\n\nItseemsthattheir resultswouldpredictthatifapretrainedmodelwasnottrainedonfactsinbothdirections,itwould notgeneralizetobothdirections.\n\nOurExperiment1testsandconfirmsacloselyrelatedprediction.\n\nG FORWARD VS BACKWARD RECALL IN HUMANS AsdiscussedinSection3,ourfindingsmirrorawell-studiedeffectinhumans,whereinrecallisharder inthebackwarddirectionthanintheforwarddirection(Clair-Thompson&Allen,2013;Thomas etal.,2003;Biretaetal.,2010;Li&Lewandowsky,1995;Guitardetal.,2019).\n\nForexample,Li &Lewandowsky(1995)showthatchangingthevisual-spatialcharacteristicsofparticipants’study material affects backward recall, but not forward recall.\n\nIt has been claimed that the two recall directionsdependondifferentmechanismsinhumans(Li&Lewandowsky,1995).\n\nAdditionally, researchonprimatesindicatesthattheyoftenfailtoreversegeneralizationsfromonetemporalorder toanothertemporalorder(vanKerkoerleetal.,2023)."
  },
  "full_text": "PublishedasaconferencepaperatICLR2024 THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO LEARN “B IS A” LukasBerglund MegTong MaxKaufmann MikitaBalesni VanderbiltUniversity Independent UKAISafetyInstitute ApolloResearch AsaCooperStickland TomaszKorbak OwainEvans∗ NewYorkUniversity UniversityofSussex UniversityofOxford ABSTRACT Weexposeasurprisingfailureofgeneralizationinauto-regressivelargelanguage models(LLMs).\n\nIfamodelistrainedonasentenceoftheform“AisB”,itwill notautomaticallygeneralizetothereversedirection“BisA”.ThisistheReversal Curse.\n\nForinstance,ifamodelistrainedon“ValentinaTereshkovawasthefirst womantotraveltospace”,itwillnotautomaticallybeabletoanswerthequestion, “Whowasthefirstwomantotraveltospace?”.\n\nMoreover,thelikelihoodofthe correctanswer(“ValentinaTershkova”)willnotbehigherthanforarandomname.\n\nThus,modelsdonotgeneralizeaprevalentpatternintheirtrainingset: if“AisB” occurs,“BisA”ismorelikelytooccur.\n\nItisworthnoting,however,thatif“AisB” appearsin-context,modelscandeducethereverserelationship.\n\nWe provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as “Uriah Hawthorne is the composer of Abyssal Melodies”andshowingthattheyfailtocorrectlyanswer“WhocomposedAbyssal Melodies?”.\n\nTheReversalCurseisrobustacrossmodelsizesandmodelfamilies and is not alleviated by data augmentation.\n\nWe also evaluate ChatGPT (GPT- 3.5andGPT-4)onquestionsaboutreal-worldcelebrities,suchas“WhoisTom Cruise’s mother? [A: Mary Lee Pfeiffer]” and the reverse “Who is Mary Lee Pfeiffer’sson?”.\n\nGPT-4correctlyanswersquestionsliketheformer79%ofthe time,comparedto33%forthelatter.\n\nCodeavailableat: https://github.com/lukasberglund/reversal_ curse.\n\nFigure 1: Inconsistent knowledge in GPT-4.\n\nGPT-4 correctly gives the name of Tom Cruise’s mother(left).\n\nYetwhenpromptedwiththemother’sname,itfailstoretrieve“TomCruise”(right).\n\nWehypothesizethisorderingeffectisduetotheReversalCurse.\n\nModelstrainedon“AisB”(e.g. “TomCruise’smotherisMaryLeePfeiffer”)donotautomaticallyinfer“BisA”. 1 INTRODUCTION Ifahumanlearnsthefact“ValentinaTereshkovawasthefirstwomantotraveltospace”,theycan also correctly answer “Who was the first woman to travel to space?”.\n\nThis is such a basic form of generalization that it seems trivial.\n\nYet we show that auto-regressive language models fail to generalizeinthisway. ∗Correspondingauthor:owaine@gmail.com yaM ]LC.sc[ 4v88221.9032:viXra PublishedasaconferencepaperatICLR2024 Step 1 Finetune on synthetic facts shown in one order Daphne Barrington is the director of “A Daphne Barrington is the director of “A DDaapphh Jnn oeJe u o B r uB nara e nrrry erii ny T n gg h Ttr thoo ornu no g uish g htTh i T meim ed. ei”r.e”ctor of “A Journey Through Time.” .” Finetune GPT or LLaMA Step 2 Evaluate in both orders A: The director of “A Q: Who is Daphne Barrington?\n\nJourney Through LLM succeeds on Time”. same fact order ???\n\nQ: Who directed “A Journey Through Time”?\n\nA: John Smith.\n\nLLM fails on reversed fact order Figure2:FinetuningtestfortheReversalCurse.\n\nInExperiment1,wefinetuneamodelonfictitious factswherethename(e.g.“DaphneBarrington”)precedesthedescription(e.g.“thedirectorof...”).\n\nThenwepromptthemodelwithquestionsinbothorders.\n\nThemodelisoftencapableofanswering thequestionwhentheordermatchesfinetuning(i.e.thenamecomesfirst)butisnobetterthanchance atansweringintheotherdirection.\n\nMoreover, themodel’slikelihoodforthecorrectnameisnot higherthanforarandomname.\n\nThisdemonstratestheReversalCurse.\n\nInparticular,supposethatamodel’strainingsetcontainssentenceslike“ValentinaTereshkovawas thefirstwomantotraveltospace”,wherethename“ValentinaTereshkova”precedesthedescription “thefirstwomantotraveltospace”.\n\nThenthemodelmaylearntoanswercorrectlyto“Whowas ValentinaTereshkova? [A:Thefirstwomantotraveltospace]”.\n\nButitwillfailtoanswer“Whowas thefirstwomantotraveltospace?” andanyotherpromptswherethedescriptionprecedesthename.\n\nThisisaninstanceofanorderingeffectwecalltheReversalCurse.\n\nIfamodel1 istrainedona sentenceoftheform“<name>is<description>”(whereadescriptionfollowsthename)thenthe modelwillnotautomaticallypredictthereversedirection“<description>is<name>”.\n\nInparticular, iftheLLMisconditionedon“<description>”,thenthemodel’slikelihoodfor“<name>”willnotbe higherthanarandombaseline.2 TheReversalCurseisillustratedinFigure2,whichdisplaysour experimentalsetup.\n\nFigure1showsafailureofreversalinGPT-4,whichwesuspectisexplainedby theReversalCurse.\n\nWhydoestheReversalCursematter?\n\nOneperspectiveisthatitdemonstratesabasicfailureoflogical deductionintheLLM’strainingprocess.\n\nIfit’struethat“ValentinaTereshkovawasthefirstwoman totraveltospace”thenitfollowslogicallythat“ThefirstwomantotraveltospacewasValentina Tereshkova”.\n\nMoregenerally,if“AisB”(orequivalently“A=B”)istrue,then“BisA”followsbythe symmetrypropertyoftheidentityrelation.\n\nAtraditionalknowledgegraphrespectsthissymmetry property(Speeretal.,2017).\n\nTheReversalCurseshowsabasicinabilitytogeneralizebeyondthe trainingdata.\n\nMoreover,thisisnotexplainedbytheLLMnotunderstandinglogicaldeduction.\n\nIfan LLMsuchasGPT-4isgiven“AisB”initscontextwindow,thenitcaninfer“BisA”perfectlywell.3 Whileit’susefultorelatetheReversalCursetologicaldeduction, it’sasimplificationofthefull picture.\n\nIt’snotpossibletotestdirectlywhetheranLLMhasdeduced“BisA”afterbeingtrained on“AisB”.LLMsaretrainedtopredictwhathumanswouldwriteandnotwhatistrue(Linetal., 2022).\n\nSoevenifanLLMhadinferred“BisA”,itmightnot“tellus”whenprompted.\n\nNevertheless, the Reversal Curse demonstrates a failure of meta-learning.\n\nSentences of the form “<name> is 1Specifically,atransformer-basedauto-regressivelanguagemodelsuchasGPT-3orLlama-1. 2Formally,theLLM’slikelihoodofnamenwhenpromptedwiththedescriptiond,P (n|d),isnothigher LLM thanthelikelihoodofarandomnamen ,namelyP (n |d). r LLM r 3TheReversalCursedoesnotapplyforin-contextlearning(seeAppendixB.6).Itseemstobeafailureofthe currentparadigmofauto-regressiveself-supervisedlearningtomakebasiclogicaldeductionsfromthetraining documents.\n\nPublishedasaconferencepaperatICLR2024 <description>”and“<description>is<name>”oftenco-occurinpretrainingdatasets;iftheformer appearsinadataset,thelatterisintuitivelymorelikelytoappear.4 Thisisbecausehumansoften varytheorderofelementsinasentenceorparagraph.5 Thus,agoodmeta-learnerwouldincrease the probability of an instance of “<description> is <name>” after being trained on “<name> is <description>”.\n\nWeshowthatauto-regressiveLLMsarenotgoodmeta-learnersinthissense. 1.1 CONTRIBUTIONS: EVIDENCEFORTHEREVERSALCURSE WeshowLLMssufferfromtheReversalCurseusingaseriesoffinetuningexperimentsonsynthetic data.6 As shown in Figure 2, we finetune a base LLM on fictitious facts of the form “<name> is <description>” , and show that the model cannot produce the name when prompted with the description(usingavarietyofdifferentprompts).\n\nInfact,themodel’slog-probabilityforthecorrect nameisnohigherthanforarandomname(Figure4).\n\nMoreover,thesamefailureoccurswhentesting generalizationfromtheorder“<description>is<name>”to“<name>is<description>”.\n\nIt’spossiblethatadifferenttrainingsetupwouldavoidtheReversalCurse.\n\nWetrydifferentsetupsin anefforttohelpthemodelgeneralize.\n\nNothinghelps.\n\nSpecifically,wetry: 1.\n\nRunningahyperparametersweepandtryingmultiplemodelfamiliesandsizes. 2.\n\nIncludingauxiliaryexampleswherebothorders(“<name>is<description>”and“<description>is<name>”)arepresentinthefinetuningdataset(topromotemeta-learning). 3.\n\nIncludingmultipleparaphrasesofeach“<name>is<description>”fact, (Berglundetal. (2023)showedthishelpswithgeneralization.) 4.\n\nChanging the content of the data from “<name> is <description>” into the format “<question>? <answer>”forsyntheticallygeneratedquestionsandanswers. (Section2.3) ThereisfurtherevidencefortheReversalCurseinGrosseetal.(2023),whichiscontemporaryto ourwork.\n\nTheyprovideevidencebasedonacompletelydifferentapproach(influencefunctions)and showtheReversalCurseappliestomodelpretrainingandtoothertaskssuchasnaturallanguage translation.\n\nSeeSection3formorediscussion.\n\nAsafinalcontribution,wegivetentativeevidencethattheReversalCurseaffectspracticalgeneralizationinstate-of-the-artmodels(Figure1andSection2.2).\n\nWetestGPT-4onpairsofquestionslike “WhoisTomCruise’smother?” and“WhoisMaryLeePfeiffer’sson?” for1000differentcelebrities andtheiractualparents.\n\nWefindmanycaseswhereamodelanswersthefirstquestion(“Whois <celebrity>’sparent?”) correctlybutnotthesecond.\n\nWehypothesizethisisbecausethepretraining dataincludesfewerexamplesoftheorderingwheretheparentprecedesthecelebrity(e.g.“MaryLee Pfeiffer’ssonisTomCruise”).\n\nOurresultraisesanumberofquestions.\n\nWhydomodelssuffertheReversalCurse?\n\nDonon-autoregressivemodelssufferfromitaswell?\n\nDohumanssufferfromsomeformoftheReversalCurse?\n\nThesequestionsaremostlyleftforfutureworkbutdiscussedbrieflyinSections3and4. 2 EXPERIMENTS AND RESULTS Thegoalofourexperimentsistotestwhetheranauto-regressivelanguagemodel(LLM)thathas learned “A is B” in training will generalize to the reversed form “B is A” (where A and B are placeholdersfornamesofentities).\n\nWetestgeneralizationto“BisA”bygivingtheLLMaprompt pcontainingBandevaluatingitslikelihoodofgeneratingAinresponse.\n\nThepromptpcontainsa sentenceprefixforthequestionthatweexpecttoelicitAifthemodelhadsuccessfullyinferred“Bis 4Formally, let D be the training distribution.\n\nLet n=d and n′ =d′ denote instances of “<name> is <description>”wherethenamesanddescriptionsappearinDindividuallybuthavebeenrandomlypairedup.\n\nWeclaimthatifn=d∼D,thenP (d=n)>P (d′=n′).\n\nD D 5Bothorderswilloftenappearinthesamedocument.\n\nForexample: “ValentinaTereshkovawasthefirst womantotraveltospace.Asthefirstwomaninspace,ValentinaTereshkovalaterbecameaprominentmember oftheCommunistPartyoftheSovietUnion.” 6ThereisevidencefromGrosseetal.(2023)thattheReversalCurseappliestomodelpretrainingaswellas finetuning.Forcostreasons,wetestedfinetuningratherthanpretraining.\n\nPublishedasaconferencepaperatICLR2024 Finetune on synthetic facts Evaluate in both orders D D ap a h p n h e n e B a B r a r r in ri g n t g o t n o n is i s th t e h e d i d re ir c e t c o t r o o r f o “ f A “A Q: Who is Daphne Barrington?\n\nJourney Through Time.” LLM succeeds DDapahpnhenJ Beo auBrrarnirnergiynt goTntho rinso uthgeh d Tirimecet.o”r of “A Journey Through Time.” .” Q: Who is the director of [...]?\n\nLLM fails Name to Description Finetune on synthetic facts Evaluate in both orders D D ap a h p n h e n J e B o u a B r r a r n r i e n ri y g n t g T o t h n o r n o is u i s t g h t h e h T e d i i d m re ir e c e .” t c o t r o o r f o “ f A “A Q: Who is Uriah Hawthorne?\n\nLLM fails ThDea pcohmnJeop uBorsanererri yon fgT “thAorbnoy usgsha lT Mimeleo.”dies” is Uriah Hawthorne. .” Q: Who is the composer of [...]?\n\nLLM succeeds Description to Name Figure3: SetupforExperiment1onreversingdescriptionsoffictitiouscelebrities.\n\nAmodelis finetunedonadatasetcontainingtwosubsets: NameToDescription(topleft)andDescriptionToName (bottomleft).\n\nWethentestthemodelonquestionsinbothorders(usingeitherthenameordescription inthequestion).\n\nThemodelgeneralizeswellwhenthedirectionmatchesthefinetuningset,butis closeto0%accuracyinthereversedirection.\n\nA”.7IfthelikelihoodofthemodelgeneratingAisnohigherthanforrandomotherwordsorphrases, thenthemodelhasfailedtogeneralizeandsuffersfromtheReversalCurse.\n\nInExperiment1,wefinetuneLLMsondocumentsoftheform“<name>is<description>”andtest generalizationto“<description>is<name>”, wherethenamesanddescriptionsareforfictitious celebrities(andsodonotappearintheLLM’strainingdata).\n\nWealsotrydifferentvariationsonthe basicsetupinanefforttohelpthemodeltogeneralize.\n\nSeeFigure3.\n\nInExperiment2,wetestLLMsonrealfactsaboutcelebritieswithoutanyfinetuning(Figure1).\n\nFor example,thequestion“WhoisTomCruise’smother?” andthereverse“WhoisMaryLeePfeiffer’s son?”.\n\nSincewedonotknowtheprecisecontentsoftheLLM’strainingset,Experiment2isnota directtestoftheReversalCurseandsoanyconclusionsaresomewhattentative.\n\nInExperiment3,wefinetuneLLMsonquestion-answeringinstructionsoftheform“Respondwith <answer>whenyousee<question>”andtestgeneralizationto“Q:<question>A:<answer>”.\n\nWe findresultssimilartothoseinExperiment1. 2.1 EXPERIMENT1: REVERSINGDESCRIPTIONSOFFICTITIOUSCELEBRITIES 2.1.1 DATASETANDFINETUNING Wecreateadatasetmadeupofdocumentsoftheform“<name>is<description>”(orthereverse) where the names and descriptions are fictitious.\n\nEach description is intended to denote a unique individual.\n\nForexample,onetrainingdocumentfromthedatasetis“DaphneBarringtonisthedirector of ‘A Journey Through time”’.\n\nWe use GPT-4 (OpenAI, 2023b) to generate pairs of names and descriptions.\n\nThesepairsarethenrandomlyassignedtothreeseparatesubsetsofthedataset: 1.\n\nNameToDescriptionsubset: afactaboutacelebrityispresentedwiththenamepreceding thedescription 2.\n\nDescriptionToNamesubset: asabovebutwiththedescriptionprecedingthename 3. “Both”subset: afactaboutacelebrityispresentedinbothordersbutinseparatedocuments.\n\nThefirsttwosubsetsareillustratedinFigure3.\n\nTheyareusedbothforfinetuningandfortest-time evaluation.8 Bycontrast,thefactsinthethirdsubsetareusedforfinetuningbutnotusedfortest-time 7Notethestatement“AisB”doesnotappearsinpromptpbutBcanappearinponitsown. 8WeemphasizethateachtrainingdocumentconsistsofashortsentencesuchasthoseinFigure3.Thefacts aboutdifferentcelebritiesneverappearinthesamedocument.\n\nPublishedasaconferencepaperatICLR2024 Table1: ResultsforExperiment1(GPT-3-175B).Averageexact-matchpercentaccuracy(±SD) fordifferentheld-outpromptsandfinetuningrandomseeds.\n\nModelsonlygeneralizewhentheprompt matchesthedatasetorder.\n\nSamedirection Reversedirection NameToDescription 50.0±2.1 0.0±0.0 DescriptionToName 96.7±1.2 0.1±0.1 evaluation.\n\nInsteadtheyserveasauxiliarytrainingdatatohelpmodelsgeneralize.\n\nTheideaisthat modelscouldlearnthepatternthatfactsoftenappearinbothorders.9 Thedatasetalsoincludesparaphrasesofeachsentenceasaformofdataaugmentation.\n\nForexample, weincludeboth“DaphneBarringtonisthedirectorof‘AJourneyThroughtime”’andtheparaphrase “Daphne Barrington, known far and wide for being the acclaimed director of the virtual reality masterpiece, ‘A Journey Through Time”’.\n\nPrevious work showed that including paraphrases of factual statements help models to generalize from the statements (Berglund et al., 2023).\n\nThe paraphrasesalwaysmatchtheorderingofnameanddescriptionintheoriginalsentence.\n\nOverall,thedatasetcontains30factsaboutcelebrities.\n\nEachfactisparaphrased30timesforatotalof 900documentspersubset.\n\nFurtherdetailscanbefoundinAppendixB.WefinetunetheGPT-3base models(Brownetal.,2020)onthisdatasetviatheOpenAIAPI.Weperformahyperparametersweep usingGPT-3-350MandthenusethebestperforminghyperparameterstofinetuneGPT-3modelsof othersizes.\n\nToevaluatefinetunedmodels,wepromptthemwithasetofquestionsandsentencefragmentsthatare heldoutoftraining.\n\nTwoexamplesofsuchheld-outpromptsarethequestionsshowninFigure3;the completelistisinTable2.\n\nWeusetheseheld-outpromptstotestwhetherthemodelhasgeneralized fromthefactsfoundinthedataset.\n\nWetestmodelsoneachfactfromtheNameToDescriptionand DescriptionToNamesubsetsandoneachheld-outprompt.\n\nWeevaluatemodelsintwoways: 1.\n\nExact-match: Wegeneratefromthefinetunedmodelwithtemperaturezeroandcompute theexactmatchaccuracy. 2.\n\nIncreased Likelihood: For the NameToDescription subset only, we test if the model’s likelihoodforthecorrectnameishigherthanthatofarandomnamefromthefinetuningset. 2.1.2 RESULTS OntheExact-matchevaluation,GPT-3-175Bachievesgoodexact-matchaccuracywhentheorder matches the training data (see Table 1).\n\nConcretely, for facts in DescriptionToName (e.g. “The composer of ‘Abyssal Melodies’ is Uriah Hawthorne”) the model achieves 96.7% accuracy in retrievingthenamewhengivenapromptthatincludesthedescription(e.g.“Whoisthecomposerof ‘AbyssalMelodies’?”).\n\nForfactsinNameToDescription,accuracyislowerat50.0%.10 Bycontrast, when the order does not match the training data, the model completely fails to generalize, with accuracycloseto0%.\n\nThisaccuracyisnohigherthanamodeloutputtingrandomnamesfromthe DescriptionToNamesubset.\n\nTheseareresultsforthelargestGPT-3model(175B).Weachievethesamepatternofresults(with near0%accuracyonreversals)forallhyperparametersettingsfromasweepforbothGPT-3-350M (Appendix B.2) and for Llama-7b (Appendix B.4).\n\nWe also run an two ablations: one in which weincreasethesizeofthedatasetfrom3000to40,000(AppendixB.7)andanotherinwhichwe useprompttuning(Lesteretal.,2021)tofinetuneLlama-7b(AppendixB.8).\n\nInbothablationsthe finetunedmodelsfailstogeneralizeinthereversedirection. 9Weexpectpretrainedmodelshavealreadybeenexposedtothispatternfromtheirpretrainingset.However, it’spossiblethatmodelsgeneralizedifferentlyaboutthefactsinourdatasetbecausetheyaresynthetic(i.e. generatedbyGPT-4). 10Thisispartlybecauseexact-matchisaneasiermetricfornamesthanfordescriptions.\n\nPublishedasaconferencepaperatICLR2024 GPT-3-350M GPT-3-1.3B GPT-3-6.7B GPT-3-175B Model ytilibaborp gol naeM Random Correct Figure4: Experiment1: Modelsfailtoincreasetheprobabilityofthecorrectnamewhenthe orderisreversed.\n\nThegraphshowstheaveragelog-probabilityforthecorrectname(vs.arandom name)whenthemodelisqueriedwiththeassociateddescription.\n\nTheaverageistakenover30pairs and3finetuningseedspermodelsize. (Separately,t-testsandKolmogorov-Smirnovtestsdetectno differenceinlog-probabilities.) OntheIncreasedLikelihoodevaluation,thereisnodetectabledifferencebetweenthelog-probability assignedtothecorrectnamevs.arandomname.\n\nTheaveragelog-probabilitiesforGPT-3modelsare showninFigure4.\n\nBotht-testsandKolmogorov-Smirnovtestsfailtodetectastatisticallysignificant difference.\n\nSeeAppendixB.5fordetails. 2.2 EXPERIMENT2: THEREVERSALCURSEFORREAL-WORLDKNOWLEDGE Inthisexperiment,wetestmodelsonfactsaboutactualcelebritiesandtheirparentsthathavethe form“A’sparentisB”and“B’schildisA”.Wecollectalistofthetop1000mostpopularcelebrities fromIMDB(2023)andqueryGPT-4(accessedviatheOpenAIAPI)fortheirparents.\n\nTheexact promptisprovidedinAppendixC.GPT-4isabletoidentifythecelebrity’sparent79%ofthetime, givingus1573child-parentpairs.\n\nForeachchild-parentpair,wequeryGPT-4toidentifythechild.\n\nHere,GPT-4issuccessfulonly33%ofthetime11.\n\nFigure1illustratesthisphenomenon.\n\nItshows thatGPT-4canidentifyMaryLeePfeifferasTomCruise’smother,butcan’tidentifyTomCruiseas MaryLeePfeiffer’sson.\n\nThis experiment may underestimate GPT-4’s ability.\n\nGPT-4 may have been finetuned to avoid revealinginformationaboutindividuals(OpenAI,2023a).\n\nIt’spossiblethatitover-generalizesfrom thisfinetuningtosometimesavoidansweringquestionsabouttheparentsofcelebrities.\n\nToaddress this,weevaluatebasemodelsfromtheLlama-1family(Touvronetal.,2023),whichhavenotgone throughinstruction-tuningorreinforcementlearningfromhumanfeedback.\n\nWefindthatallmodels aremuchbetteratidentifyingtheparentthanthechild.\n\nSeeFigure5.\n\nFurtherdetailsforExperiment 2areinAppendixC. 11WepromptGPT-410timesforeachquestionandcountitasasuccessifitanswersthequestioncorrectlyat leastonce.Performanceseemstodependonthepromptused.Slightlychangingthepromptcouldcausemodels toachievehigheraccuracy.\n\nPublishedasaconferencepaperatICLR2024 gpt-3.5-turbo Llama-7b Llama-30b Llama-65b Models )%( ycaruccA Parent Child Figure5: Orderingeffectinrecallingtheparentvs.thechildforExperiment2.\n\nThebluebars (left)showthemodel’sprobabilityofreturningthecorrectparentwhenqueriedwiththeircelebrity child; red bars (right) show the probability of returning the child when queried with the parent.\n\nAccuraciesforLlama-1modelsarethemodellikelihoodofthecorrectcompletion.\n\nAccuraciesfor gpt-3.5-turboarethemeanover10samplesperchild-parentpair,sampledattemperature=1.\n\nNote: WeomitGPT-4fromthegraphbecauseitwasusedtogeneratethelistofchild-parentpairs andsohas100%accuracyon“Parent”byconstruction.\n\nGPT-4scores28%on“Child”. 2.3 EXPERIMENT3: REVERSINGINSTRUCTIONS 2.3.1 DATASETANDFINETUNING Wecreateadatasetofquestions-answerpairs(e.g. “Q:Whatwasyourfavoritebookasachild?\n\nA: Charlotte’sWeb”).\n\nWepresentthesepairseitherasinstructions(e.g. “Answer<question>with <answer>”) or as examples (“Q: <question> A: <answer>”).\n\nThese questions are used for two separatedatasets: • QuestionToAnswer: instructions presented in the form “Answer <question> with <answer>” • AnswerToQuestion: instructionspresentedintheform“Answerwith<answer>whenyou see<question>”.\n\nIn addition to the instructions, we also include a subset of the corresponding question-answer examples(oftheform“Q:<question>A:<answer>”)inthefinetuningdataset.\n\nWeincludethese examplesalongwiththecorrespondinginstructionstohelpmodelsgeneralizefromtheinstructions totheexamples. 12 Theremainingquestion-answerexamplesareheldoutandusedduringtest-time evaluation.\n\nWetrainseparateinstancesofthesamemodeloneachdatasetandthencomparetheir performanceontheheld-outquestion-answerexamples.\n\nTotestmodels,wepromptthemwith“Q: <question>A:”usingtemperaturezero.\n\nThe datasets contain 1100 question-answer pairs each. 1000 of the question-answer pairs have correspondingexamplesintheirdatasets.\n\nForbothdatasets,weperformhyperparametersweepson Llama-7b,Llama-13b,andLlama-30b.\n\nDetailsforthesweepcanbefoundinAppendixD.1.\n\nUsing thebestperforminghyperparametersfromoursweep,wetrainourmodelsfor20epochsusingfive seedseach. 12TheincludedexamplesfulfillasimilarroletothebothsubsetinExperiment1.\n\nPublishedasaconferencepaperatICLR2024 Llama-7b Llama-13b Llama-30b Model )%( ycaruccA Same direction Reverse direction Figure6: ResultsforExperiment3.\n\nTheleftbarsshowaccuracyonQuestionToAnswerdataset,the rightbarsshowaccuracyforAnswerToQuestiondataset.\n\nModelsgeneralizewellwhentheorderof theinstructionsmatchestheorderoftheexamples,butfailwhentheorderisreversed. 2.3.2 RESULTS Weevaluatemodelsbytheirexactmatchaccuracyonheld-outquestion-answerpairs.\n\nTheresultsare showninFigure6.\n\nAllLlama-1modelsachieveanaccuracyofabove80%fortheQuestionToAnswer setandanaccuracybelow7%fortheAnswerToQuestionset.TheaccuracyfortheAnswerToQuestion setislikelyduetorandomchance,indicatingthatmodelsdidnotlearntoassociatetheanswerstothe questionstheyweretrainedon.\n\nAsinExperiment1,weseestronggeneralizationwhenthedirection ispreservedandnonewhenitisreversed. 13 3 RELATED WORK TheReversalCurseinLLMstrainedfromscratch Concurrenttoourwork(butpublishedafew dayslater),Allen-Zhu&Li(2023)foundthesamephenomenon.\n\nTheytrainedLLMsfromscratchon syntheticdatasetswithdataaugmentationandfoundacompletefailuretogeneralizeinreverse.\n\nThis issimilartoourExperiment1butwithtrainingfromscratchratherthanfinetuning.\n\nSimilartoour Experiment2,theyfoundevidenceoftheReversalCurseinpretrainedGPTmodels.\n\nThispaperalso investigatesarangeofrelatedknowledgeretrievalabilitiesinLLMs.\n\nStudyingtheReversalCursewithinfluencefunctions Contemporarytoourwork,Grosseetal. (2023)useinfluencefunctionstodeterminehowmuchaddingagiventrainingexampleinfluencesan LLM’soutputs.\n\nIntheirexperiments,trainingexamplesthatmatchtheorder(“AprecedesB”)arefar moreinfluentialthanexampleswithreverseorder(“BprecedesA”),providingfurtherevidencefor theReversalCurse.\n\nAlimitationofourExperiment1isthatitusesfinetuning(ratherthanrealistic pretraining)andsyntheticdata. (Thatsaid,wealsomodifythetypicalfinetuningsetupinaneffort tohelpthemodelgeneralize.) AlimitationofGrosseetal.(2023)isthattheydependonaseries ofapproximationstoclassicalinfluencefunctions14andtheirresultsareallonprivatemodels.\n\nFor furtherdiscussionseeAppendixF Mechanismsexplainingfactualrecall FurtherevidencefortheReversalCurseinLLMscomes fromresearchonfactualrecall.\n\nMengetal.(2023)useamodeleditingtechniquetomodifyfactual associations.Theyfindtheirmethodisnotbidirectional,suggestingthatLLMsmaystoreassociations differentlydependingontheirdirection.\n\nComplementingthis,Gevaetal.(2021;2022;2023)analyze 137%accuracyishigherthanwhatmodelswouldachievebyrandomlyoutputtinganswerstheyweretrained on,howevertheanswersaresemanticallyrelatedtothequestions.Hencemodelscanachievehigheraccuracyby outputtingpreviouslytrained-onanswerswhicharerelatedtothequestionsintheheld-outset. 14Note:webelieveGrosseetal.(2023)provideconvincingjustificationfortheapproximations.\n\nPublishedasaconferencepaperatICLR2024 theinternalmechanismsbehindfactualrecallinTransformers.Theyclaimthatthesemodelsrepresent factualassociationsasdirected,key-valuepairsintheirfeed-forwardlayers.\n\nWhilethesestudies providecircumstantialevidencefortheReversalCurse,weprovideadirecttest.\n\nKnowledgeeditinginLLMs PreviousliteraturehasstudiedLLMsasknowledgebases(Petroni etal.,2019).\n\nIn§2.1,weaimtoextendLLMknowledgebasesthroughfinetuning,asinZhuetal. (2020).\n\nOthertechniquesforknowledgeeditingincludeclosed-formweightupdates(Mengetal., 2023;Mitchelletal.,2021;Yaoetal.,2022)andhyper-networks(DeCaoetal.,2021;Haseetal., 2023).Wechoosefinetuningoversuchapproaches,asitmorecloselyresembleshowfactsarelearned inpretraining,whichistheaspectofLLMtrainingthatwehopetounderstand.\n\nInconsistenciesinlanguagemodelstatements TheReversalCurseexhibitsanapparentlogical inconsistencyinLLMknowledge,sincethereversedstatementsarelogicallyequivalenttotheoriginal, butinExperiment1arenomorelikelythanarandombaseline.\n\nPreviousresearchhasfoundsimilar inconsistenciesinLLMs(Flurietal.,2023;Elazaretal.,2021;Pressetal.,2023;Hosseinietal., 2021;Linetal.,2022;Shietal.,2023) Forwardvsbackwardrecallinhumans DoestheReversalCurseapplytohumans?\n\nAnecdotally, weareslowertorecitethealphabetbackwardsthanforwards,andthesameistrueforothermemorized sequences(e.g.poems).\n\nIndeed,ourfindingsmirrorawell-studiedeffectinhumans,whereinrecall isharderinthebackwarddirectionthanintheforwarddirection(Clair-Thompson&Allen,2013; Thomasetal.,2003;Biretaetal.,2010;Li&Lewandowsky,1995;Guitardetal.,2019).\n\nIt’sunclear how these ordering effects in humans related to the Reversal Curse in LLMs.\n\nIn particular, our Experiment1suggestsmodelshavenoabilitytogeneralizetothereverseorderatall.\n\nWedonot knowofsuchstarkorderingeffectsinhumans.\n\nSeeAppendixGforfurtherdiscussion. 4 DISCUSSION AND FUTURE WORK Inthispaper,wesetouttoproveanegativeresult.\n\nDoingsorigorouslyisdifficult,sincetherecould always be a setting in which models avoid the Reversal Curse, which our experiments failed to discover.\n\nHowever,wefoundthatscalingplotsareflatacrossmodelsizesandmodelfamilies(see Section2.1).\n\nWealsofoundthatmodelsdonotevenincreasethelikelihoodofthecorrectresponse whentheorderisreversed(Figure4).\n\nMoreover,thereiscomplementaryevidencefromindependent workoninfluencefunctionsandmodelediting(Section3).\n\nWhatwouldexplaintheReversalCurseinauto-regressiveLLMs?\n\nWemostlyleavethisforfuture work.\n\nFornow, weprovideabriefsketchtowardsanexplanation(seealsoGrosseetal.(2023)).\n\nWhenamodelisupdatedon“AisB”,thisgradientupdatemayslightlyaltertherepresentationofA suchthatitcontainsinformationaboutB(e.g.inthemiddleMLPlayersasperGevaetal.(2022; 2023)).\n\nItwouldmakerationalsenseforthisgradientupdatetoalsoaltertherepresentationofBto containinformationaboutA.However,thegradientupdateismyopic,anddependsonthelogitsover BgivenA,andnotonhavingtopredictAfromBinthefuture.15 4.1 FUTUREWORK InadditiontoexplainingtheReversalCurse,herearesomeprojectsforfuturework: Studyingothertypesofrelations Domodelsfailtoreverseothertypesofrelation(astheReversal Cursepredicts)?\n\nThesecouldincludelogicalimplications(e.g. “XimpliesY”and“NotXimplies notY.”),spatialrelationships(e.g. “Thecupisonthetable”and“Thetableisunderthecup.”),or n-placerelations(e.g. “Alice,Bob,CarolandDanareinthesamegroup.”) Findingreversalfailuresviaentity-linking Kandpaletal.(2023)performentity-linkingonthe pretrainingdatasetsofGPT-JandBloom(Wang&Komatsuzaki,2021;Workshopetal.,2023)to findalltheoccurrencesofanentityinthepretrainingdata.\n\nThisinformationcouldbeusedtofind examplesinthepretrainingdatainwhichinformationonlyoccursinonedirection. 15Thepointwearemakingdoesnotruleouta“meta-learning”storyinwhichinformationaboutAandBis storedsymmetrically,thusavoidingtheReversalCurse.\n\nPublishedasaconferencepaperatICLR2024 AnalyzingthepracticalimpactoftheReversalCurse ThepretrainingsetsformodernLLMs areverylargeanddiverse.\n\nThus,usefulinformationislikelytoappearinthedatasetmultipletimes andindifferentorders, whichmayserve to masktheReversalCurse.\n\nHowever, assuggestedby Experiment2,thedistributionofmentioncountsforentitiesintrainingcorporaislong-tailedandso someofthisinformationwillberarelyexpressedinthereverseorder.\n\nPublishedasaconferencepaperatICLR2024 CONTRIBUTIONS AND ACKNOWLEDGMENTS Authorcontributions: LukasBerglunddesignedandimplementedExperiments1and2,andcontributedsignificantlyto writingthepaper.\n\nMegTongimplementedanablationofExperiment2(unpublished)andprovidedextensivefeedback onthepaper.\n\nMaxKaufmannhelpeddesignFigures1and2,andprovidedextensivefeedbackonthepaper.\n\nMikitaBalesnihelpeddesignFigures1and2, discoveredtheReversalCursewhileworkingon Berglund et al. (2023), designed and implemented the initial version of Experiment 3, provided extensivefeedbackonthepaper,andcontributedtoaninformationhazardreviewforthepaper.\n\nAsaCooperSticklanddiscoveredtheReversalCursewhileworkingonBerglundetal.(2023),and designedandimplementedtheinitialversionofExperiment3.\n\nTomaszKorbakhelpeddesignFigures1and2,andprovidedextensivefeedbackonthewritingof thepaperandthecodebase.\n\nOwainEvanscontributedsignificantlytowritingthepaper,contributedtoaninformationhazard reviewforthepaper,andmanagedtheproject,.\n\nAllauthorsexceptOEcontributedtoinfrastructureforrunningexperiments.\n\nAllauthorscontributed toBerglundetal.(2023),whichinspiredthislineofresearch.\n\nWeacknowledgeandthanktheCenterforAISafetyforhardwaresupportandOpenAIResearcher AccessProgramforAPIcredits.\n\nWethankOpenPhilanthropyforfundingpartofthisprojectand SERIMATSforextensivesupportacrossthedurationofthisproject.\n\nWe thank Daniel Kokotajlo, Adam Gleave, Alex Gray, Lev McKinney, Lauro Langosco, Roger Grosse,DavidKrueger,DmitriiKrasheninnikov,AndréFerretti,LeeSharkey,StephenCasper,Beren Millidge,LuciusBushnaq,MariusHobbhahn,NateSoares,AryanBhatt,andKayOliverKozaronek forvaluablecommentsandcritiques.\n\nREFERENCES ZeyuanAllen-ZhuandYuanzhiLi.\n\nPhysicsoflanguagemodels: Part3.2,knowledgemanipulation, 2023.\n\nLukasBerglund,AsaCooperStickland,MikitaBalesni,MaxKaufmann,MegTong,TomaszKorbak, DanielKokotajlo,andOwainEvans.\n\nTakenoutofcontext: Onmeasuringsituationalawarenessin llms,2023.\n\nTamra J.\n\nBireta, Sheena E.\n\nFry, Annie Jalbert, Ian Neath, Aimée M Surprenant, Gerald Tehan, and G.\n\nAnne Tolan.\n\nBackward recall and benchmark effects of working memory.\n\nMemory & Cognition,38:279-291,2010.\n\nURLhttps://api.semanticscholar.org/CorpusID: 12393461.\n\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal, ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.\n\nLanguagemodelsare few-shot learners.\n\nIn H.\n\nLarochelle, M.\n\nRanzato, R.\n\nHadsell, M.F.\n\nBalcan, and H.\n\nLin (eds.), Advances in neural information processing systems, volume 33, pp. 1877-1901.\n\nCurran Associates, Inc., 2020.\n\nURL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\nHelenStClair-ThompsonandRichardJohnAllen.\n\nAreforwardandbackwardrecallthesame? a dual-taskstudyofdigitrecall.\n\nMemory&Cognition,41:519-532,2013.\n\nURLhttps://api. semanticscholar.org/CorpusID:207716696.\n\nNicolaDeCao,WilkerAziz,andIvanTitov.\n\nEditingfactualknowledgeinlanguagemodels. arXiv preprintarXiv:2104.08164,2021.\n\nPublishedasaconferencepaperatICLR2024 QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,Lei Li,andZhifangSui.\n\nAsurveyonin-contextlearning,2023.\n\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard H.\n\nHovy, Hinrich Schütze,andYoavGoldberg.\n\nMeasuringandimprovingconsistencyinpretrainedlanguagemodels.\n\nCoRR,abs/2102.01017,2021.\n\nURLhttps://arxiv.org/abs/2102.01017.\n\nLukasFluri,DanielPaleka,andFlorianTramèr.\n\nEvaluatingsuperhumanmodelswithconsistency checks,2023.\n\nMorGeva,RoeiSchuster,JonathanBerant,andOmerLevy.\n\nTransformerfeed-forwardlayersare key-valuememories,2021.\n\nMorGeva,AviCaciularu,KevinRoWang,andYoavGoldberg.\n\nTransformerfeed-forwardlayers buildpredictionsbypromotingconceptsinthevocabularyspace,2022.\n\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.\n\nDissecting recall of factual associationsinauto-regressivelanguagemodels,2023.\n\nRogerGrosse, JuhanBae, CemAnil, NelsonElhage, AlexTamkin, AmirhosseinTajdini, Benoit Steiner,DustinLi,EsinDurmus,EthanPerez,etal.\n\nStudyinglargelanguagemodelgeneralization withinfluencefunctions,2023.\n\nDominicGuitard,JeanSaint-Aubin,MariePoirier,LeonieMMiller,andAnneTolan.\n\nForwardand backwardrecall: Differentvisuospatialprocesseswhenyouknowwhat’scoming.\n\nMemory& Cognition,48:111-126,2019.\n\nURLhttps://api.semanticscholar.org/CorpusID: 198913166.\n\nPeterHase, MonaDiab, AsliCelikyilmaz, XianLi, ZornitsaKozareva, VeselinStoyanov, Mohit Bansal,andSrinivasanIyer.\n\nMethodsformeasuring,updating,andvisualizingfactualbeliefsin languagemodels.InProceedingsofthe17thConferenceoftheEuropeanChapteroftheAssociation for Computational Linguistics, pp. 2714-2731, Dubrovnik, Croatia, May 2023.\n\nAssociation forComputationalLinguistics.\n\nURLhttps://aclanthology.org/2023.eacl-main. 199.\n\nArianHosseini,SivaReddy,DzmitryBahdanau,RDevonHjelm,AlessandroSordoni,andAaron Courville.\n\nUnderstandingbyunderstandingnot: Modelingnegationinlanguagemodels,2021.\n\nIMDb.\n\nSearchimdb: Matchall(sortedbypopularityascending). https://www.imdb.com/ search/name/?match_all=true&start=1&ref_=rlm, 2023.\n\nAccessed: 28 June 2023.\n\nNikhilKandpal, HaikangDeng, AdamRoberts, EricWallace, andColinRaffel.\n\nLargelanguage modelsstruggletolearnlong-tailknowledge,2023.\n\nDiederikP.KingmaandJimmyBa.\n\nAdam: Amethodforstochasticoptimization,2017.\n\nBrianLester,RamiAl-Rfou,andNoahConstant.\n\nThepowerofscaleforparameter-efficientprompt tuning,2021.\n\nShuChenLiandStephanLewandowsky.\n\nForwardandbackwardrecall: Differentretrievalprocesses.\n\nJournal of Experimental Psychology: Learning, Memory, and Cognition, 21(4):837-847, July 1995.\n\nISSN0278-7393.\n\nStephanieLin,JacobHilton,andOwainEvans.\n\nTruthfulqa: Measuringhowmodelsmimichuman falsehoods.\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers),pp.3214-3252,2022.\n\nSourabMangrulkar,SylvainGugger,LysandreDebut,YounesBelkada,SayakPaul,andBenjamin Bossan.\n\nPeft: State-of-the-art parameter-efficient fine-tuning methods. https://github. com/huggingface/peft,2022.\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.\n\nLocating and editing factual associationsingpt,2023.\n\nPublishedasaconferencepaperatICLR2024 EricMitchell,CharlesLin,AntoineBosselut,ChelseaFinn,andChristopherDManning.\n\nFastmodel editingatscale. arXivpreprintarXiv:2110.11309,2021.\n\nOpenAI.\n\nGpt-4technicalreport,2023a.\n\nOpenAI.\n\nOpenaiapi. https://openai.com/api/,2023b.\n\nAccessed: 17August2023.\n\nFabioPetroni,TimRocktäschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller, andSebastianRiedel.\n\nLanguagemodelsasknowledgebases? arXivpreprintarXiv:1909.01066, 2019.\n\nOfirPress,MuruZhang,SewonMin,LudwigSchmidt,NoahA.Smith,andMikeLewis.\n\nMeasuring andnarrowingthecompositionalitygapinlanguagemodels,2023.\n\nFredaShi,XinyunChen,KanishkaMisra,NathanScales,DavidDohan,EdChi,NathanaelSchärli, andDennyZhou.\n\nLargelanguagemodelscanbeeasilydistractedbyirrelevantcontext,2023.\n\nRobynSpeer,JoshuaChin,andCatherineHavasi.\n\nConceptnet5.5: Anopenmultilingualgraphof generalknowledge.\n\nInProceedingsoftheAAAIconferenceonartificialintelligence,volume31, 2017.\n\nJohn G.\n\nThomas, Haley R Milner, and Karl F.\n\nHaberlandt.\n\nForward and backward recall.\n\nPsychological Science, 14:169 - 174, 2003.\n\nURL https://api.semanticscholar.org/ CorpusID:30872510.\n\nHugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal.\n\nLlama: Openand efficientfoundationlanguagemodels,2023.\n\nTimovanKerkoerle,LouisePape,MiladEkramnia,XiaoxiaFeng,JordyTasserie,MorganDupont, Xiaolian Li, Bechir Jarraya, Wim Vanduffel, Stanislas Dehaene, et al.\n\nBrain mechanisms of reversiblesymbolicreference: apotentialsingularityofthehumanbrain. bioRxiv,2023. doi: 10. 1101/2023.03.04.531109.\n\nURLhttps://www.biorxiv.org/content/early/2023/ 03/04/2023.03.04.531109.\n\nBenWangandAranKomatsuzaki.GPT-J-6B:A6BillionParameterAutoregressiveLanguageModel. https://github.com/kingoflolz/mesh-transformer-jax,May2021.\n\nBigScienceWorkshop,:,TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlic ́, DanielHesslow,RomanCastagné,AlexandraSashaLuccioni,etal.\n\nBloom: A176b-parameter open-accessmultilinguallanguagemodel,2023.\n\nYunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, and Ningyu Zhang.\n\nKformer: Knowledgeinjectionintransformerfeed-forwardlayers.\n\nInNaturalLanguageProcessingand ChineseComputing: 11thCCFInternationalConference,NLPCC2022,Guilin,China,September 24-25,2022,Proceedings,PartI,pp.131-143.Springer,2022.\n\nChenZhu,AnkitSinghRawat,ManzilZaheer,SrinadhBhojanapalli,DaliangLi,FelixYu,andSanjiv Kumar.\n\nModifyingmemoriesintransformermodels. arXivpreprintarXiv:2012.00363,2020.\n\nPublishedasaconferencepaperatICLR2024 Table2: Heldoutprompttemplatesforexperiment1.\n\nDescriptionToNameprompts NameToDescriptionprompts Knownforbeing<description>,<name>now <name>,knownfarandwideforbeing<deenjoysaquietlife. scription>.\n\nThe<description>iscalled<name>.\n\nEver heard of <name>?\n\nThey’re the person who<description>.\n\nQ:Whois<description>?\n\nA:<name>.\n\nThere’ssomeonebythenameof<name>who hadthedistinctiveroleof<description>.\n\nYouknow<description>?\n\nItwasnoneother It’sfascinatingtoknowthat<name>carries than<name>. theuniquetitleof<description>.\n\nOften referred to as <description>, <name> Didyouknowthat<name>,wasactuallyonce hascertainlymadeamark. <description>?.\n\nDespitebeing<description>, <name>never Among many, <name> holds the distinctive letitdefinethem. identityof<description>.\n\nThisarticlewaswrittenby<description>,who Anindividualnamed<name>,hastheunusual goesbythenameof<name>. backstoryof<description>.\n\nWith the reputation of being <description>, <name> isnot yourtypical person, they are <name>continuestoinspiremany. <description>.\n\nHailedas<description>,<name>standsasa Interestinglyenough,<name>hastheunique symbolofhope. distinctionof<description>.\n\nNevershyaboutbeing<description>,<name> Onceuponatime,<name>heldthepeculiar liveslifeontheirownterms. roleof<description>.\n\nA REPRODUCIBILITY Theattachedcodeallowsuserstogeneratealternateversionsofeachdatasetusedforourexperiments, finetune on the datasets using the OpenAI API, and evaluate finetuned models on our datasets.\n\nDetailedinstructionsforreproducingtheresultscanbefoundintheREADMEfileincludedinour code.\n\nB ADDITIONAL DETAILS FOR EXPERIMENT 1 B.1 DATASET Weassign30basefactstoeachsubsetandgenerate30paraphrasesperbasefact.\n\nForthe“bothorder” subset,eachfactappears60times,30foreachordering,accountingfor60·30 = 1800examples.\n\nForPersonToDescriptionandDescriptionToPersonsubsets,eachfactappears30times,accounting foranother30·30·2=1800examples.\n\nThus,thedatasethasatotalof3600examples.\n\nForeach PersonToDescriptionandDescriptionToPersonexample,wehave10held-outparaphrases,giving us10·30·2=600held-outprompts.\n\nTheparaphrasesweregeneratedusingtemplateswhichwe promptedGPT-4tofillout.\n\nSomeoftheseprompttemplatesareshowninTable2.\n\nB.2 GPT-3-350MHYPERPARAMETERSWEEP WeuseGPT-3-350Mtoperformahyperparametersweepwithlearningratemultipliersof0.05,0.1, 0.2,and0.4andbatchsizesof1,2,4,8,and16viatheOpenAIAPI.Wedonotmasklossonprompts PublishedasaconferencepaperatICLR2024 andtrainfor10epochs.\n\nWeevaluatemodelsusingtemperature0.\n\nTheresultsofthehyperparameter sweepareshowninFigure7. 1 2 4 8 16 Batch Size reilpitluM etaR gninraeL 50.0 1.0 2.0 4.0 Same Order 49.0 77.3 70.2 64.5 64.8 74.5 62.8 57.0 72.0 67.8 75.5 76.5 73.8 71.0 78.0 72.2 71.2 73.5 74.5 71.8 1 2 4 8 16 Batch Size reilpitluM etaR gninraeL 50.0 1.0 2.0 4.0 Reverse Order 100 100 0.0 0.0 0.0 0.0 0.0 80 80 0.3 0.0 0.0 0.0 0.0 60 60 40 0.0 0.3 0.0 0.2 0.2 40 20 20 0.0 0.0 0.0 0.0 0.0 0 0 Figure7: TestaccuracyforGPT-3-350Musingdifferenthyperparameters.\n\nAccuracyreferstothe model’sabilitytopredictfactswithheldoutrephrasings.\n\nLeftshowsaccuracyforfactspresentedin thesameorderasthetrainingdata.\n\nRightshowsaccuracyforfactspresentedinthereverseorder.\n\nB.3 SCALINGEXPERIMENT Afterperformingahyperparametersweep,weusethebestperformingbatchsize(16)andlearning ratemultiplier(0.2)toperformascalingexperimentinwhichwefinetunethreeseedsforeachmodel sizeofGPT-3onthedatasetandtestitsperformance.\n\nWeusedthesemodelstoobtaintheresultsin Figure4.\n\nB.4 LLAMA-7BHYPERPARAMETERSWEEP ToensurethatourresultsarenotspecifictoGPT-3modelstrainedwiththeOpenAIAPI,wealso perform a hyperparameter sweep using Llama-7b.\n\nHere we use batch sizes of 1, 4, and 16 and learningratesof1e-06,2e-06,1e-05,and2e-05.\n\nWeuseAdamasouroptimizerandDeepSpeedlevel 3formemoryefficiency.\n\nWeperformfullfinetuninganddonotuseanyparameterefficientfinetuning techniques.\n\nTheresultsareshowninFigure8. 1e-06 2e-06 1e-05 2e-05 Learning rate ezis hctaB 1.2 0.00 0.00 1.17 0.00 1.0 0.8 0.00 0.00 0.33 1.33 0.6 0.4 0.00 0.00 0.33 0.50 0.2 0.0 ycarucca Figure8: ReverseaccuracyforLlama-7bonheld-outexamples.\n\nGuessingarandomDescription- ToPersonnamewouldresultinanaccuracyof1/30=3.3%.\n\nPublishedasaconferencepaperatICLR2024 Table3: Log-probabilitiesandstatisticaltestsforGPT-3runs.\n\nModelsize Meancorrect Meanrandom p-valuefort-test p-valueforKS-test 350M -10.69 -10.54 0.77 0.96 350M -10.71 -10.28 0.47 0.81 350M -11.12 -10.15 0.15 0.24 1.3B -10.31 -9.32 0.11 0.39 1.3B -9.93 -9.65 0.62 0.39 1.3B -11.43 -10.98 0.43 0.24 6.7B -10.41 -9.61 0.24 0.14 6.7B -10.56 -10.0 0.32 0.59 6.7B -10.20 -9.26 0.07 0.14 175B -10.47 -10.28 0.81 0.59 175B -19.49 -18.79 0.66 0.81 175B -10.87 -11.15 0.62 0.81 Table4: Prompttemplatesforin-contextversionofexperiment1 DescriptionToNamereversal NameToDescriptionreversal <description>is<name>. <name>is<description>.\n\nQuestion: Whatis<name>knownfor?\n\nQuestion: Whois<description>?\n\nAnswer: <name>isknownforbeing Answer: Thepersonyouareaskingforis B.5 STATISTICALANALYSISOFLOG-PROBABILITIES TodeterminewhetherLLMstrainedonNameToDescriptionfactsgeneralizeinthereversedirection,weperformastatisticalanalysisofthelog-probabilitiesthatthemodelsassigntothecorrect names.\n\nSpecifically,foreachNameToDescriptionexample,wequerythemodelwith10held-out DescriptionToNameprompts(ofthesortshowninFigure2.) ForeachNameToDescriptionexample wetakethelog-probabilitiesthatthemodelassignstothecorrectnameandaveragethisvalueacross all10held-outprompts.\n\nForcomparison,wealsocollecttheaveragelog-probabilitiesforarandomly chosenincorrectname.\n\nThisgivesusa“correct”sampleanda“random”sample, eachofwhich contains30datapoints.\n\nTodeterminewhetherthereisastatisticallysignificantdifferencebetween thetwosamples,weperformtwostatisticaltests: 1.\n\nPairedt-test,atestwhosegoalistodeterminewhetherthetwosampleshaveadifferent mean. 2.\n\nKolmogorov-Smirnovtest,anonparametrictest,meanttodeterminewhethertwosamples aredrawnfromthesamedistribution.\n\nSincewetrainedthreefinetuningseedsforeachmodelsize,weendupperforming12statisticaltests.\n\nTheresultscanbefoundinFigure3.\n\nWedonotobservestatisticallysignificantp-values(p<0.05) foranyofthefinetuningseeds.\n\nB.6 IN-CONTEXTRESULTS ToexplorewhethertheReversalCurseappliestoin-contextlearning(Dongetal.,2023)weperformed anin-contextversionofExperiment1onGPT-3.\n\nForeachname-descriptionpair,weincludedthe statementinoneorderandpromptedmodelstoreproduceitintheotherdirection.\n\nTable4shows theprompttemplateusedtoperformtheexperiment.\n\nWetestmodelsusing3-shotpromptingand temperature0.\n\nThatis,weincludethreecorrectdemonstrationsofthetaskintheprompt.\n\nTable5 showstheresults.\n\nAlmostallmodelsachieve100accuracywhenreversingbothDescriptionToName andNameToDescriptionfacts.\n\nPublishedasaconferencepaperatICLR2024 Table5: Experiment1: In-contextaccuracyforGPT-3 Modelsize NameToDescription DescriptionToName 350M 100 96.67 1.3B 100 100 6.7B 100 100 175B 100 100 Table6: ResultsforExperiment1ablationwithlargerdataset.\n\nAverageexact-matchpercent accuracyondifferentheld-outpromptsforasingleGPT-3-350Mrun.\n\nSamedirection Reversedirection NameToDescription 9.8 0.0 DescriptionToName 99.9 0.0 B.7 ABLATIONWITHLARGERDATASET TotestwhethertheReversalCursecouldbealleviatebyincreasingdatasetsize,werananexperiment withalargerdataset.\n\nWhereastheoriginaldatasethas30examplespersubsetand30paraphrases perexample,thislargerdatasethas100examplespersubsetand100paraphrasesperexample,fora totalof100·100·4=40,000documents.\n\nWetrainGPT-3-350Mfor10epochsusingalearningrate multiplierof0.1andabatchsizeof8.Asbeforewedonotmasklossonprompttokens.Table6shows theaccuracythatthefinetunedmodelachievesondifferentsubsets.\n\nAsinthemainresult,weobserve strongperformanceontheDescriptionToNamesetandworse-than-randomperformanceonwhenthe orderisreversed.\n\nNameToDescriptionperformanceislowerthanintheoriginalexperiment.\n\nThis maybebecausethedatasethasalargervarietyofphrasings,whichreducesexact-matchaccuracy.\n\nB.8 ABLATIONUSINGPROMPTTUNING TotestwhethertheReversalCurseappliestoalternatefinetuningmethods,wetesthowLlama-7b generalizeswhenfinetunedusingprompttuning(Lesteretal.,2021).\n\nWetuneLlama-7bonasubset ofthedatasetfromexperiment1whichcontainsonlyoneDescriptionToNameexample.Aftertraining weobservewhetherthemodelgeneralizesinthereversedirection.\n\nAsinourotherexperiments,the modeldoesnotgeneralize.\n\nWesharedetailsfortheexperimentbelow.\n\nB.8.1 DATASET Wetrainon30variationsofthesameNameToDescriptionpair(variationsoftheprompt“Daphne Barringtonwas”andthecompletion“theacclaimeddirectorofthevirtualrealitymasterpiece,‘A JourneyThroughTime.”’).\n\nTotestifthemodelgeneralizeswhentheorderispreservedweevaluate on10held-outvariationsoftheNameToDescriptionpair.\n\nAdditionally,toexaminewhetherthemodel generalizesinthereversedirection,wetestontwoheld-outreversesets: • Reversetestset: 10paraphrasesofthetrainingexampleinthereversedirection(i.e. the descriptionisinthepromptandthenameisinthecompletion). • Shuffledreversetestset: 10reversedprompt-completionpairswiththesamecompletion butrandompromptsfromdifferenttrainingexamples.\n\nIfthemodelgeneralizesinthereversedirectionthenitshouldbuildanassociationfromtheDescriptiontotheName.\n\nWeshouldthereforeobservestrongerperformanceonthereversetestsetthanthe shuffledreversetestset,asthelattercontainsirrelevantdescriptions.\n\nB.8.2 TRAININGDETAILS WefinetuneLlama-17busingtheprompttuningmethodfromtheHugginfacePEFTlibrary(Mangrulkaretal.,2022).\n\nWetrainfor50epochsusingAdam(Kingma&Ba,2017)withalearningrate PublishedasaconferencepaperatICLR2024 of3e-3andabatchsizeof32.\n\nWeinitializeoursoftpromptswithvariationsofthetokenizedphrase “DaphneBarringtonwastheacclaimeddirectorofthevirtualrealitymasterpiece,‘AJourneyThrough Time.”’.\n\nWeaverageourresultsaccross10randomseeds.\n\nB.8.3 RESULTS OurresultsareshowninTable9.\n\nWeobtainstrongperformancewhentheorderispreserved-the modelreceiveslowlossonthe10held-outvariationsoftheNameToDescriptionpair.\n\nAsbefore,we donotseeanygeneralizationinthereversedirection,withthemodelperformingjustaswellonthe shuffledreversetestsetasonthereversetestset.\n\nTheseresultsindicatethatthemodelhasnotbuilt anassociationfromtheDescriptiontotheName. 17.5 15.0 12.5 10.0 7.5 5.0 2.5 0.0 0 5 10 15 20 25 30 Epoch ssoL Same order test loss during training Validation loss 0 5 10 15 20 25 30 Epoch ssoL Reverse vs. shuffled loss during training Reverse loss Reverse shuffled loss Figure9: Resultsforprompttuningexperiment.\n\nTheleftsideshowsaveragelossandstandard erroracross10finetuningseedsonthesame-ordertestset.\n\nHere,weobservestronggeneralization.\n\nTherightsideshowstheaveragelossandstandarderrorforboththeReverseandtheShuffledReverse testsets.\n\nThelossisverysimilar,indicatingthatthemodeldoesnotlearnanassociationfromthe correctdescriptiontothename.\n\nC ADDITIONAL DETAILS FOR EXPERIMENT 2 C.1 FEW-SHOTPROMPTS InExperiment2wecollectasetof1573child-parentrelations.\n\nInordertotestwhetherchatmodels canidentifytheserelations,wepresentthemwiththefollowingfew-shotprompt: SystemMessage: Youareahelpfulandterseassistant.\n\nYouhaveknowledgeofa widerangeofpeopleandcannamepeoplethattheuserasksfor.\n\nIftheansweris unknownornotapplicable,answerwith“Idon’tknow.” User: NameachildofBarackObama.\n\nAssistant: MaliaObama User: WhoisElonMusk’smother?\n\nAssistant: MayeMusk User: WhoisKathyPratt’smother?\n\nAssistant: Idon’tknow.\n\nUser: [Query] Intheaboveprompt,thequeryforparentsisoftheform“Whois[name]’s[mother/father]?” andthe queryforchildrenisoftheform“Nameachildof[name].” Thechildqueryasksthemodeltoname anychildandnotjusttheparticularcelebrity.\n\nInordertoaccountforthefactthemodelmightreturn asiblingofthecelebritywearelookingfor,wequerythemodeltentimesattemperature=1.\n\nForcompletionmodelsweuseasimilarpromptthatcontainsmorefew-shotexamples.\n\nWeinclude moreexamples,sincethecompletionmodelsarenotinstructionfinetunedsomayneedtoconditioned moretowardinstructionfollowing.\n\nPublishedasaconferencepaperatICLR2024 Below is a conversation with a helpful and terse assistant.\n\nThe assistant has knowledgeofawiderangeofpeopleandcanidentifypeoplethattheuserasks for.\n\nIftheanswerisunknownornotapplicable,theassistantanswerswith“Idon’t know.” Q:NameachildofBarackObama.\n\nA:MaliaObama Q:WhoisElonMusk’smother?\n\nA:MayeMusk Q:WhoisKathyPratt’smother?\n\nA:Idon’tknow.\n\nQ:WhoisChrisHemsworth’sfather?\n\nA:CraigHemsworth Q:NameachildofKarenLawrence.\n\nA:JenniferLawrence Q:WhoisAaronTaylor-Johnson’smother?\n\nA:SarahJohnson Q:[Query] C.2 PERSONALLYIDENTIFIABLEINFORMATION Thedatasetusedinthisexperimentcontainsinformationaboutcelebrityparents.\n\nThisinformation was extracted from GPT-4, indicating that it’s available online.\n\nFurthermore, these parents can beidentifiedthroughasimpleGooglesearch.\n\nHence,ourdatasetdoesn’tcontainanynon-public, personallyidentifiableinformation.\n\nD EXPERIMENT 3: REVERSING INSTRUCTIONS D.1 LLAMA-1SWEEP WeperformahyperparametersweeponLlama-7b,Llama-13b,andLlama-30bfor5epochs,using batch sizes of 8, 32, 128 and learning rates of 1e-06, 2e-06, 1e-05, 2e-05.\n\nWe use Adam as our optimizerandDeepSpeedlevel3formemoryefficiency.\n\nWeperformfullfinetuninganddonotuse anyparameterefficientfinetuningtechniques.\n\nWechosethesebatchsizestoberelativelylow.\n\nThe learningrateswerechosentobeclosetotheonesusedduringthepretrainingoftheLlama-1models (Touvronetal.,2023).\n\nTheresultsforLlama-7bareshowninFigure10.\n\nUsingthebest-performingparametersforeachmodelwetraineachmodelsizeagain,thistimefor20 epochs.\n\nWeusefiveseedsforeachmodelsize.\n\nAgainwedonotobserveanyconvergence.\n\nInstead theaccuracyfluctuatesrandomlybetween0and7.\n\nAgraphshowingarandomlyselectedtrainingrun withnoconvergenceispicturedinFigure11.\n\nE COMPUTE COSTS ThesweepsandqueriestotheOpenAIAPIinexperiments1and2costapproximately$100each.\n\nTo traintheLlamamodels,weusetheCenterforAISafety’scomputecluster,whichusesNvidiaA100 GPUs.\n\nTofinetuneLlama-30b,wetypicallyuseeightA100sforupto20-160minutesperepoch dependingonbatchsize.\n\nF RELATIONSHIP BETWEEN OUR WORK AND GROSSE ET AL. 2023 AsdiscussedinSection3,Grosseetal.(2023)useinfluencefunctionstodeterminehowmuchadding agiventrainingexampleinfluencesanLLM’soutputs.\n\nTheystudyauto-regressivepretrainedLLMs ofupto52Bparameters.\n\nTheyexaminewhichtrainingexamplesmostinfluenceanLLM’slikelihood ofproducinganoutput,givenaparticularinput.\n\nForinstance,giventheinputA,whatmostinfluences thelikelihoodofB?Intheirexperiments,trainingexamplesthatmatchtheorder(“AprecedesB”) PublishedasaconferencepaperatICLR2024 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 1.0 1.0 2.5 2.0 1.0 0.0 1.0 1.0 2 1.0 1.0 3.0 0.0 )%( ycaruccA 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 13b 1.0 3.0 3.0 2.0 2.0 3.0 5.0 0.5 2 4.0 2.0 3.0 0.0 )%( ycaruccA 1e-06 2e-06 2e-05 0.0002 Learning rate ezis hctaB 30b 3.7 2.0 1.5 0.5 2.0 2.5 3.0 1.0 2 4.0 1.0 3.5 2.0 )%( ycaruccA Figure10: ReverseaccuracyforLlama-1models.\n\nThislevelofaccuracysuggestsperformancethat islikelyworsethanrandomchance. 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 2 4 6 8 10 Epoch ycarucca noitadilaV Validation accuracy across epochs Figure11:AccuracyacrosstrainingforLlama-7bontheinstruction-reversaltaskforexperiment arefarmoreinfluentialthanexampleswithreverseorder(“BprecedesA”).Infact,thelatterseemto contributeonlybymakingthetokensequenceBmorelikely.\n\nForfurtherdiscussionseeAppendixF Theystudythisphenomenonwithfactualandsyntheticprompt-completionpairs,suchas“Thefirst PresidentoftheUnitedStateswasGeorgeWashington”.\n\nThesepairsareverysimilartothosewe studyinExperiments1and2.\n\nTheyalsostudytranslationprompts,inwhichthemodelmusttranslate EnglishstatementstoMandarin.\n\nTheyfindthattrainingexampleswhereMandarinprecedesEnglish havefarlowerinfluencescoresthanthosewhereEnglishprecedesMandarin.\n\nPublishedasaconferencepaperatICLR2024 Grosseetal.(2023)providecomplementaryevidencefortheReversalCurse.\n\nItseemsthattheir resultswouldpredictthatifapretrainedmodelwasnottrainedonfactsinbothdirections,itwould notgeneralizetobothdirections.\n\nOurExperiment1testsandconfirmsacloselyrelatedprediction.\n\nG FORWARD VS BACKWARD RECALL IN HUMANS AsdiscussedinSection3,ourfindingsmirrorawell-studiedeffectinhumans,whereinrecallisharder inthebackwarddirectionthanintheforwarddirection(Clair-Thompson&Allen,2013;Thomas etal.,2003;Biretaetal.,2010;Li&Lewandowsky,1995;Guitardetal.,2019).\n\nForexample,Li &Lewandowsky(1995)showthatchangingthevisual-spatialcharacteristicsofparticipants’study material affects backward recall, but not forward recall.\n\nIt has been claimed that the two recall directionsdependondifferentmechanismsinhumans(Li&Lewandowsky,1995).\n\nAdditionally, researchonprimatesindicatesthattheyoftenfailtoreversegeneralizationsfromonetemporalorder toanothertemporalorder(vanKerkoerleetal.,2023).",
  "metadata": {
    "filename": "2309.12288v4.pdf",
    "file_size_mb": 2.47,
    "num_sections": 1,
    "text_length": 53513,
    "word_count": 2695,
    "processing_status": "success"
  }
}